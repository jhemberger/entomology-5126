---
title: "Lab 13: Geostatistics"
subtitle: Entomology 5126 - Spring 2026
author: Jeremy Hemberger and Brian Aukema
date: last-modified
date-format: long
format: html
---

------------------------------------------------------------------------

# Introduction

You made it! Today's lab is the last lab of the course (Sniff!) save for the lab exam (Whoopee!). We will cover one aspect of geostatistics today: fitting variograms.

Many principles of lattice/areal data and spatial linear models apply to geostatistics. Both types of analyses model the spatial dependence in the error term.

For lattice or areal data, our spatial error models are more constrained, and thus in one sense "simpler." Lattice methods apply where the data represent regions, rather than points, and for problems where there is a clear neighborhood structure thought to influence the process under study. In that sense, lattice or areal data give us more opportunity to construct scientifically meaningful spatial correlation models (through the neighborhood structure). For geostatistical models, the spatial error model is sometimes easier to choose, and we have a few more tools at our disposal for choosing models.

The boundaries between lattice and geostatistical modeling can be blurred. For example, if we are studying islands and our neighborhoods are defined in terms of inverse distance, then it is not always clear that lattice models are the most appropriate. At the same time, it is not completely clear that geostatistical approaches are the best in such cases.

# Geostatistics

We will focus on *ordinary kriging* today. That is, we will assume there is some constant but unknown mean. In other words, there is just an intercept, that's all. This is the most common type of kriging. Of course, there is also *universal kriging* - the mean response depends on not only a constant but also covariates (or factors if an ANOVA). Universal kriging is a bit tricky; you must estimate the trend in order to obtain the spatial dependence in your residuals (i.e., to fit a variogram), yet the estimates of spatial error (i.e., nugget, range, sill...) also influence the estimates of your mean terms (e.g., the slopes of covariates in which you may be interested). I think you can see the challenges of circular logic here!

Unfortunately, this is often ignored as a practical problem. If the goal is simply spatial prediction at new points, the problem is not too severe. If the problem is inference on your covariates (e.g., does soil depth and/or elevation affect probability of finding water?), the challenge is not trivial. The best theoretical routes in these cases involve Bayesian methods. We will not cover Bayesian methods in this lab, as they are a relatively advanced topic, and we are nearing the end of the course.

There are three major packages for dealing with geostatistical data. The first is `geoR`. It has some great functions for fitting variograms and fitting confidence envelopes around them, similar to putting confidence envelopes around our Ripley's K functions in a previous lab. Another popular package is `gstat`. The third is a package that we have used before, `nlme`, which can be used to model different types of random effects, such as spatial error models in geostatistics. We will use it again today so please load `nlme` now.

# Today's assignment

Today's data set is called "wells." I do not know very much about it. It has 3 columns: $x$, $y$, and $z$. I do not have metadata for it, although it has been used quite a bit as a training tool for exercises in ordinary kriging. I guess it also provides a valuable lesson in safeguarding your data and maintaining the metadata! Assuming this is real data, and someone did spend money collecting it, it is proof that data can walk away in a hurry if you are not careful!

Let's pretend the dataset contains concentrations of nitrate (NO$_3^-$) in drinking water (the *z* column) at different *x* and *y* locations on a $1 \times 1$ km grid. The Minnesota Department of Health (MDH) recommends that there should be no more than 10 ppm nitrate-nitrogen in drinking water.

1.  You should have just loaded the `nlme` package. Please also load the `scatterplot3d` package, which is useful for visualizing data. Also, load the `sp` package. This package creates uniform classes for all of the different types of spatial data and objects, with a goal of making it easier to switch between different types of analyses.\

2.  Read in the data. Let's first take a look at the spatial trends. Here are a couple of ways (these will also work for visualizing marked spatial point processes):\

    1.  `> library(scatterplot3d)`\
        `> scatterplot3d(mydata$x, mydata$y, mydata$z, type="h")`\
        The optional `type="h"` part indicates to draw drop lines to the $x-y$ plane.

    2.  `> library(sp)`\
        `> spatialdata <- SpatialPointsDataFrame(coords=`\
        $~~~~~~~~~~~~~~~~~~~~$`cbind(mydata$x, mydata$y), mydata)`\
        `> bubble(spatialdata, "z", fill=F)`\
        The `fill=F` instructs to avoid colouring in the circles.

    Examining the bubbleplot, does it look like there is spatial variation in the response? That is, do points that are closer together tend to have the same values?

3.  Recall that mixed effects models can handle correlated error structures; we used one example of such in our time series laboratory on repeated measures or longitudinal data. Today, we'll model spatial dependence in the error by using a semivariogram.\
    The command that we used before was `lme()`. With that command, we needed to specify the model formula, the data, and the random effects (i.e., some sort of a grouping factor). However, with geostatistical data, there is typically no grouping factor. That is, we are looking at only one spatial surface (not multiple people, blocks, etc.).

    Instead of the `lme()` command, we will use `gls()`, short for *generalized least squares*. It behaves a lot like the `lme()` command, but does not specify a random effect. Instead, it specifies a correlation structure typified by a variogram (more on this later$\ldots$). Estimation of model coefficients is still performed via maximum likelihood or restricted maximum likelihood.\
    Before you begin this lab, it may be helpful to construct a chart to keep track of your results. Create six columns for six models (one without spatial dependence, an empirical sample variogram, and four models with two different types of spatial dependence). As you work through the lab, please clearly identify the type of model in the column heading. Complete the table with five rows for values of AIC, intercept, sill, range, and nugget. Of course, as you progress through the lab, you will notice that some rows will not have an entry (e.g., the empirical sample variogram does not have an AIC value).

    For the first model, fit an intercept to the data using the `gls()` command (of course, if we were doing universal kriging, you would at this point specify some simple starting model other than just the intercept):

    `> fm1 <- gls(z`$\sim$`1, data=mydata)`

    Examine the residuals with a residual plot:

    `> plot(fitted.values(fm1), residuals(fm1))`

    We know that the assumption of independence might be violated, but we can at least initially peek at the assumptions of normality and homoscedasticity. Next, examine the summary output. We made need to take the results so far with a grain of salt, because we are not accounting for potential spatial dependence that may be skewering our significance tests (although, given the clarity of the initial result, at least when I did it, this is probably not a major concern for this data set).

    Putting concerns of spatial dependence aside temporarily,

    1.  Does it appear, statistically, that there is detectable amount of nitrate present in the well water overall (i.e., different from zero)?

    2.  On average, is the mean level of nitrate in a well significantly different from the maximum health standard set by the Minnesota Department of Health? (*Hint:* You may need to return to Question 9 on Lab 5 to answer this question. Remember to use the error degrees of freedom for your test statistic.)

4.  Ok, let's start thinking about the spatial dependence in the data. Let's construct the sample semivariogram from our model residuals. This is the first step before we fit a theoretical model to the spatial errors:

    `> samplevg <- Variogram(fm1, form =`$\sim$`x + y, resType="response")`

    The first argument contains the model under consideration. This helps R know where to find the residuals. The next argument specifies the covariate(s) to be used for specifying the distance pairs. Note that these are just the $x$ and $y$ locations from the dataset specified in `fm1` and are critical to help formulate the variogram; they have nothing to do with fitting a trend surface such as in universal kriging. The `resType` argument specifies the type of residuals. We will not cover standardizing our residuals in this course, so just use the raw or "response" residuals for now. There are other aguments you can add as well, such as `type="robust"` if you'd like to use the semivariogram robust to outliers first proposed by Noel Cressie.

    Examine the sample variogram by simply typing its name and hitting `Enter`. These are estimates of the spatial dependence in the sample data at different distances. You will see the number of pairs in each distance bin. This is where things get a little fuzzy with regards to constructing a reliable semivariogram. For example, you will find these suggestions and guidelines in the literature:

    -   Never fit the semivariogram beyond one half (or two thirds) the maximum distance in the data

    -   The number of pairs in each bin should be greater than 30

    -   Never compute a semivariogram unless the number of locations is greater than 200 or 300

    Which guideline makes it impossible for us to continue this lab?

5.  Yes, nice rules, but unfortunately for real life data, even following the guidelines *does not guarantee* a lot of precision in the estimated semivariogram. Hence, we take an approach to estimate the semivariogram with whatever data we have, then augment that with some measure (however feeble) of the precision of the estimates. This is what you frequently have to do in real life.

    Plot the sample semivariogram.

    `> plot(samplevg)`

    The line going through the semivariogram is a `loess` smoother. Essentially, this is a moving average that helps you see trends.

    As noted above, estimating spatial correlation at the extremes of the sample space is not very reliable. Choose and report a maximum distance value that you feel is sensible.

6.  Refit the sample variogram, but this time specify an additional argument with your chosen value: `maxDist=?`. Then, plot the sample variogram again, re-specifying the maximum distance for your $x$ (distance) axis:

    `> plot(samplevg, xlim=c(0,?))`

    This little plot may be your best friend for the next hour, so I suggest that you get aquainted. I suggest that you paste a copy into a Word document so you don't lose her/him, or even print a copy that you can decorate with pencil marks as you estimate nuggets and ranges.

    Ok, let's fit a theoretical model to our semivariogram. Some people simply look at their plot, make a guess, and use those guesses in their **final** analysis. **Don't EVER do that.** Seriously, what do they think statistics is for? If you do enjoy simply pulling numbers straight out of your arse and attaching scientific credibility to them, use Bayesian statistics. (You will understand this joke after we talk more about Bayesian statistics).

    We will do something slightly different. Take a long hard look at your semivariogram. **Do** visually estimate and report the nugget (if any), the range, and the sill in your table. Also, estimate and report the *nugget ratio*. This is defined as the nugget semivariance divided by the total semivariance (represented by the sill).

7.  We will use these estimates as *initialization values* for constructing a theoretical semivariogram. That is, the computer will attempt to determine the true values, but it appreciates some help in getting started in its optimization routine. Your initial guesses do *not* form part of the answer; they only help speed along the optimization routine (or at least we hope!).

    Of course, there are different types of semivariograms. This is where we typically haul out our favourite geostatistics book and look at pretty pictures to see which one matches the sample semivariogram the closest. Below I've reprinted a table and figure from section 5.3.2 *Spatial correlation structures* from José Pinherio and Douglas Bates' book *Mixed-effects models in S and S-PLUS* (2000) Springer.

    ::: center
    :::

    In this table, $s$ is distance, while the correlation parameter $\rho$ is generally referred to as the range in the spatial statistics literature. The corresponding figures with range = 1 and nugget effect = 0.1 are below:

    ::: center
    :::

    Let's play with two types of semivariogram models: exponential and spherical. First, let's fit an exponential model (model 3 in your table):

    `> fm1.exp <- gls(z`$\sim$`1, data, corr=corExp(c(R, NR),`

    $~~~~~~~~~~~~~~~~~~~~$`form =`$\sim$`x + y, nugget=T))`

    You have to put numbers *in place of* the `R` and `NR` placeholders in the above command. These specify the *range* and the *nugget ratio*. The `nugget=T` just tells the program that you think there is a nugget effect. Examine the summary output:

    `> summary(fm1.exp)`

    The first thing you should notice is that your range and nugget are not the starting values that you specified. Rather, they reflect the maximum likelihood estimates of these parameters, based on your data and model.

    1.  What is the value of the sill? More importantly, how did you derive it? (Yes, it is right there on your summary output, but you may need a little help).

    2.  Some semivariogram models (like the exponential) do not have a true range, but rather report an "effective range" (even if the output says "range"). The effective range is defined as the distance at which the semivariogram value is 95% of the sill. Do you have a range or effective range, and what is its value?

8.  At this point, you might be scratching your head at the estimate of the range when you compare it to your sample variogram. Hmmm. Well, it's an estimate, but it doesn't tell us anything about the variability in the estimate. What we really need is a measure of variation.

    We *could* of course divide the estimate by some measure of variation to obtain a test statistic, which we could compare to a test statistic distribution to determine whether the estimate is signficantly different from zero. This is not always done with nuggets and sills and ranges, however, because it is sometimes unclear exactly how to determine the standard error, or what the appropriate test statistic distributions are.

    For this lab, we can access 95% confidence intervals about the variogram parameter estimates, however:

    `> intervals(fm1.exp)`

    *Now, whether or not this command worked, please keep reading!* If the confidence interval overlaps zero, the parameter is likely not significant. If the confidence interval varies over 5 or 10 orders of magnitude, or even returns `NaN` (not a number), you should be very concerned about how precise the estimates are. Who knows, it *is* possible that you may even get an error message indicating that the variance-covariance matrix is non-positive definite; i.e., that the confidence intervals cannot be obtained. Any or all of these can be warning signs that your variogram model is not well specified. Perhaps, for example, you are requesting an estimate of a parameter that simply is better off left out, or is so close to zero that it can't be reliably estimated.

    **This can be frustrating.** I get *equally* frustrated, however, with canned statistical software packages that provides an answer for anything and everything, sensible or not, because we are conditioned by the scientific community to expect answers. One of my favourite quotes comes from John Tukey (who contributed a great deal to the theory of multiple comparisons in *post-hoc* tests):

    > The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.

    R is nice because it can *give* you an answer in the summary output, but it can *also* tell you from looking at the `intervals()` command that it might be a poor answer.

    When I constructed this lab exercise, the nugget parameter was giving me some grief (OK, lots of grief). Refit your exponential model *without* the nugget (you will need to remove the nugget ratio, and specify `nugget=F`). Examine and enter the estimates of the range, sill, and intercept in your table, and check the intervals as before. The intervals may change, making more sense (or less), or they may be substantially similar to the previous step. Let's compare the two models:

    `> anova(fm1.exp, fm1.expSansNugget)`

    This command constructs a likelihood ratio test. In other words, it compares the change in deviance between the two models, and compares that change in deviance to a $\chi^2$ distribution with degrees of freedom equal to the change in the degrees of freedom (in this case, 1 df, because the second model is not estimating the nugget). If the $p$-value is not significant, it means that adding that new parameter did not explain significant variation, so you can proceed with the simpler model.\
    So, which exponential model do you prefer: the one with the nugget effect, or the one without the nugget effect?

9.  Fit a spherical semivariogram **with** a nugget effect (fifth model in table). The syntax is exactly the same as before, except the correlation structure is specified as `corSpher`. Examine the summary, and report and comment on the values of the sill and the range. Do you think these estimates are more consistent with your sample semivariogram than the estimates from the exponential model?

10. Check the intervals on the spherical semivariogram. Then, fit a spherical semivariogram **without** a nugget effect (last model in table). Compare the two models with a likelihood ratio test. Is the nugget effect necessary?

11. I will assume that you obtained a consistent answer for necessity of estimating a nugget for both the exponential and spherical models. If not, for learning purposes, choose both models without a nugget.

    First, just from visual inspection and examination of your preliminary results, which semivariogram model do you think fits the data better: exponential or spherical? Now, do a formal comparison. Because they are not nested, we cannot do a likelihood ratio test. However, we can compare them with information criteria such as AIC. Which model fits the best? Did this agree with your expert judgement?

12. Compare this "best" model with your very first model without any spatial dependence specified (before you fit the sample semivariogram) using a likelihood ratio test. (The models are considered nested, because one has an intercept, and the other has an intercept plus a bunch of spatial error parameters). Was it necessary to account for spatial dependence in this dataset? (That is, was this lab a complete waste of time?)

You have just completed one of the most challenging parts of geostatistical analysis: modeling the spatial error structure in the variogram. One of the frequent goals of geostatistical data analysis is to predict new $z$ values on the spatial surface, given the spatial correlation present (now that you've defined it). This is part of the science of *kriging*. A good package for prediction of new values is `gstat`, should you ever need it. However, we will not do that as part of this week's exercise.
