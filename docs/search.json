[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Below, you’ll find the schedule to all of the topics and material that we’ll cover this semester. A few notes on the important links (Content, Lab, and Example) are included below. Each week only contains a single link for each category: this is intentional! The material is all organized/compiled by week, rather than by day, even if we cover multiple topics over the course of a week.\n\nContent (): Here you’ll find the readings, lecture slides, and all other material that supports the lecture components of the course.\nLab (): This page contains the instructions, data, and other material for the 13 labs where we’ll explore examples of topics using real data.\nExample (): This page will contain worked-examples of the various topics that we’ll cover over the course of the semester.\n\n\n\nModule 1: RStudio and Tidyverse crash course\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\nContent\n\n\nLab\n\n\nExample\n\n\n\n\n\n\nWeek 1\n\n\n1/21/26\n\n\nOrientation to class\n\n\n\n\n\n\n\n\n\n\n\n\n1/23/26\n\n\nData wranglin’\n\n\n\n\n\n\n\n\n\n\n\n\n\nModule 2: Reviewing foundations in statistics\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\nContent\n\n\nLab\n\n\nExample\n\n\n\n\n\n\nWeek 2\n\n\n1/28/26\n\n\nSampling & probability\n\n\n\n\n\n\n\n\n\n\n\n\n1/30/26\n\n\nRandom variables and probability distributions\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n2/4/26\n\n\nTransformations\n\n\n\n\n\n\n\n\n\n\n\n\n2/6/26\n\n\nHypotheses and one-sample t-tests\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n2/11/26\n\n\nDependent and independent two-sample t-tests\n\n\n\n\n\n\n\n\n\n\n\n\n2/13/26\n\n\nAnalysis of variance (ANOVA)\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\n2/18/26\n\n\nEvaluating assumptions, linear models, correlation\n\n\n\n\n\n\n\n\n\n\n\n\n2/20/26\n\n\nSimple linear regression\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n2/25/26\n\n\nMultiple regression\n\n\n\n\n\n\n\n\n\n\n\n\n2/27/26\n\n\nMultiple regression\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n3/4/26\n\n\nModel selection\n\n\n\n\n\n\n\n\n\n\n\n\n3/6/26\n\n\nMidterm\n\n\n\n\n\n\n\n\n\n\n\n\n\nModule 3: Integrating time into our models\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\nContent\n\n\nLab\n\n\nExample\n\n\n\n\n\n\nWeek 9\n\n\n3/18/26\n\n\nMidterm review\n\n\n\n\n\n\n\n\n\n\n\n\n3/20/26\n\n\nIntro to space and time\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\n3/25/26\n\n\nMixed effects models\n\n\n\n\n\n\n\n\n\n\n\n\n3/27/26\n\n\nInformation critera\n\n\n\n\n\n\n\n\n\n\n\n\n\nModule 4: Integrating space into our models\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\nContent\n\n\nLab\n\n\nExample\n\n\n\n\n\n\nWeek 11\n\n\n4/1/26\n\n\nSpatial point processes: Ripley’s K\n\n\n\n\n\n\n\n\n\n\n\n\n4/3/26\n\n\nGeneralized linear models\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\n\n\n4/8/26\n\n\nGLMs and analogies to point process models\n\n\n\n\n\n\n\n\n\n\n\n\n4/10/26\n\n\nSpatial point processes: models\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\n\n\n4/15/26\n\n\nLattice data: MW test, Geary’s C, Moran’s I\n\n\n\n\n\n\n\n\n\n\n\n\n4/17/26\n\n\nLattice data: SAR and CAR models\n\n\n\n\n\n\n\n\n\n\n\nWeek 14\n\n\n4/22/26\n\n\nGeostatistics\n\n\n\n\n\n\n\n\n\n\n\n\n4/24/26\n\n\nGeostatistics\n\n\n\n\n\n\n\n\n\n\n\nWeek 15\n\n\n4/29/26\n\n\nCurrent topics: species distribution models\n\n\n\n\n\n\n\n\n\n\n\n\n5/1/26\n\n\nCourse wrap-up\n\n\n\n\n\n\n\n\n\n\n\nWeek 16\n\n\n5/6/26\n\n\nFinal exam"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Anyone attempting to analyze data from lab, field, or existing datasets are frequently confronted with a stark reality: organisms rarely stay in the same place in space and time. It is well known that spatial and temporal dependence in biological/ecological data can affect inference; but how can we (1) detect potential problems; (2) determine their severities; and (3) apply the appropriate methods so that our analyses robustly and accurately represent reality and allow clear and unbiased inference? Attempts to locate suitable tools often encounter mathematical jargon inaccessible to the average practitioner. The goal of this course is to provide a gentle, but thorough foundation to the concepts and tools needed to analyze, interpret, and communicate spatially and temporally dependent data.\nThe first half of the semester starts with a brief refresher on using R for data management and linear models (regression and ANOVA). In the second half of the semester, we will extend these foundational frameworks to include an introduction to temporal data analysis and three major areas of spatial statistics: spatial point processes, lattice data, and geostatistics. By bridging basic theories and practices, students will leave this course understanding why it is important to contend with spatiotemporal dependence and the skills needed to confidently apply these practices to their own data."
  },
  {
    "objectID": "syllabus.html#description",
    "href": "syllabus.html#description",
    "title": "Syllabus",
    "section": "",
    "text": "Anyone attempting to analyze data from lab, field, or existing datasets are frequently confronted with a stark reality: organisms rarely stay in the same place in space and time. It is well known that spatial and temporal dependence in biological/ecological data can affect inference; but how can we (1) detect potential problems; (2) determine their severities; and (3) apply the appropriate methods so that our analyses robustly and accurately represent reality and allow clear and unbiased inference? Attempts to locate suitable tools often encounter mathematical jargon inaccessible to the average practitioner. The goal of this course is to provide a gentle, but thorough foundation to the concepts and tools needed to analyze, interpret, and communicate spatially and temporally dependent data.\nThe first half of the semester starts with a brief refresher on using R for data management and linear models (regression and ANOVA). In the second half of the semester, we will extend these foundational frameworks to include an introduction to temporal data analysis and three major areas of spatial statistics: spatial point processes, lattice data, and geostatistics. By bridging basic theories and practices, students will leave this course understanding why it is important to contend with spatiotemporal dependence and the skills needed to confidently apply these practices to their own data."
  },
  {
    "objectID": "syllabus.html#student-learning-outcomes",
    "href": "syllabus.html#student-learning-outcomes",
    "title": "Syllabus",
    "section": "Student learning outcomes",
    "text": "Student learning outcomes\nUpon successful completion of this course, students will be able to:\n\nApply the appropriate techniques to characterize spatial and temporal dependence in their data\nUnderstand how spatial and temporal dependence can affect statistical inference\nIdentify and choose the appropriate analytical methods to contend with such dependence in widely-used statistical methods like regression and ANOVA\nCritically evaluate and understand the strengths and limits of these approaches\nPresent analytical results in a format befitting a scientific presentation or publication"
  },
  {
    "objectID": "syllabus.html#meeting-times",
    "href": "syllabus.html#meeting-times",
    "title": "Syllabus",
    "section": "Meeting times",
    "text": "Meeting times\nLecture: We will meet for two lectures per week, with a lab session following after the second lecture. Attendance is, obviously, strongly encouraged! For some of you, the material in weeks 1-3 might mostly be review from previous courses. Despite this, I still encourage you to attend to (1) make sure your memory serves and (2) get to know your fellow students (i.e., collaborators). We’ll also be re-enforcing the expectations for future lab assignments.\nLab: Because campus does not have a computer lab available for our class, please bring your own laptop with R and RStudio installed to class. If you don’t have access to one, please let me know and I’ll facilitate furnishing one for you! The lab session each week will emphasize hands-on analytical skills and statistical methods relevant to the week’s lecture content. Each lab will have an accompanying assignment/report that will be graded."
  },
  {
    "objectID": "syllabus.html#student-assessments",
    "href": "syllabus.html#student-assessments",
    "title": "Syllabus",
    "section": "Student assessments",
    "text": "Student assessments\nLecture: Lecture-related components will comprise 45% of the course grade and include: (1) a midterm exam (20%); and (2) a comprehensive final exam (25%). These will include concepts covered in the lectures and may include relevant examples or reference to skills practiced in the labs, as well.\nLab: There are a total of 13 labs, each with a lab assignment/report that will be evaluated. The first three periods will be devoted to a primer of using R/RStudio for managing, wrangling, and interpreting data, and the reports associated with these will receive feedback but will not be included in the course grade. The remaining 10 lab assignments/reports will be graded and count toward 40% of total course grade (4% per report). There will also be a lab final worth 15% of the total course grade. In this, students will be given a real data set to analyze and develop a report for during the last lab period.\nLab reports will be due one week following the lab exercise at the beginning of the lab. Please upload these into Canvas. Please don’t email these to me! There’s no grade penalty for late submissions of lab reports. That said, I highly recommend staying caught up. The level of feedback I’m able to give will decline exponentially with each passing day, so that’s effectively the only punishment.\nAt the end of the course, letter grades will be assigned to the quantitative point totals from student performance in the class. Pretty standard stuff here!\nPoint Breakdown Assignment Points Percent Lab reports (10 × 16) 160 40% Lab final 60 15% Lecture midterm 80 20% Lecture final 100 25% Total 400 100%\nFinal Grade Breakdown Grade Range Grade Range A 93–100% C 73–76% A− 90–92% C− 70–72% B+ 87–89% D+ 67–69% B 83–86% D 63–66% B− 80–82% D− 60–62% C+ 77–79% F &lt; 60%\nAdjustments to final grades based on class participation or instructional interaction demonstrating mastery may be made on a case-by-case basis. Similarly, adjustments to the entire class based on comparisons to previous offerings may be made."
  },
  {
    "objectID": "syllabus.html#learning-material",
    "href": "syllabus.html#learning-material",
    "title": "Syllabus",
    "section": "Learning material",
    "text": "Learning material\n\nTextbooks/Readings\nThis course does not have a single, assigned text, however we’ll make use of material from several reference books, journal articles, and websites throughout the semester. All of these materials will be posted on the course website. These are for reference only and serve as more in-depth treatments of some of the course material.\n\n\nSoftware\nThe purpose of this class is to teach concepts over software - - this ensures that after this course, students will be able to implement the methods covered in their software of choice. That said, we need to choose some computational tool to do our work, so we will be using R and its integrated development environment, RStudio, in this course. R is the most widely used language and statistical software in the life sciences. It’s available on all major operating systems and is 100% free and open-source ensuring that students will retain access to the toolkits developed in this course long after the semester ends. We’ll cover all of the basics of installing and using R to load, manipulate, and visualize data, assuming that folks may have some familiarity with R, but with enough detail that novices will be able to pick up the necessary skills as well. However, if you’d like to get a head start on learning R, some great, free online resources are included on the course website."
  },
  {
    "objectID": "syllabus.html#policies",
    "href": "syllabus.html#policies",
    "title": "Syllabus",
    "section": "Policies",
    "text": "Policies\n\nAttendance\nAttendance in lectures and labs is strongly encouraged. Can you learn these skills on your own? Sure. But you’ll get a better outcome and enjoy the process a whole lot more by doing it with your friends and colleagues! This is especially the case when learning a new skillset like R - - being able to quickly bounce a question off of someone IRL is always preferable to spending hours trawling StackExchange for an answer. Please give me a heads-up if you will be away for a week or more. If you will miss any assessment activity, it is your responsibility to contact me in advance so suitable arrangements can be made for a make-up exercise given valid circumstances (time-sensitive field work, bereavement, medical, etc.). Absence of a good faith attempt to let me know of your anticipated or unexpected absence may result in a grade of zero for missed assessments.\n\n\nAI, LLMs, and Bullshit\nI highly recommend NOT using ChatGPT or other large language models (aka LLMs) in this (or any…) class.\nThese tools, while highly functional and time-saving in certain situations, are at direct odds with the practice of learning and developing competence in concepts and skills - be they coding, writing, or other artistic pursuits. I’m not opposed to using these tools in a variety of circumstances, and I use ChatGPT for a variety of tasks to provide a quick review of my own work. These tools are phenomenal resources for coding when you already know what you are doing. Using LLMs requires careful skill, attention, practice, and they tend to be useful only in specific limited cases.\nUsing ChatGPT to write all of your code will not help you learn how to master R or understand the statistical concepts that we are learning in this class. What these models do is essentially amalgamate all of the publicly-available code on GitHub and spit out a plausible looking answer. This is otherwise known as bullshitting. Sure, bullshitting can sometimes get you part of, or, in rare instances, all the way to the right answer. But more often than not, the code it spits out will contain components that are outright wrong or have stuff you don’t need. These models don’t care if the “answer” provided is wrong or right (nor can the models actually know). It doesn’t care if your code runs or not. It just cares that the crap it spits back at you looks plausible. A hilarious, and slightly terrifying example of this comes from Andrew Heiss:\n\n“A good analogy for this is with recipes. ChatGPT is really confident at spitting out plausible-looking recipes. A few months ago, for fun, I asked it to give me a cookie recipe. I got back something with flour, eggs, sugar, and all other standard-looking ingredients, but it also said to include 3/4 cup of baking powder. That’s wild and obviously wrong, but I only knew that because I’ve made cookies before. I’ve seen other AI-generated recipes that call for a cup of horseradish in brownies or 21 pounds of cabbage for a pork dish. In May 2024, Google’s AI recommended adding glue to pizza sauce to stop the cheese from sliding off. Again, to people who have cooked before, these are all obviously wrong (and dangerous in the case of the glue!), but to a complete beginner, these look like plausible instructions.”\n\nThe purpose of this course is to outfit y’all with the concepts and tools needed to accurately and honestly portray the results of hard-won data that we use to understand the way the world works so that we can be better stewards and citizens of our shared planet. I want y’all to succeed in this. I don’t want to read a bunch of bullshit generated by an LLM. It’s not useful to you. It’s not useful to me. It’s a waste of time.\nAll that said, I’m not going to play Sherlock and try and guess whether or not what you’re turning in is AI-generated. As I mentioned above, there are legitimate and good use-cases for LLMs in the work that we do such as helping debug some code, interpreting an error message that you don’t understand, or helping translate what individual lines of code may be doing if you can’t understand from the documentation. However, there’s a fairly good chance that I will notice if what you’re turning in came from a LMM. There are tells in the way machines write code versus the way most humans do. While this may not impact your grade, it will impact your learning. And it will make me sad. Please don’t make me sad.\n\n\nPhones and electronic devices\nObviously we’ll need access to computers for this course, and taking notes during lectures on your computers is fine. However, I would suggest taking real, physical notes given the abundance of research that links this to improved comprehension and learning outcomes. As for cell phone use, please silence them and put them in a bag.\n\n\nStudent conduct\nThe University seeks an environment that promotes academic achievement and integrity, that is protective of free inquiry, and that serves the educational mission of the University. Similarly, the University seeks a community that is free from violence, threats, and intimidation; that is respectful of the rights, opportunities, and welfare of students, faculty, staff, and guests of the University; and that does not threaten the physical or mental health or safety of members of the University community. As a student at the University you are expected to adhere to the Student Conduct Code as approved by the Board of Regents. The Student Conduct Code can be found in the UMN Policy Library: https://regents.umn.edu/policy/allboard-of-regents.\n\n\nAcademic integrity\nAcademic integrity is essential to a positive teaching and learning environment. All students enrolled in University courses are expected to complete coursework responsibilities with fairness and honesty. The University Student Code of Conduct defines scholastic dishonesty as:\n\n”…submission of false records of academic achievement; cheating on assignments or examinations; plagiarizing; altering, forging, or misusing a University academic record; taking, acquiring, or using test materials without faculty permission; acting alone or in cooperation with another to falsify records or to obtain dishonestly grades, honors, awards, or professional endorsement.”\n\nRarely - especially among upper division courses - have I found this to be a problem. I encourage and expect students to work together on laboratory assignments; in real practice, you will collaborate or discuss with colleagues the best analytical route to choose. However, the work you submit must be your own. Plagiarism (from classmates or the internet), cheating during tests or exams, or misrepresenting the nature of your involvement in any assigned work may result in disciplinary action, including, but not limited to, a penalty up to and including an “F” or “N” for the course.\n\n\nStudent accommodations\nOccasionally, the need arises to employ different testing techniques in assessing student mastery of the subject matter. Determining appropriate disability accommodations is a collaborative process. You as a student must register with Disability Services and provide documentation of your disability; I will then work with Disability Services to determine appropriate accommodations. See http://disability.umn.edu.\n\n\nStress management\nWe live in continually unprecedented times. On top of all the external stressors in our lives, strained relationships, anxiety, depression, substance addiction problems, and more can impede learning, reduce your ability to participate in daily activities, and reduce your academic performance (in addition to just making your life more difficult and miserable!). Please realize that you are not alone and that a broad range of confidential mental health services are available on campus to assist you. Reaching out for help is challenging – but it is a smart and courageous thing to do. http://www.mentalhealth.umn.edu"
  },
  {
    "objectID": "content/slides/slides-test.html#getting-up",
    "href": "content/slides/slides-test.html#getting-up",
    "title": "Week 1",
    "section": "Getting up",
    "text": "Getting up\n\nTurn off alarm\nGet out of bed"
  },
  {
    "objectID": "content/slides/slides-test.html#going-to-sleep",
    "href": "content/slides/slides-test.html#going-to-sleep",
    "title": "Week 1",
    "section": "Going to sleep",
    "text": "Going to sleep\n\nGet in bed\nCount sheep"
  },
  {
    "objectID": "content/w1-content.html",
    "href": "content/w1-content.html",
    "title": "Week 1",
    "section": "",
    "text": "View slides in full screen\n       \n      \n    \n  \n\n\n\n\n\n\nTip\n\n\n\nTip: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.",
    "crumbs": [
      "Content",
      "Module 1: RStudio and Tidyverse crash course",
      "Week 1: Class orientation and introduction to R and data wrangling"
    ]
  },
  {
    "objectID": "content/w1-content.html#slides",
    "href": "content/w1-content.html#slides",
    "title": "Week 1",
    "section": "",
    "text": "View slides in full screen\n       \n      \n    \n  \n\n\n\n\n\n\nTip\n\n\n\nTip: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.",
    "crumbs": [
      "Content",
      "Module 1: RStudio and Tidyverse crash course",
      "Week 1: Class orientation and introduction to R and data wrangling"
    ]
  },
  {
    "objectID": "lab/w5-lab.html",
    "href": "lab/w5-lab.html",
    "title": "Lab 5: Simple linear regression",
    "section": "",
    "text": "Today’s lab focuses on simple linear regression. ANOVA is used to examine factors; regression is used for continuous variables. Once you have both techniques in your toolbox, you will be able to do almost anything! This includes combining both factors and continuous variables in the same model (Analysis of Covariance, or ANCOVA), and changing assumptions about the distribution of the data (e.g., generalized linear models). This basic ANOVA and regression framework will serve us well as we move into more advanced topics in spatial and temporal analyses through the month of March.",
    "crumbs": [
      "Labs",
      "Module 2: Reviewing foundations in statistics",
      "Lab 5: Simple linear regression"
    ]
  },
  {
    "objectID": "lab/w5-lab.html#useful-commands-for-fitting-regression-models",
    "href": "lab/w5-lab.html#useful-commands-for-fitting-regression-models",
    "title": "Lab 5: Simple linear regression",
    "section": "Useful commands for fitting regression models",
    "text": "Useful commands for fitting regression models\n\nThe linear model command, lm(). Here are the steps:\n\nDecide what to call your model. E.g.,\n&gt; fm1 &lt;-\nSpecify the lm() command, and determine what y variable you are analyzing. It should be something that is, or can be transformed to be, normally distributed.\n&gt; fm1 &lt;- lm(y\\(\\sim\\)\nList the continuous variable(s) that you are analyzing. For example, if you examining the effect of precipitation on a response variable y, you might write\n&gt; fm1 &lt;- lm(y\\(\\sim\\)precipitation\nSpecify the dataset you are using.\n&gt; fm1 &lt;- lm(y\\(\\sim\\)precipitation, data=mylabdata)\n\n\nHey! This looks very similar to fitting an ANOVA (which is also a linear model)!\nThe summary() function. Examine the \\(F\\)-statistic of the overall model, which you can find at the bottom of the summary. This tells you whether there is any signficant relationship between your response variable and your predictor. (Alternatively, you can also recover the same \\(F\\)-statistic from the anova() command, because even a regression can be analysed using an ANOVA framework. There just aren’t as many replicates in each “group”). If the \\(p\\)-value is greater than 0.05, then there is nothing significant. Go have lunch and design another experiment.\nRemember, while the \\(p\\)-value from the overall model is good to examine, you should also simultaneously examine how well the model fits and if a transformation might be necessary:\n&gt; plot(residuals(fm1)\\(\\sim\\)fitted.values(fm1))\nOnce you have a good model, to get detailed information on model coefficients:\n&gt; summary(fm1)\nRemember, a residual plot should show roughly equal variance around the horizonal zero line across the plot. If the spread is like a fan, for example, you may need to apply a transformation. You can do this right in your model fitting command:\n&gt; fm1 &lt;- lm(sqrt(y)\\(\\sim\\)precipitation, data=mylabdata)",
    "crumbs": [
      "Labs",
      "Module 2: Reviewing foundations in statistics",
      "Lab 5: Simple linear regression"
    ]
  },
  {
    "objectID": "lab/w6-lab.html#factors-vs.-continuous-or-discrete-variables",
    "href": "lab/w6-lab.html#factors-vs.-continuous-or-discrete-variables",
    "title": "Lab 6: Multiple linear regression and ANCOVA",
    "section": "Factors vs. continuous or discrete variables",
    "text": "Factors vs. continuous or discrete variables\nAs a reminder, if you have a numeric variable (like trials 1 and 2) and you would like to convert it to a factor, use the as.factor() command:\n&gt; myniftydata$trial &lt;- as.factor(myniftydata$trial)\nIf you ever need to turn an appropriate categorical variable (such as above) into a numeric variable, use this command:\n&gt; myniftydata$trial &lt;- as.numeric(as.character(myniftydata$trial))",
    "crumbs": [
      "Labs",
      "Module 2: Reviewing foundations in statistics",
      "Lab 6: Multiple linear regression and ANCOVA"
    ]
  },
  {
    "objectID": "lab/w11-lab.html#logistic-models",
    "href": "lab/w11-lab.html#logistic-models",
    "title": "Lab 11: Spatial point process models",
    "section": "Logistic models",
    "text": "Logistic models\nModels that use the binomial family (i.e., 0/1 data) are known as logistic models. These are quite common; e.g., presence/absence data. Note that we are talking about the response, not the predictor variables. For logistic models, the canonical link function is known as the “logit”. Using simple linear regression (but you could use ANOVA or ANCOVA of course): \\[\\log \\left(\\frac{p(Y)}{1-p(Y)}\\right)=\\beta_0 + \\beta_1 x\\] Back-transforming link functions can be a little more complex. To get the probability of the reponse occurence from a logistic model, use: \\[p(Y) = \\frac{\\exp(\\beta_0 + \\beta_1 x)}{1 + \\exp(\\beta_0 + \\beta_1 x)}\\]",
    "crumbs": [
      "Labs",
      "Module 4: Integrating space into our models",
      "Lab 11: Spatial point process models"
    ]
  },
  {
    "objectID": "lab/w11-lab.html#poisson-models",
    "href": "lab/w11-lab.html#poisson-models",
    "title": "Lab 11: Spatial point process models",
    "section": "Poisson models",
    "text": "Poisson models\nPoisson models utilize the natural log: \\[\\log(y)=\\beta_0 + \\beta_1 x\\] Back-transforming the link function: \\[y = \\exp(\\beta_0 + \\beta_1 x)\\]",
    "crumbs": [
      "Labs",
      "Module 4: Integrating space into our models",
      "Lab 11: Spatial point process models"
    ]
  },
  {
    "objectID": "lab/w11-lab.html#fitting-generalized-linear-models",
    "href": "lab/w11-lab.html#fitting-generalized-linear-models",
    "title": "Lab 11: Spatial point process models",
    "section": "Fitting generalized linear models",
    "text": "Fitting generalized linear models\nWe use the generalized linear models command, glm(). Hey, it contains that lm part!\n\nDecide what to call your model. E.g.,\n&gt; fm1 &lt;-\nSpecify the glm() command, and determine what y variable you are analyzing. Remember, for logistic regression, it must be binomial, like a list of 0/1s.\n&gt; fm1 &lt;- glm(y\\(\\sim\\)\nList the variable(s) that you are analyzing for purposes of the experiment. For example, if you examining the effect of water (either as a factor or continuous variable) on a response variable \\(y\\) (perhaps 0/1 germination), you might write\n&gt; fm1 &lt;- glm(y\\(\\sim\\)water\nSpecify the dataset you are using.\n&gt; fm1 &lt;- glm(y\\(\\sim\\)water, data=mylabdata)\nNow specify the generalized linear model family. There are a bunch to choose from (see ?glm if you are curious).\n&gt; fm1 &lt;- glm(y\\(\\sim\\)water, data=mylabdata, family=\"binomial\")\n\nThe anova() command looks at the sequential effects of the different terms in the model. The test statistic when working with deviance is \\(\\chi^2\\) distributed. We have to specify this, so the command becomes\n&gt; anova(fm1, test=\"Chisq\")\nThe summary() command will give you the coefficients of your regression or ANOVA or ANCOVA line. The tests to see if each coefficient is different from zero is done with a Wald test, rather than a \\(t\\) test, but is completely analagous. It results in a \\(p\\) value, so the output will look pretty familiar. The summary will also contain an AIC value for that particular model.\nIf you ever need to do generalized linear models in a mixed effects framework, you need the generalized linear mixed models command, glmmPQL(), which you can access after loading library(MASS). The PQL part refers to the model fitting algorithm, which uses something called “penalized quasi-likelihood” (just in case you were wondering.) Newer methods, including “adaptive Gaussian quadrature” and “2nd order Laplace approximations” can be found in library(lme4) and the associated lmer() command. The syntax for fitting random effects is a little different in the latter, so you may need to check the help files there.",
    "crumbs": [
      "Labs",
      "Module 4: Integrating space into our models",
      "Lab 11: Spatial point process models"
    ]
  },
  {
    "objectID": "lab/w2-lab.html#reading-data",
    "href": "lab/w2-lab.html#reading-data",
    "title": "Lab 2: data wrangling and plotting",
    "section": "Reading data",
    "text": "Reading data\nAfter exporting data from Excel (File/Save As… for a worksheet and choosing text format(tab delimited)) you can import it with the read.table() command, which we learned last week.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 2: Data wrangling and plotting"
    ]
  },
  {
    "objectID": "lab/w2-lab.html#creating-a-list-of-numbers",
    "href": "lab/w2-lab.html#creating-a-list-of-numbers",
    "title": "Lab 2: data wrangling and plotting",
    "section": "Creating a list of numbers",
    "text": "Creating a list of numbers\nA list of numbers, which you may need to specify the rows that you’d like, for example, can be created by concatenation or the c() command. Recall, to stick numbers together in a “vector” (list of numbers, like a spreadsheet column), name the vector, and use the concatenate command:\n&gt; myniftyvector &lt;- c(1,2,3,4,5)\nAs an aside, a short form for a sequence of numbers in a row, as above, is simply 1:5. There are two other commands you may find useful at some point in this course: seq() and rep(). I will let you experiment with these using the help commands ?seq and ?rep. It may be a little strange at first, but if you read the examples at the bottom of the instructions and try one or two test cases, it becomes clear.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 2: Data wrangling and plotting"
    ]
  },
  {
    "objectID": "lab/w2-lab.html#subsetting-data",
    "href": "lab/w2-lab.html#subsetting-data",
    "title": "Lab 2: data wrangling and plotting",
    "section": "Subsetting data",
    "text": "Subsetting data\nQuite often, we want to perform an operation or analysis on only one portion of the data. R uses indexing to select variables, in a scheme like this:\n&gt; myniftydata[rowsIwant, columnsIwant]\nIt’s quite simple to select various rows or columns. You can either name the the columns you want, or use the indexing position. A comma separates the row from the column arguments.\n&gt; myniftydata[1:5,2] selects the first 5 rows in the 2nd column.\n&gt; myniftydata[-3,2] selects all rows EXCEPT the 3rd, in the 2nd column.\n&gt; myniftydata[,2:6] selects all rows and columns 2 through 6.\n&gt; myniftydata[1:5, c(\"year\",\"trial\")] selects the first 5 rows of two columns named “year” and “trial.”\nA quick way to select one entire column is using a dollar sign. The dollar sign specifies which column within a dataframe, by name:\n&gt; myniftydata$trial\nIf you want the 5th observation, you could index that using brackets. If you are using the dollar sign to specify a column, the bracket index does not use a comma separator as above because you are only specifying the rows within the specific column:\n&gt; myniftydata$trial[5] picks the 5th observation in the trial column.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 2: Data wrangling and plotting"
    ]
  },
  {
    "objectID": "lab/w2-lab.html#creating-a-new-column",
    "href": "lab/w2-lab.html#creating-a-new-column",
    "title": "Lab 2: data wrangling and plotting",
    "section": "Creating a new column",
    "text": "Creating a new column\nYou can create a new column in a dataframe by using the dollar sign and naming the new column. For example,\n&gt; myniftydata$diameter &lt;- 1.9 creates a new column named diameter that is full of the values 1.9. You can put text into a new column simply by using quotations around the text value.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 2: Data wrangling and plotting"
    ]
  },
  {
    "objectID": "lab/w2-lab.html#deleting-an-old-column",
    "href": "lab/w2-lab.html#deleting-an-old-column",
    "title": "Lab 2: data wrangling and plotting",
    "section": "Deleting an old column",
    "text": "Deleting an old column\nSpecify the offending column as NULL:\n&gt; myniftydata$junk &lt;- NULL",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 2: Data wrangling and plotting"
    ]
  },
  {
    "objectID": "lab/w2-lab.html#selecting-elements-of-a-dataframe",
    "href": "lab/w2-lab.html#selecting-elements-of-a-dataframe",
    "title": "Lab 2: data wrangling and plotting",
    "section": "Selecting elements of a dataframe",
    "text": "Selecting elements of a dataframe\nNow, often we do not want just any rows or columns, but we want all rows that have a certain value. This can be accomplished using mathematical operators. For example,\n&gt; myniftydata[myniftydata$trial==4,] selects all of the rows in the dataframe where the trial column has a value of 4 (and all columns of those specific rows).\n&gt; myniftydata[myniftydata$diameter &gt;= 30,] selects all rows in the dataframe where the diameter is greater than or equal to 30 (and all columns of those specific rows).\n&gt; myniftydata[(myniftydata$dbh &gt;= 30 & myniftydata$trial==4),] selects all rows with dbh greater than or equal to thirty in the fourth trial (and all columns of those specific rows). The ampersand & is the AND argument; use a pipe \\(\\mid\\) for the OR argument.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 2: Data wrangling and plotting"
    ]
  },
  {
    "objectID": "lab/w2-lab.html#factors-vs.-continuous-or-discrete-variables",
    "href": "lab/w2-lab.html#factors-vs.-continuous-or-discrete-variables",
    "title": "Lab 2: data wrangling and plotting",
    "section": "Factors vs. continuous or discrete variables",
    "text": "Factors vs. continuous or discrete variables\nLast class, we examined summary statistics on the data after we imported it. You may have transformed the ‘trial’ column in the rodent data set so that instead of “first” or “second” it said “1” or “2”. When you performed a summary on the data, you received summary statistics for the trial variable: a minimum, mean, maximum…. Of course, that made little sense: who cares if the mean of the trial is 1.5? It was a categorical variable, not a numerical variable. Instead of having values of “1” or “2” the trials could have (and probably should have) been named “A” and “B”.\nWhen you read in data, R makes its best guess at what type of variables you have. The summary() command is useful: if you have a numerical or discrete variable, you will see a numerical summary (min, max, mean, quantiles…). If you have a categorical or text variable, you will see the different values followed by a colon and the frequency with which those values appear.\nIf you have a numeric variable (like trials 1 and 2) and you would like to convert it to a factor, use the as.factor() command:\n&gt; myniftydata$trial &lt;- as.factor(myniftydata$trial)\nThis command actually creates a new variable, trial, in the dataframe, and puts in the values of myniftydata$trial converted to a factor. Because the new variable has the same name as an existing variable, the old one is overwritten.\nIf you ever need to turn a categorical variable into a numeric variable (rare, but it may happen…) use this command:\n&gt; myniftydata$trial &lt;- as.numeric(as.character(myniftydata$trial))",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 2: Data wrangling and plotting"
    ]
  },
  {
    "objectID": "lab/w2-lab.html#checking-data",
    "href": "lab/w2-lab.html#checking-data",
    "title": "Lab 2: data wrangling and plotting",
    "section": "Checking data",
    "text": "Checking data\nLast week, we examined the commands dim(), nrow(), and summary(). Remember, you can do the summary() command on a single column just as well as the entire dataframe!",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 2: Data wrangling and plotting"
    ]
  },
  {
    "objectID": "lab/w9-lab.html#repeated-measures",
    "href": "lab/w9-lab.html#repeated-measures",
    "title": "Lab 9: Mixed-effects models for time series",
    "section": "Repeated measures",
    "text": "Repeated measures\nThere are many different types of temporal analyses, covering an entire semeseter’s worth (and beyond) of material. One area of time series analysis includes “repeated measures” analyses. As you might expect, it involves taking repeated measures on an experimental subject (whether that be a plant, animal, human, etc…). For example, you might take repeated measurements on a person to examine how fast a particular drug is cleared from the body. In statistics, such studies are commonly called “longitudinal studies” but should not be confused with any spatial analyses.\nMixed effects models are useful for repeated measures studies because they can accommodate the temporal dependence of multiple observations on each experimental subject.",
    "crumbs": [
      "Labs",
      "Module 3: Integrating time into our models",
      "Lab 9: Mixed-effects models for time series"
    ]
  },
  {
    "objectID": "lab/w9-lab.html#librarynlme",
    "href": "lab/w9-lab.html#librarynlme",
    "title": "Lab 9: Mixed-effects models for time series",
    "section": "library(nlme)",
    "text": "library(nlme)\nRecall at the beginning of the course I had mentioned that R has many flexible extensions that can be added on to the base package. You can load a package with the commands that you will need using the library() command. Do be aware that if you are installing R on your own computer, you may need to intall the package first (get it from the internet onto your disk drive; one time only) and then load it with the library() command (get it from the disk into the computer memory; each session).\nThere are a variety of ways to fit and evaluate linear mixed effects models. Today we will use commands in package nlme, written by Douglas Bates and Jose Pinheiro. The computational underpinnings are rock solid. There is also an updated package lme4 that I use frequently (especially the lmer() model fitting command), but we will not use that today.",
    "crumbs": [
      "Labs",
      "Module 3: Integrating time into our models",
      "Lab 9: Mixed-effects models for time series"
    ]
  },
  {
    "objectID": "lab/w9-lab.html#the-model-fit",
    "href": "lab/w9-lab.html#the-model-fit",
    "title": "Lab 9: Mixed-effects models for time series",
    "section": "The model fit",
    "text": "The model fit\nIn the nlme package, linear mixed effects models are fit with the lme(). Not surprisingly, it looks very similar to something we have seen before. This is how to fit a model with mixed effects:\n\nDecide what to call your model. E.g.,\n&gt; fm1 &lt;-\nSpecify the lme() command, and determine what y variable you are analyzing. It should be something that is, or can be transformed to be, normally distributed.\n&gt; fm1 &lt;- lme(y\\(\\sim\\)\nList the variable(s) that you are analyzing and the dataset. For example, &gt; fm1 &lt;- lme(y\\(\\sim\\)time, data=mydata\nSpecify the random effects. These are usually byproducts of how you designed your experiment. Today, we will use a random intercept for each experimental subject chosen at random.\n&gt; fm1 &lt;- lme(y\\(\\sim\\)time, data=mydata, random =\\(\\sim\\)1\\(\\mid\\)person)\n\nAs an aside, if you were using the package, the equivalent command would be specified\n&gt; fm1 &lt;- lmer(y\\(\\sim\\)time + (1\\(\\mid\\)person), data=mydata)",
    "crumbs": [
      "Labs",
      "Module 3: Integrating time into our models",
      "Lab 9: Mixed-effects models for time series"
    ]
  },
  {
    "objectID": "lab/w9-lab.html#evaluating-models",
    "href": "lab/w9-lab.html#evaluating-models",
    "title": "Lab 9: Mixed-effects models for time series",
    "section": "Evaluating models",
    "text": "Evaluating models\nYou will still be able to evaluate \\(F\\) tests and intercept and slope coefficients for the model as before. However, the method in which coefficient estimates are obtained has changed. Now, we use maximum likelihood methods, not ordinary least squares. Just a different mathematical technique, that’s all.\nWithout ordinary least squares, we have lost our ability to calculate \\(R^2\\) values. They simply do not exist. Instead we have something known as information criteria: other measures of fit of a statistical model.\nThe most common information criterion is Akaike’s Information Criterion, originally proposed by Hirotugu Akaike, defined as\n\\[AIC = -2\\ln(L)+2k\\]\nwhere \\(2k\\) is twice the number of parameters (i.e., a penalty for adding more variables to improve the model fit, analagous to \\(R^2_{adj}\\)) and \\(L\\) is the maximized value of the likelihood function for the estimated model. We will not cover this in detail. Similarly, we will not cover other types of information criteria (AICc, BIC, DIC, etc.) but they are all variations on the theme.\nHere are some things you should remember about AIC values and model selection:\n\nAIC values do not give you a test of the model in terms of hypothesis testing. Rather, information criteria are just a tool for model selection (comparing between models).\nAIC values vary. They are not bounded between 0 and 1 like an \\(R^2\\) value.\nThe lower the number, the better the model fit, but you can only compare models with the same response variable and the same data set. For example, you can compare \\(y \\sim x\\) vs. \\(y \\sim w\\) but you cannot compare \\(y \\sim x\\) vs. \\(sqrt(y) \\sim x\\)\nIf your model has random effects, you can only compare models with different nested random effects if each model has the same fixed effects structure. See below for more details.",
    "crumbs": [
      "Labs",
      "Module 3: Integrating time into our models",
      "Lab 9: Mixed-effects models for time series"
    ]
  },
  {
    "objectID": "lab/w9-lab.html#nesting-random-effects",
    "href": "lab/w9-lab.html#nesting-random-effects",
    "title": "Lab 9: Mixed-effects models for time series",
    "section": "Nesting random effects",
    "text": "Nesting random effects\nSome experimental designs incorporate nesting. In the social sciences, you may see the term ‘multilevel models.’’ We know that a good rule for experimental analysis is to always analyze an experiment consistent with the manner in which it was designed.\nPretend that you select three regions, and then four plots within each of those regions, before you sample the height (\\(y\\)) and diameter (\\(x\\)) of 10 trees each. Your analysis should reflect the regional and plot-within-region variation. Nesting random effects is done with the forward slash in R:\n\n&gt; fm1 &lt;- lme(y ~ diameter, data=mydata, random = ~ 1 | region/plot)\nWe are now presented with some philosophical choices. You could just leave the analysis at that. Another prevalent school of thought encourages statistical models to be as simple as possible. For example, if the plot-to-plot variation in a site is not significant, perhaps we can just set that aside. Hence, we frequently test to see if we can remove the lowest level of nesting. We may start by analysing the experiment consistent with the manner in which it was designed, but then see if we can make it simpler.\nTo test whether we can remove the lowest level of nesting, we fit both models and compare them using a likelihood ratio test. (This is analagous to the additional sums of squares tests that I showed in class).\n&gt; fm1 &lt;- lme(y ~ diameter, data=mydata, random = ~ 1 | region)\n&gt; fm2 &lt;- lme(y ~ diameter, data=mydata, random = ~ 1 | region/plot)\n&gt; anova(fm1, fm2)\nNote that both models have the same response variable and the same fixed effects structure. The output from the anova() command specifying both models separted by a comma will produce a likelihood ratio test with an associated \\(p\\)-value. If the \\(p\\)-value is less than 0.05, one model is statistically better than the other. In that case, choose the model with the lowest AIC value.\nIt is not terribly common to have nesting in time series data, but it can happen (e.g., taking repeated height (\\(y\\)) measurements through time (\\(x\\)) for the trees that are nested in different sites, as above). We do not have nesting in today’s lab.",
    "crumbs": [
      "Labs",
      "Module 3: Integrating time into our models",
      "Lab 9: Mixed-effects models for time series"
    ]
  },
  {
    "objectID": "lab/w9-lab.html#the-last-word",
    "href": "lab/w9-lab.html#the-last-word",
    "title": "Lab 9: Mixed-effects models for time series",
    "section": "The last word",
    "text": "The last word\nIt can be a little confusing to remember what can be compared with what (similar fixed effects models with changed nested random effects, different fixed effects models without random effects, etc.) because the likelihood world has all sorts of little nuances. I keep the above sections as simple as possible without going into detail about the different likelihood computations behind these models (maximum likelihood, restricted maximum likelihood, penalized quasi-likelihood, etc.).\nBottom line: model selection is a bit of an art. What I might do is start with the most consistent model with my experimental design, find the best fixed-effects structure, and then simplify the random effects. Other times, I might start with a model without any random effects at all, find an OK model, and then put the random effects back in to see if things still work. There is no recipe book, unfortunately.",
    "crumbs": [
      "Labs",
      "Module 3: Integrating time into our models",
      "Lab 9: Mixed-effects models for time series"
    ]
  },
  {
    "objectID": "lab/w9-lab.html#reading-the-output",
    "href": "lab/w9-lab.html#reading-the-output",
    "title": "Lab 9: Mixed-effects models for time series",
    "section": "Reading the output",
    "text": "Reading the output\nThe anova() and summary() commands work as before. Once you have a good model, to get detailed information on model coefficients, use the summary() command.\nThe summary will appear similar to tables that you have seen before, with a few twists. First, you will see summary statistics for AIC, BIC, and the log likelihood.\nNext, you will see a listing of random effects. Random effects are often reported when you specify a model, but not in the resulting equation of the line (see last question today).\nLet’s say you were working with blood pressure readings on different people, perhaps in different treatment groups. You might see this output from a model \\(y \\sim time + trt\\) with a random effect for person:\nRandom effects:\n Formula: ~1 | person\n        (Intercept) Residual\nStdDev:    8.917896 5.032072\nThis would specify that the resulting expected blood pressure value \\(\\hat{y}\\) (for a given timepoint and treatment group) could be adjusted at random up or down by a standard deviation of 8.9 to account for variability among people. Then, on top of that, there is another random adjustment of \\(\\pm\\)5.03 standard deviations for all noise left over (so, if blood pressure reading is the experimental unit, variation among readings within each person).\nFinally, you will see a fixed effects table - essentially your model summary. (Ignore the table of intraclass correlations below that during today’s lab). You will also see information on the distribution of residuals, which can be used (along with a graphical residual plot!) to determine if the assumptions of your model are being met.",
    "crumbs": [
      "Labs",
      "Module 3: Integrating time into our models",
      "Lab 9: Mixed-effects models for time series"
    ]
  },
  {
    "objectID": "lab/w9-lab.html#check-your-residuals",
    "href": "lab/w9-lab.html#check-your-residuals",
    "title": "Lab 9: Mixed-effects models for time series",
    "section": "Check your residuals!",
    "text": "Check your residuals!\n&gt; plot(residuals(fm1)\\(\\sim\\)fitted.values(fm1))\n&gt; plot(fm1) # Finally, a shortcut!",
    "crumbs": [
      "Labs",
      "Module 3: Integrating time into our models",
      "Lab 9: Mixed-effects models for time series"
    ]
  },
  {
    "objectID": "lab/w8-lab.html",
    "href": "lab/w8-lab.html",
    "title": "Lab 8: Time series: Autoregressive models",
    "section": "",
    "text": "Time Series\nAnalysis of time series is typically taught as a one-semester course, providing exposure to autocorrelation functions, partial autocorrelation functions, detrending data, spectral approaches, moving average (MA), autoregressive moving average (ARMA), autoregressive integrated moving average (ARIMA), and more. Today we will play with autoregressive (AR) models. This is where most time series courses begin.\n\n\nAutoregressive models\nTo this point in the course, we have been doing variations on a theme of explaining some continuous response variable \\(y\\) by some response variable \\(x\\). \\[y \\sim x\\] We have learned that \\(x\\) can be continuous (i.e., regression) or a factor (i.e., ANOVA). We can even add more continuous variables (i.e., multiple regression) and mix continuous variables and factors (i.e., ANCOVA).\nNow we’re going to put in a twist. Auto of course means self, so autoregression is regressing a variable on itself. \\[y \\sim y\\] Huh?\nWell, yeah, something like that. Actually, the covariate is the variable shifted backwards in time (lags, denoted as \\(k\\)). The general form of an AR(\\(k\\)) model can be written:\n\\[y_t = \\beta_0 + \\beta_1y_{t-1} + \\beta_2y_{t-2} + \\ldots + \\beta_ky_{t-k} + \\epsilon_i\\]\nwhere \\(y\\) is the variable of interest, \\(t\\) denotes time, \\(k\\) the time lags (i.e., what happened today depended on what happened yesterday, and perhaps two days ago, and\\(\\ldots\\)), and we assume the remaining errors \\(\\epsilon_i\\)~\\(N(0,\\sigma^2)\\).\nNow, there are alternate ways to parameterize an autoregressive model. Although we will deal with modeling the temporal dependence in the mean term in today’s lab, another common parameterization includes modeling the temporal autocorrelation in the error term. Below, I have written a “first-order autoregressive model”, or AR(1), where the principal dependence structure is related to a single-step time lag. For now, this is FYI only, although we will return to this concept as we work with lattice and areal data in spatial statistics in a few weeks.\n\\[y = \\beta_0 + \\beta_1x + e_i\\] where\n\\[\\begin{aligned}\ne_i &=& \\rho e_{i-1} + u_i,\\quad i=1,2,\\cdots,n,\\\\\ne_0 &\\sim& N\\left(0, \\frac{\\sigma^2}{1-\\rho^2}\\right),\n\\end{aligned}\\]\nand we assume \\(u_i\\sim {\\rm iid} N(0,\\sigma^2)\\) and \\(0&lt;\\rho&lt;1\\). For example, \\(e_{10} = \\rho e_{9} + u_{10}\\).\nA property of an AR(1) model is that \\[cor(e_i, e_j) = \\rho^{|i-j|},\\] where \\(0&lt;\\rho&lt;1\\).\nTwo things to note from the above structure:\n\nIf \\(\\rho\\) is close to 0, the temporal correlation is small.\nIf two time points \\(i\\) and \\(j\\) are far apart, the temporal correlation is small.\n\nIf you have a hard time understanding such notation for now, that’s OK; it will become more clear by Lab 12. In this lab, we’ll focus on modeling the temporal autocorrelation in the mean term.\n\n\nToday’s Data\nToday’s data is stored in Lab8data2024.txt. It is a tab-delimited text file with appropriate headers. The data are the daily temperatures at the Minneapolis St. Paul airport (weather station KMSP) from 01 January through 13 March 2024. They were obtained from the National Oceanic and Atmospheric Administration https://www.weather.gov/mpx/mspclimate on March 14, 2024. There are five columns: Month, Day, Julian Day, Max, and Min (in degrees Farenheit). The goal of today’s lab is to describe the daily temperatures with an appropriate model.\n\n\nAssignment\n\nLoad the data, check a summary, and, of course, graph it. Let’s work with the maximum daily temperature. Plot maximum daily temperature vs. Julian day.\n&gt; plot(Max\\(\\sim\\)Julian, data=mydata)\nThis graph indicates to me that spring is coming! It appears that the daily maximum temperature increases with the passage of time since January the 1st, as we might expect. It looks like a rising trend, so fit a straight line to the data:\n&gt; fm1 &lt;- lm(Max\\(\\sim\\)Julian, data)\n&gt; summary(fm1)\nYou can plot the fitted line on top of the original plot like this:\n&gt; lines(fitted.values(fm1))\nHmm. The straight line trend looks like it fits pretty well, but I should of course take a look at the residual plot.\n&gt; plot(residuals(fm1)\\(\\sim\\)fitted.values(fm1))\nThe spread around zero is beautiful, with roughly equal variance across the plot from residuals less than 20 vs. greater than 20. I do not think I need a transformation of the response variable. (If you want to push yourself (not for homework): If you were working with minimum temperature data and wanted to use a transformation, could you use a square root transformation on these data? Why or why not? How might you proceed?).\nSince this is a lab focused primarily on assumption \\(\\#2\\) of linear models (i.e., errors are independent!), we’ll just pretend the assumption of equal variance is good enough for now.\nNow let’s explore the thorny issue of autocorrelation. Have I mentioned that these are real data?\nThe current model examines daily maximum temperature as a function of time. Time is placed in the ‘mean’ part of the model. Sometimes, simply specifying the temporal variable in the mean part of the model will explain away all temporal autocorrelation. Our goal, when working with temporal data, is to end up with uncorrelated errors (model assumption #2: errors are independent!).\nWe will use a Durbin-Watson test to check model residuals for temporal autocorrelation. It’s very simple.\nLoad the car package written by John Fox, a Professor Emeritus in the Department of Sociology at McMaster University. Prof. Fox had along and fruitful career developing regression methods in statistics. His package (“Companion to Applied Regression”) contains the necessary function.\n&gt; durbinWatsonTest(fm1)\nThis function tests the residuals of a model (in this case, fm1). In the output, you will see four things: a lag term (1 indicates the previous time step), an autocorrelation value, a D-W statistic testing whether that autocorrelation estimate is significantly different from zero (i.e., uncorrelated!), and a \\(P\\)-value. A significant \\(P\\)-value indicates that there IS significant autocorrelation remaining in the residuals of the model.\nTest the residuals for autocorrelation. What is the outcome? (Don’t read the next sentence while you answer this question).\nBummer. (Hey! I told you not to read this sentence!) Well, I thought the graph and the model looked ok, even if the \\(R^2\\) wasn’t very high (\\(F_{1,71}=39.54; P&lt;0.0001; R^2=0.358\\)). But now we know that modeling time (as Julian date) in the mean term did not quite take care of the autocorrelation in the data. (Literature terminology: when you place a variable in the mean term that successfully explains the autocorrelation, this is sometimes known as whitening the trend).\nLet’s change approaches to see if we can come up with a good explanation of daily maximum temperature, and still come up with uncorrelated errors. We will try an autoregressive model. With this model, we only deal with the \\(y\\) variable and derivations of itself (although you could add other variables, I suppose).\nIn constructing an autoregressive model, we first check the autocorrelation function:\n&gt; acf(data$Max)\nThe autocorrelation function measures the correlation of the series with itself lagged \\(k\\) time steps. You will see a correlation scale on the \\(y\\) axis ranging between \\(+1\\) and \\(-1\\). On the \\(x\\) axis, you will note integers for increasing previous time steps, or lags. So, for example, if you were to see a vertical line corresponding to \\(+0.5\\) at \\(k=3\\), that would indicate that the time series at time \\(t\\) (often denoted \\(y_t\\)) appears to be positively correlated with a copy of itself at \\(y_{t-3}\\) (three steps previous). This implementation of the function even includes some nifty blue dotted lines that shows statistical significance of the correlation (anything within the blue lines is statistically indistinguishable from zero).\nWhy does the autocorrelation at lag \\(k=0\\) equal one?\nThe acf() function shows us the autocorrelation through time. There are, of course, many different ways that autocorrelation can propogate through time. For instance, if everything looks relatively similar to the last previously observed time, we might expect that the correlation would “stick around” for \\(2,3,4\\ldots\\) time steps until it decays and disappears. This behaviour characterizes a very popular class of autoregressive models termed AR(1), or first order autoregressive. (Please note: This is also what some SAS software users call ‘repeated measures.’ But you can see that ‘repeated measures’ is something measured repeatedly, and the temporal dependence in the data is a separate issue! The autocorrelation structure need not be first order, even though it is a very frequent phenomenon. Do not confound measuring something repeatedly with specifying one type of autocorrelation).\nFor first-order autoregressive models, the autocorrelation \\(\\rho\\) decays exponentially \\(\\rho^k\\) at the \\(k^{th}\\) lag. So, if the autocorrelation at lag 1 is 0.4, we might expect that it would decay to \\(0.4^2=0.16\\) and \\(0.4^3=0.064\\) at time lags 2 and 3 (and so on). If you plot the autocorrelation function and see a gentle positive decay through several time lags, you are probably staring at first order autocorrelation.\nWe have all heard arguments about how unreliable weather forecasts are. From looking at your acf() graph, in a general sense, from only having a two and a half month list of daily temperatures, what is the farthest out in time that you could make a statistically relevant guesstimate of that day’s maximum temperature?\nOf course, there are many different orders of autocorrelation. Many time series of organisms that display oscillating population dynamics show second order autocorrelation, for example.\nWhile the acf() function provides some strong diagnostic hints for determining order, a helpful way to diagnose order is to examine the partial autocorrlelation function, pacf(). A partial autocorrelation function displays the correlation left at time lag \\(k\\) after removing autocorrelation with an AR(\\(k-1\\)) model. So, for example, if you have an AR(1) model, you would expect the autocorrelation at \\(k=2\\) to be non-significant once you account for “what happened yesterday” (\\(k=1\\)).\nSome students find it difficult to grasp the difference between the acf() and pacf() functions. To review, you can think of the above acf() and pacf()functions as a two step process that works in tandem. First, is there autocorrelation? What does it look like? (Does it decay slowly like something contagious passed down from day to day, does it bounce back and forth so an immediate time step looks diametrically opposite the previous one, etc.) Second, what is the order of the autocorrelation? How far back does the time series look like itself? (These questions are rhetorical - do not answer them for the homework). Put another way, the acf() can describe the pattern. The pacf() can tell you what is causing that pattern.\nPlot the partial autocorrelation function:\n&gt; pacf(data$Max)\nThe dashed blue lines indicate significance values. Whoa! If you are looking at the same chart I was, I didn’t expect to see that. It’s always tempting as an instructor to change a few points and “fix” the assignment - but then you wouldn’t really be getting an education, would you? There are a couple of curiosities in this graph.\nDescribe for me exactly what this chart is telling you about trends in the data, and, in a general sense, what you think of that information.\nOnce we know the order, we can lag the data and regress \\(y_t\\) on \\(y_{t-k}\\). Create two new columns with the maximum temperature lagged one day and then two days. Be careful when doing this! It’s easy to lag the data in the wrong direction. You can either do this in a spreadsheet, or in R:\n&gt; data$MaxLag1 &lt;- c(NA, data[1:(nrow(data)-1), \"Max\"])\n&gt; data$MaxLag2 &lt;- c(NA, NA, data[1:(nrow(data)-2), \"Max\"])\n&gt; data[1:5,] # Check the first five rows\nNotice that the first \\(k\\) observations are always lost, so when doing this type of time series analysis, there is a limit to how many lags you can reasonably study, especially with a very short data series. While there might be statistical rationale from the pacf() graph to explore a time lag more than five or seven days out, I would not do that here.\nPlot the Max temperature against its lag variable of \\(k=1\\). By eye, describe what the correlation looks like (if any).\nRegress the maximum temperature on it’s \\(k=1\\) lag (in other words, maximum temperature as a function of the temperature the day before). If you left the missing value for the first observation’s lag in the data set, you will see a new message that appears on a line between the Residual standard error and the Multiple R-squared lines in the summary() command.\nComment on the \\(R^2\\) and the apparent overall significance of the model. Remember, the inference from some of the test statistics might be slightly off if there is significant autocorrelation, so view the stats with some caution. Examine the residual plot, and \\(\\ldots\\)\nCheck the D-W statistic. Is there any temporal autocorrelation left after accounting for \\(y_{t-1}\\)? Is an AR1 time series model for maximum temperature necessary?\nIf you want to see your work graphed, with the \\(\\hat{y}_t\\):\n&gt; plot(Max\\(\\sim\\)Julian, data)\n&gt; lines(fitted.values(yourAR1model))\nDo note that the equation predicting the daily maximum temperature is not the same as the plot function, which is just graphing two variables for you; i.e., your final model is not\n\\[temperature = \\beta_0 + \\beta_1day\\]\nIt can be kind of confusing when you are graphing a model that uses a variable not on the \\(x\\)-axis per se.\nNow, find and report a good model (time series or not) for minimum daily temperature. When reporting a model, be sure to report the equation of the line and define the variables and their units. For an example of how to report model summary statistics, see question #3. Have fun!",
    "crumbs": [
      "Labs",
      "Module 3: Integrating time into our models",
      "Lab 8: Time series: Autoregressive models"
    ]
  },
  {
    "objectID": "lab/w1-lab.html",
    "href": "lab/w1-lab.html",
    "title": "Lab 1: an introduction to R",
    "section": "",
    "text": "Download lab data  Download R project file",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#frontmatter",
    "href": "lab/w1-lab.html#frontmatter",
    "title": "Lab 1: an introduction to R",
    "section": "",
    "text": "Download lab data  Download R project file",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#r",
    "href": "lab/w1-lab.html#r",
    "title": "Lab 1: an introduction to R",
    "section": "R",
    "text": "R\nR is a software environment for statistical computing and graphics. It’s also a word that pirates say, but that’s not important right now. R is the best statistical software available, is free, and runs on Windows, Macintosh, and Linux platforms. R may be downloaded for personal use from http://www.r-project.org. R is open-source, which means that many people work on it around the world and are constantly improving it. Updated versions are released on the website every 2 months or so. If there is an advancement in statistics, it is incorporated rapidly.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#installing-packages",
    "href": "lab/w1-lab.html#installing-packages",
    "title": "Lab 1: an introduction to R",
    "section": "Installing packages",
    "text": "Installing packages\nLike ecology, there are many subdisciplines of statistics. R comes as a ‘base’ package. Depending on what you want to do, this may be sufficient. Often, however, you will need to install a contributed addition to do more specialized tasks, like spatial or temporal analyses. These additions are called “packages.” Think of statistics as creating a recipe to analyze your data, where R is an oven to bake your creation. Packages, then, are extra features you can add to the oven (e.g., air fryer, stainless steel griddle, extra burner, etc.) if your recipe needs it. You can either use the “Load package” command from the “Packages” menu, or you can simply type\nlibrary(whateverpackageyouneed)\nIn today’s lab, you will not need any extra packages.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#making-mistakes",
    "href": "lab/w1-lab.html#making-mistakes",
    "title": "Lab 1: an introduction to R",
    "section": "Making mistakes",
    "text": "Making mistakes\nR is very powerful, and is not always very friendly to beginners. But if you start slow, and master a few skills early on, you will find that it is not overly burdensome (and you may even find yourself enjoying it).\nWhen R executes a command, you will either see a new command prompt on the next line, or the output of what you requested. If you make a syntax error (i.e., ask R to do something it does not understand), you will get an error. Simply retype the command, correcting your mistake (did you miss a comma, quote, or parenthesis?).",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#getting-help",
    "href": "lab/w1-lab.html#getting-help",
    "title": "Lab 1: an introduction to R",
    "section": "Getting help",
    "text": "Getting help\nFortunately, R has LOTS of help. Here are some good ways to obtain help if you are really stuck:\n\nLook hard at your screen. Is the mistake obvious (see cartoon below)?\nConsult a classmate.\nAsk me.\nConsult a help manual or book.\nAsk the internet.\nAsk ChatGPT. In all seriousness, troubleshooting error codes is an excellent use of GenAI tools as it can quickly aggregate information from a bunch of different sources that you might otherwise find yourself after great effort. The key here is, once you find the error/solution to the issue, internalizing what exactly it is that went wrong and why.\nType in a question mark before the command you are trying to use. For example, ?summary provides help on the “summary” command. This is a good way to find all of the arguments that may be used with a command, for example. Unfortunately, you may find that the help files are most useful if you are already experienced with R.\nAsk the R-help mailing list (the email address can be found through the R website, www.r-project.org). You will generally get a response in an hour or less, at any time day or night, because there are millions of useRs around the world. Often, however, the answers are provided by experienced R users, who may speak more Math than English and may have maladapted social skills.\n\n\n\n\n\n\n\n\nPro tip: 95.6% of the time, a silly spelling or syntax error is the reason behind your code is not working\n\n\n\nPro tip: Keep notes!\nIt is a good idea to keep a notes while you work. As our analyses get progressively more difficult this semester, you will find yourself needing commands from earlier in the semester. If you make your own notes of tricks and techniques that you know worked (e.g., what was the simple command I used to read in data? can I copy and paste it later?), it can avert lots of headaches in the future. What I do with every single project I am working on, is create a project_notes R Notebook file. I treat this like a digital lab notebook, recording what I do each day, issues I’m running into, ideas for future analyses or figures to make, a checklist of tasks I have yet to accomplish, and as a sandbox to play with analysis ideas/building skeletons of functions.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#working-with-batch-files-r-scripts",
    "href": "lab/w1-lab.html#working-with-batch-files-r-scripts",
    "title": "Lab 1: an introduction to R",
    "section": "Working with batch files (“R scripts”)",
    "text": "Working with batch files (“R scripts”)\nA common method of statistical analysis uses “batch files.” This is a rather old, but effective, way of conducting analyses. Essentially, one writes the programming code and sends it to the statistical software, either line by line or in large chunks. The computer runs the code, and then gives the output. That could include error messages (hopefully not!), or the answer to your analysis. The really nice thing about a batch file is that you can save it for later, and always rerun the analysis later. Think of it like writing a recipe (batch file) before putting your creation into the oven (R).\\ The most common batch file interface in R is RStudio available from http://www.rstudio.com. RStudio is a free and open source integrated development environment (IDE) for R. You can run it on your desktop (Windows, Mac, or Linux) or even over the web using RStudio Server.\\ With RStudio, you can write your batch code on one window, and then send directly to R that is running in another window (Ctrl-Enter to send a single line). You can easily save and reload your analyses later by saving the batch file with a “.R” file extension. RStudio also has the advantage of syntactic highlighting. That is, when you mouse over a parenthesis, quotes, etc., the program will highlight its partner. It is an easy way to keep track of missing punctuation that can drive a computer program (and the user!) crazy.\\ Open R studio and create a new R script (File/New File/R script…). First, save it somewhere so that you can find it back later (File/Save As…). The file will have a .R file extension that designates it as an R coding script file. You should now have a blank file named iLOVElearning.R or similar. Now close it (File/Close…).",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#sending-code-to-r",
    "href": "lab/w1-lab.html#sending-code-to-r",
    "title": "Lab 1: an introduction to R",
    "section": "Sending code to R",
    "text": "Sending code to R\nOpen your blank file again (File/Open…). See how easy it is to pick up where you left off? In the top of the blank file, write:\n2+2\nWhile your cursor is on the 2 + 2 line, hold down Ctrl and then press Enter to send the code from the script to R below. In the R console, you should see\n&gt; 2+2\n[1] 4\nChange your code to a different instruction, such as 2 + 3, and send that to the R console. Easy, eh? Save your script file, then close it.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#annotating-code",
    "href": "lab/w1-lab.html#annotating-code",
    "title": "Lab 1: an introduction to R",
    "section": "Annotating code",
    "text": "Annotating code\nWhen writing code, it is helpful to include comments to yourself so that you can remember what you were doing, why, when, and so on. In R, you can comment anything by first using a # sign. Anything after the # sign is not executed. So, if you were to type in:\n3 + 3\nR would spit out\n&gt; 6\nAnd, if you were to type in\n3 + 3   # My first attempt at R, January 19, 2024\nyou would also simply get\n&gt; 6\nOpen the file that you just closed. Write as follows:\n#####################\n## I am learning R\n## This is lab one\n## My name\n## January 19, 2024\n#####################\n​\n## My first code starts here ##\n2+2\nThis is good practice when analyzing any data set. My code is always annotated where I read in data, conduct exploratory data analysis, manipulate variables, and so on. Do the same for your homeworks, and your thesis/dissertation analyses. Your future self will thank you!\nKeep in mind, comments are meant to clarify your code/objects in your code, not act as an extensive note-taking tool. Start a project_notes file for stuff like that! Comments should help either future you, or someone else who has never met you, run your code successfully and understand its purpose.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#object-oriented-programming",
    "href": "lab/w1-lab.html#object-oriented-programming",
    "title": "Lab 1: an introduction to R",
    "section": "Object-oriented programming",
    "text": "Object-oriented programming\nR uses something called “object-oriented” programming. This means that you assign names to various pieces of your data and analyses. You can name things whatever you want to help you keep track. Analyses are performed on these objects, one command at a time through something called a “command line interface.” It’s like a batch file in slow motion: if there is an error, you will see it instantly.\\ To assign names to various pieces of your data or analyses, simply use an arrow (&lt;-), or an equal sign if you prefer. This will become clear below.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#commands-in-r",
    "href": "lab/w1-lab.html#commands-in-r",
    "title": "Lab 1: an introduction to R",
    "section": "Commands in R",
    "text": "Commands in R\nMost commands in R use parentheses to specify the arguments (arguments are like options). If there are no arugments, the parentheses are still necessary. A common syntax error is forgetting the right parenthesis. One command you will might use to close R at the end of the session (if you don’t want to use the file menu) might be quit(), for example.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#entering-data-by-hand",
    "href": "lab/w1-lab.html#entering-data-by-hand",
    "title": "Lab 1: an introduction to R",
    "section": "Entering data by hand",
    "text": "Entering data by hand\nR is like an overgrown calculator, really. Simple calculations are straightforward, and you can sometimes guess at any commands or operators you need (like sqrt()). You can stick numbers together using a technique called “concatenation.” To stick the numbers together in a “vector” (list of numbers, like a spreadsheet column), name the vector, and use the concatenate command:\nmyniftyvector &lt;- c(1,2,3,4,5)\nSend that to the R console with Ctrl+Enter. Then if you type and send\nmyniftyvector\nyou will get\n[1] 1 2 3 4 5\nYou have just created your first object!",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#entering-data-into-r",
    "href": "lab/w1-lab.html#entering-data-into-r",
    "title": "Lab 1: an introduction to R",
    "section": "Entering data into R",
    "text": "Entering data into R\nOk, seriously, if you have lots of data, you are not going to enter it like that. An easy way is to transfer it from a spreadsheet. When R reads a spreadsheet into memory, it goes into what is called a “dataframe.” Dataframes are essentially a collection of “vectors” or lists of numbers. So, each variable and its column is a vector, and all of them side-by-side in the spreadsheet make a dataframe.\\\n\nLoading Excel files directly into R\nDisclaimer; I do not use or recommend this method because there are a few “point and click” steps that are difficult to reproduce if you forget a step. I include it because many people use this method and is simple for small datasets.\\ R Studio has a tab labeled “Import Dataset” from which one can choose importation from an Excel spreadsheet. Once the file is identified, the data will be shown in a Data Preview screen. You can make adjustments (identifying rows or columns to exclude, for example), by editing options or clicking buttons at the bottom of the screen under Import Options. Notice that as you change options (e.g., excluding the first row as column names), the code that imports the data (bottom right pane) changes.\\ In the interest of reproducible results, I start with a simple text file and write the accompanying code to load that data set directly. Then I never have to worry about changes to the recipe. That method is shown below.\\\n\n\nLoading plain text files directly into R\nAt the bottom of every Excel spreadsheet are tabs. I keep three tabs in every Excel spreadsheet.\\ In the first tab, I keep all metadata or records of what the variables mean, how the data were collected, and so on. The second tab might contain the raw data as collected, and might be a little messy, perhaps with a final column of notes. The third tab is a cleaned version of my data in logical or rectangular record format, with proper column headers (just text, no special characters). To avoid problems, keep column titles to one word each. Make sure each one is different. Pay attention to capitalization vs. small case: if your spreadsheet column is named “Year,” then R will not understand if you later ask for the column named “year.” Also, avoid special characters in your column titles.\\ When preparing to import data, I first save my spreadsheet as as a regular Excel file so that Excel saves all of the tabs. I then go to the final tab where I have my cleaned data, ready for import and analysis. For this tab, I save it as a plain, tab-delimited text file (File/Save As… then choose file type as Text (tab delimited) (*.txt).\\ To load data into R, I use the read.table() command, for example:\nmyniftydata &lt;- read.table(\"D:/path/data.txt\", header=T, sep=\"\\t\", na.strings=\".\")\nBy using object-oriented programming, your data set can be referred to simply as myniftydata from now on. The read.table() command reads data files. Inside the parentheses, you provide the arguments separated by commas.\\\n\nThe first argument is the path to the data file. Enclose the path in quotes. You can make this whatever you’d like; R can read data off a network drive (H:/), your USB memory stick, or even the internet (instead of a file name, use http://www.whateverpath.com/file.txt).\\\nThe header argument is a logical True or False that indicates whether your data set contains column titles. You may abbreviate the answer to simply T or F.\nThe sep command specifies the delimiter between columns. You can change this as needed, of course. Space delimited files (sometimes called *.dat files) are sep=\" \" while comma separated files (often named *.csv files in Microsoft Excel) are sep=\",\". Because there is no “tab” character on the keyboard (like a space or comma), tab-delimited files are specified in a special way: sep=\"\\``t\".\nThe na.strings=\".\" argument tells R what denotes your missing data. A period is a good symbol to use, although sometimes asterisks or special numbers are used. In climatological data, -9 or some similar identifier is frequently used. R will take these values and change them to NA.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#checking-data",
    "href": "lab/w1-lab.html#checking-data",
    "title": "Lab 1: an introduction to R",
    "section": "Checking data",
    "text": "Checking data\nOnce you get the data read into R, it is a good idea to check the data. There are several commands you could use, including:\\\n\ndim(myniftydata) returns the number of rows and columns in the dataframe.\nnrow(myniftydata) returns the number of rows.\nncol(myniftydata) returns the number of columns.\nhead(myniftydata) returns the first five rows.\ntail(myniftydata) returns the last five rows.\n\nPerhaps the most useful command is\nsummary(myniftydata)\nThis command summarizes the dataset you have just input (or whatever object happens to be specified - it is a very versatile command). For a dataset, R will list all the different variables, along with brief summary statistics for each. This is the best command to spot problems. All numerical columns will appear with summary statistics min, max, mean, etc. All columns with text, that are often indicative of categorical variables (next week’s lab), will appear with the number of observations and type of variable (i.e., character). (Value added: if you or one of your classmates knows how to convert a character column to a categorical factor variable right now, note how the summary() output changes!). If you input numerical data and see a frequency list from a summary command, there is a good chance that there is a nonnumber somewhere in the column. Check the original data, correct the mistake, and try again.\\",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#sub-setting-data",
    "href": "lab/w1-lab.html#sub-setting-data",
    "title": "Lab 1: an introduction to R",
    "section": "Sub-setting data",
    "text": "Sub-setting data\nQuite often, we want to perform an operation or analysis on only one portion of the data. R uses indexing to select variables, in a scheme like this:\nmyniftydata[rowsIwant, columnsIwant]\nIt’s relatively simple to select various rows or columns. You can either name them, or use the indexing position. A comma separates the row from the column arguments.\\ &gt; myniftydata[1:5,2] selects the first 5 rows in the 2nd column.\\ &gt; myniftydata[-3,2] selects all rows EXCEPT the 3rd, in the 2nd column.\\ &gt; myniftydata[,2:6] selects all rows and columns 2 through 6.\\ &gt; myniftydata[3:5,] selects rows 3 to 5 of all columns.\\ &gt; myniftydata[1:5, c(\"year\",\"trial\")] selects the first 5 rows of two columns named “year” and “trial.”\\ The commands above use base R. Packages such as tidyR have exploded in popularity in the last decade and use a slightly different syntax. Our TA, Audrey, is a great resource for questions about the Tidyverse!\\",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#objects-in-memory",
    "href": "lab/w1-lab.html#objects-in-memory",
    "title": "Lab 1: an introduction to R",
    "section": "Objects in memory",
    "text": "Objects in memory\nYou can always view objects in memory using the ls() command. To delete a specific object from memory, such as mydata, type remove(mydata). Generally, you should not need to do this unless you are working with multiple copies of HUGE datasets and R tells you it has run out of computer memory. To remove all objects from memory, type remove(list=ls()).\\",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#data-sets-for-assignments",
    "href": "lab/w1-lab.html#data-sets-for-assignments",
    "title": "Lab 1: an introduction to R",
    "section": "Data sets for assignments",
    "text": "Data sets for assignments\nThe datasets for each class may be found on Canvas. Download the file to your workspace (e.g., USB drive, network space, laptop, etc). You may want to create a folder for this class named ENT5126 or iLOVEstats or something similar and putting all data files there (including your R scripts). Then you can use this file path:\\ \"D:/ENT5126/whateverdatafile.txt\" (change the drive, depending on where your data resides). Note that when specifying data pathways in the read.table() command, the slashes must be forward!",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w1-lab.html#assignment",
    "href": "lab/w1-lab.html#assignment",
    "title": "Lab 1: an introduction to R",
    "section": "Assignment",
    "text": "Assignment\nData for a rodent toxicology study are stored in a spreadsheet named Lab1data.xls. The assignment is simple (?): open the data, clean it up, and import it into R.\\ Please\\\n\nList all of the errors that you have found in the data.\nProvide a copy of a printout of the spreadsheet containing the metadata.\nProvide a copy of a printout of the data in rectangular record format, ready for export to R.\nProvide a printout of the summary() command for the data from R, once you have read it in.\nPretend that were working with European data where the decimals in the pulse column were not periods but rather commas. How would you adapt the read.table() command?",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 1: An introduction to R"
    ]
  },
  {
    "objectID": "lab/w13-lab.html",
    "href": "lab/w13-lab.html",
    "title": "Lab 13: Geostatistics",
    "section": "",
    "text": "Introduction\nYou made it! Today’s lab is the last lab of the course (Sniff!) save for the lab exam (Whoopee!). We will cover one aspect of geostatistics today: fitting variograms.\nMany principles of lattice/areal data and spatial linear models apply to geostatistics. Both types of analyses model the spatial dependence in the error term.\nFor lattice or areal data, our spatial error models are more constrained, and thus in one sense “simpler.” Lattice methods apply where the data represent regions, rather than points, and for problems where there is a clear neighborhood structure thought to influence the process under study. In that sense, lattice or areal data give us more opportunity to construct scientifically meaningful spatial correlation models (through the neighborhood structure). For geostatistical models, the spatial error model is sometimes easier to choose, and we have a few more tools at our disposal for choosing models.\nThe boundaries between lattice and geostatistical modeling can be blurred. For example, if we are studying islands and our neighborhoods are defined in terms of inverse distance, then it is not always clear that lattice models are the most appropriate. At the same time, it is not completely clear that geostatistical approaches are the best in such cases.\n\n\nGeostatistics\nWe will focus on ordinary kriging today. That is, we will assume there is some constant but unknown mean. In other words, there is just an intercept, that’s all. This is the most common type of kriging. Of course, there is also universal kriging - the mean response depends on not only a constant but also covariates (or factors if an ANOVA). Universal kriging is a bit tricky; you must estimate the trend in order to obtain the spatial dependence in your residuals (i.e., to fit a variogram), yet the estimates of spatial error (i.e., nugget, range, sill…) also influence the estimates of your mean terms (e.g., the slopes of covariates in which you may be interested). I think you can see the challenges of circular logic here!\nUnfortunately, this is often ignored as a practical problem. If the goal is simply spatial prediction at new points, the problem is not too severe. If the problem is inference on your covariates (e.g., does soil depth and/or elevation affect probability of finding water?), the challenge is not trivial. The best theoretical routes in these cases involve Bayesian methods. We will not cover Bayesian methods in this lab, as they are a relatively advanced topic, and we are nearing the end of the course.\nThere are three major packages for dealing with geostatistical data. The first is geoR. It has some great functions for fitting variograms and fitting confidence envelopes around them, similar to putting confidence envelopes around our Ripley’s K functions in a previous lab. Another popular package is gstat. The third is a package that we have used before, nlme, which can be used to model different types of random effects, such as spatial error models in geostatistics. We will use it again today so please load nlme now.\n\n\nToday’s assignment\nToday’s data set is called “wells.” I do not know very much about it. It has 3 columns: \\(x\\), \\(y\\), and \\(z\\). I do not have metadata for it, although it has been used quite a bit as a training tool for exercises in ordinary kriging. I guess it also provides a valuable lesson in safeguarding your data and maintaining the metadata! Assuming this is real data, and someone did spend money collecting it, it is proof that data can walk away in a hurry if you are not careful!\nLet’s pretend the dataset contains concentrations of nitrate (NO\\(_3^-\\)) in drinking water (the z column) at different x and y locations on a \\(1 \\times 1\\) km grid. The Minnesota Department of Health (MDH) recommends that there should be no more than 10 ppm nitrate-nitrogen in drinking water.\n\nYou should have just loaded the nlme package. Please also load the scatterplot3d package, which is useful for visualizing data. Also, load the sp package. This package creates uniform classes for all of the different types of spatial data and objects, with a goal of making it easier to switch between different types of analyses.\n\nRead in the data. Let’s first take a look at the spatial trends. Here are a couple of ways (these will also work for visualizing marked spatial point processes):\n\n\n&gt; library(scatterplot3d)\n&gt; scatterplot3d(mydata$x, mydata$y, mydata$z, type=\"h\")\nThe optional type=\"h\" part indicates to draw drop lines to the \\(x-y\\) plane.\n&gt; library(sp)\n&gt; spatialdata &lt;- SpatialPointsDataFrame(coords=\n\\(~~~~~~~~~~~~~~~~~~~~\\)cbind(mydata$x, mydata$y), mydata)\n&gt; bubble(spatialdata, \"z\", fill=F)\nThe fill=F instructs to avoid colouring in the circles.\n\nExamining the bubbleplot, does it look like there is spatial variation in the response? That is, do points that are closer together tend to have the same values?\nRecall that mixed effects models can handle correlated error structures; we used one example of such in our time series laboratory on repeated measures or longitudinal data. Today, we’ll model spatial dependence in the error by using a semivariogram.\nThe command that we used before was lme(). With that command, we needed to specify the model formula, the data, and the random effects (i.e., some sort of a grouping factor). However, with geostatistical data, there is typically no grouping factor. That is, we are looking at only one spatial surface (not multiple people, blocks, etc.).\nInstead of the lme() command, we will use gls(), short for generalized least squares. It behaves a lot like the lme() command, but does not specify a random effect. Instead, it specifies a correlation structure typified by a variogram (more on this later\\(\\ldots\\)). Estimation of model coefficients is still performed via maximum likelihood or restricted maximum likelihood.\nBefore you begin this lab, it may be helpful to construct a chart to keep track of your results. Create six columns for six models (one without spatial dependence, an empirical sample variogram, and four models with two different types of spatial dependence). As you work through the lab, please clearly identify the type of model in the column heading. Complete the table with five rows for values of AIC, intercept, sill, range, and nugget. Of course, as you progress through the lab, you will notice that some rows will not have an entry (e.g., the empirical sample variogram does not have an AIC value).\nFor the first model, fit an intercept to the data using the gls() command (of course, if we were doing universal kriging, you would at this point specify some simple starting model other than just the intercept):\n&gt; fm1 &lt;- gls(z\\(\\sim\\)1, data=mydata)\nExamine the residuals with a residual plot:\n&gt; plot(fitted.values(fm1), residuals(fm1))\nWe know that the assumption of independence might be violated, but we can at least initially peek at the assumptions of normality and homoscedasticity. Next, examine the summary output. We made need to take the results so far with a grain of salt, because we are not accounting for potential spatial dependence that may be skewering our significance tests (although, given the clarity of the initial result, at least when I did it, this is probably not a major concern for this data set).\nPutting concerns of spatial dependence aside temporarily,\n\nDoes it appear, statistically, that there is detectable amount of nitrate present in the well water overall (i.e., different from zero)?\nOn average, is the mean level of nitrate in a well significantly different from the maximum health standard set by the Minnesota Department of Health? (Hint: You may need to return to Question 9 on Lab 5 to answer this question. Remember to use the error degrees of freedom for your test statistic.)\n\nOk, let’s start thinking about the spatial dependence in the data. Let’s construct the sample semivariogram from our model residuals. This is the first step before we fit a theoretical model to the spatial errors:\n&gt; samplevg &lt;- Variogram(fm1, form =\\(\\sim\\)x + y, resType=\"response\")\nThe first argument contains the model under consideration. This helps R know where to find the residuals. The next argument specifies the covariate(s) to be used for specifying the distance pairs. Note that these are just the \\(x\\) and \\(y\\) locations from the dataset specified in fm1 and are critical to help formulate the variogram; they have nothing to do with fitting a trend surface such as in universal kriging. The resType argument specifies the type of residuals. We will not cover standardizing our residuals in this course, so just use the raw or “response” residuals for now. There are other aguments you can add as well, such as type=\"robust\" if you’d like to use the semivariogram robust to outliers first proposed by Noel Cressie.\nExamine the sample variogram by simply typing its name and hitting Enter. These are estimates of the spatial dependence in the sample data at different distances. You will see the number of pairs in each distance bin. This is where things get a little fuzzy with regards to constructing a reliable semivariogram. For example, you will find these suggestions and guidelines in the literature:\n\nNever fit the semivariogram beyond one half (or two thirds) the maximum distance in the data\nThe number of pairs in each bin should be greater than 30\nNever compute a semivariogram unless the number of locations is greater than 200 or 300\n\nWhich guideline makes it impossible for us to continue this lab?\nYes, nice rules, but unfortunately for real life data, even following the guidelines does not guarantee a lot of precision in the estimated semivariogram. Hence, we take an approach to estimate the semivariogram with whatever data we have, then augment that with some measure (however feeble) of the precision of the estimates. This is what you frequently have to do in real life.\nPlot the sample semivariogram.\n&gt; plot(samplevg)\nThe line going through the semivariogram is a loess smoother. Essentially, this is a moving average that helps you see trends.\nAs noted above, estimating spatial correlation at the extremes of the sample space is not very reliable. Choose and report a maximum distance value that you feel is sensible.\nRefit the sample variogram, but this time specify an additional argument with your chosen value: maxDist=?. Then, plot the sample variogram again, re-specifying the maximum distance for your \\(x\\) (distance) axis:\n&gt; plot(samplevg, xlim=c(0,?))\nThis little plot may be your best friend for the next hour, so I suggest that you get aquainted. I suggest that you paste a copy into a Word document so you don’t lose her/him, or even print a copy that you can decorate with pencil marks as you estimate nuggets and ranges.\nOk, let’s fit a theoretical model to our semivariogram. Some people simply look at their plot, make a guess, and use those guesses in their final analysis. Don’t EVER do that. Seriously, what do they think statistics is for? If you do enjoy simply pulling numbers straight out of your arse and attaching scientific credibility to them, use Bayesian statistics. (You will understand this joke after we talk more about Bayesian statistics).\nWe will do something slightly different. Take a long hard look at your semivariogram. Do visually estimate and report the nugget (if any), the range, and the sill in your table. Also, estimate and report the nugget ratio. This is defined as the nugget semivariance divided by the total semivariance (represented by the sill).\nWe will use these estimates as initialization values for constructing a theoretical semivariogram. That is, the computer will attempt to determine the true values, but it appreciates some help in getting started in its optimization routine. Your initial guesses do not form part of the answer; they only help speed along the optimization routine (or at least we hope!).\nOf course, there are different types of semivariograms. This is where we typically haul out our favourite geostatistics book and look at pretty pictures to see which one matches the sample semivariogram the closest. Below I’ve reprinted a table and figure from section 5.3.2 Spatial correlation structures from José Pinherio and Douglas Bates’ book Mixed-effects models in S and S-PLUS (2000) Springer.\n\n\n\nIn this table, \\(s\\) is distance, while the correlation parameter \\(\\rho\\) is generally referred to as the range in the spatial statistics literature. The corresponding figures with range = 1 and nugget effect = 0.1 are below:\n\n\n\nLet’s play with two types of semivariogram models: exponential and spherical. First, let’s fit an exponential model (model 3 in your table):\n&gt; fm1.exp &lt;- gls(z\\(\\sim\\)1, data, corr=corExp(c(R, NR),\n\\(~~~~~~~~~~~~~~~~~~~~\\)form =\\(\\sim\\)x + y, nugget=T))\nYou have to put numbers in place of the R and NR placeholders in the above command. These specify the range and the nugget ratio. The nugget=T just tells the program that you think there is a nugget effect. Examine the summary output:\n&gt; summary(fm1.exp)\nThe first thing you should notice is that your range and nugget are not the starting values that you specified. Rather, they reflect the maximum likelihood estimates of these parameters, based on your data and model.\n\nWhat is the value of the sill? More importantly, how did you derive it? (Yes, it is right there on your summary output, but you may need a little help).\nSome semivariogram models (like the exponential) do not have a true range, but rather report an “effective range” (even if the output says “range”). The effective range is defined as the distance at which the semivariogram value is 95% of the sill. Do you have a range or effective range, and what is its value?\n\nAt this point, you might be scratching your head at the estimate of the range when you compare it to your sample variogram. Hmmm. Well, it’s an estimate, but it doesn’t tell us anything about the variability in the estimate. What we really need is a measure of variation.\nWe could of course divide the estimate by some measure of variation to obtain a test statistic, which we could compare to a test statistic distribution to determine whether the estimate is signficantly different from zero. This is not always done with nuggets and sills and ranges, however, because it is sometimes unclear exactly how to determine the standard error, or what the appropriate test statistic distributions are.\nFor this lab, we can access 95% confidence intervals about the variogram parameter estimates, however:\n&gt; intervals(fm1.exp)\nNow, whether or not this command worked, please keep reading! If the confidence interval overlaps zero, the parameter is likely not significant. If the confidence interval varies over 5 or 10 orders of magnitude, or even returns NaN (not a number), you should be very concerned about how precise the estimates are. Who knows, it is possible that you may even get an error message indicating that the variance-covariance matrix is non-positive definite; i.e., that the confidence intervals cannot be obtained. Any or all of these can be warning signs that your variogram model is not well specified. Perhaps, for example, you are requesting an estimate of a parameter that simply is better off left out, or is so close to zero that it can’t be reliably estimated.\nThis can be frustrating. I get equally frustrated, however, with canned statistical software packages that provides an answer for anything and everything, sensible or not, because we are conditioned by the scientific community to expect answers. One of my favourite quotes comes from John Tukey (who contributed a great deal to the theory of multiple comparisons in post-hoc tests):\n\nThe combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.\n\nR is nice because it can give you an answer in the summary output, but it can also tell you from looking at the intervals() command that it might be a poor answer.\nWhen I constructed this lab exercise, the nugget parameter was giving me some grief (OK, lots of grief). Refit your exponential model without the nugget (you will need to remove the nugget ratio, and specify nugget=F). Examine and enter the estimates of the range, sill, and intercept in your table, and check the intervals as before. The intervals may change, making more sense (or less), or they may be substantially similar to the previous step. Let’s compare the two models:\n&gt; anova(fm1.exp, fm1.expSansNugget)\nThis command constructs a likelihood ratio test. In other words, it compares the change in deviance between the two models, and compares that change in deviance to a \\(\\chi^2\\) distribution with degrees of freedom equal to the change in the degrees of freedom (in this case, 1 df, because the second model is not estimating the nugget). If the \\(p\\)-value is not significant, it means that adding that new parameter did not explain significant variation, so you can proceed with the simpler model.\nSo, which exponential model do you prefer: the one with the nugget effect, or the one without the nugget effect?\nFit a spherical semivariogram with a nugget effect (fifth model in table). The syntax is exactly the same as before, except the correlation structure is specified as corSpher. Examine the summary, and report and comment on the values of the sill and the range. Do you think these estimates are more consistent with your sample semivariogram than the estimates from the exponential model?\nCheck the intervals on the spherical semivariogram. Then, fit a spherical semivariogram without a nugget effect (last model in table). Compare the two models with a likelihood ratio test. Is the nugget effect necessary?\nI will assume that you obtained a consistent answer for necessity of estimating a nugget for both the exponential and spherical models. If not, for learning purposes, choose both models without a nugget.\nFirst, just from visual inspection and examination of your preliminary results, which semivariogram model do you think fits the data better: exponential or spherical? Now, do a formal comparison. Because they are not nested, we cannot do a likelihood ratio test. However, we can compare them with information criteria such as AIC. Which model fits the best? Did this agree with your expert judgement?\nCompare this “best” model with your very first model without any spatial dependence specified (before you fit the sample semivariogram) using a likelihood ratio test. (The models are considered nested, because one has an intercept, and the other has an intercept plus a bunch of spatial error parameters). Was it necessary to account for spatial dependence in this dataset? (That is, was this lab a complete waste of time?)\n\nYou have just completed one of the most challenging parts of geostatistical analysis: modeling the spatial error structure in the variogram. One of the frequent goals of geostatistical data analysis is to predict new \\(z\\) values on the spatial surface, given the spatial correlation present (now that you’ve defined it). This is part of the science of kriging. A good package for prediction of new values is gstat, should you ever need it. However, we will not do that as part of this week’s exercise.",
    "crumbs": [
      "Labs",
      "Module 4: Integrating space into our models",
      "Lab 13: Geostatistics"
    ]
  },
  {
    "objectID": "lab/w12-lab.html#r-packages-for-today",
    "href": "lab/w12-lab.html#r-packages-for-today",
    "title": "Lab 12: Lattice data: Spatial linear models",
    "section": "R packages for today",
    "text": "R packages for today\nThe most powerful and flexible package for spatial linear models is the spatialreg package. There are others, especially useful for non-normally distributed responses. Load and use the spatialreg and spdep packages today. The spdep package is used to specify spatial neighborhooods and contains functions to test for spatial autocorrelation. Both packages are written and maintained by Roger Bivand, a true pioneer in the field of lattice data analysis. He is an economic geographer at the Norwegian School of Economics in Bergen, Norway.\nIf you would like to explore mapping as well (below, optional), load the maps, maptools, and ggplot2 packages as well.",
    "crumbs": [
      "Labs",
      "Module 4: Integrating space into our models",
      "Lab 12: Lattice data: Spatial linear models"
    ]
  },
  {
    "objectID": "lab/w12-lab.html#we-could-use-a-map",
    "href": "lab/w12-lab.html#we-could-use-a-map",
    "title": "Lab 12: Lattice data: Spatial linear models",
    "section": "We could use a map!",
    "text": "We could use a map!\nOn the course website, I’ve included a PDF of a map of the counties included in this analysis. The map is for reference only. The numbers on the bottom panel indicate the number of tornadoes. Alternatively, you can learn how to create a similar map in R below. Again, this is optional.",
    "crumbs": [
      "Labs",
      "Module 4: Integrating space into our models",
      "Lab 12: Lattice data: Spatial linear models"
    ]
  },
  {
    "objectID": "lab/w12-lab.html#producing-maps-in-r-courtesy-of-jake",
    "href": "lab/w12-lab.html#producing-maps-in-r-courtesy-of-jake",
    "title": "Lab 12: Lattice data: Spatial linear models",
    "section": "Producing maps in R (courtesy of Jake)",
    "text": "Producing maps in R (courtesy of Jake)\nIf you want to learn more about how to produce maps of states and counties in R, this section is for you! This is not a requirement to complete the assignment, however, so you may jump to section 3.4 and begin the assignment if you wish.\nAs with most tasks performed in R, there are many ways to produce maps. The maps package is useful for quickly producing basic maps of different countries, US states, or US states with counties. Try loading the maps and mapdata packages and typing the following commands into R:\n&gt; map(\"usa\") # Map the continental US\n&gt; map(\"state\") # Map the continental US with state boundaries\n&gt; map(\"county\") # Map the continental US with county boundaries\n&gt; map(regions = \"thailand\") # Map of Thailand\nThe maps package also provides other functions to add layers on top of a “base” map\n&gt; map(\"county\", regions = \"Wisconsin\") # A map of Wisconsin with county boundaries\n&gt; map.text(\"county\", regions = \"Wisconsin\") # Add county names to the map\nTo have more control over the maps produced and to make fancier maps, you will need to branch out into other packages. The following examples make use of the package ggplot2. The map_data() function from ggplot2 provides us with a way to access the data used in the maps package and provide more customization.\n&gt; wi &lt;- map_data(map = \"county\", region = \"Wisconsin\") # Create a dataframe of our WI polygons\n&gt; head(wi)\nEach row of the data in this dataframe corresponds to a “corner” or vertex of the county polygons you mapped previously. The polygons are organized into groups, which tells ggplot which corners should be connected by lines. We can plot this dataframe with the code below. The first function (ggplot()) tells ggplot what data to use and what variables are mapped to the x and y axes. The second function is connected to the first with a +, which is unique to ggplot code. You can link many ggplot functions together with + symbols to create graphs. The second function geom_polygon() tells ggplot that the data it is plotting should be represented as polygons with no fill color and black lines.\n&gt; ggplot(data = wi, aes(x = long, y = lat)) +\ngeom_polygon(fill = NA, color = \"black\")\nOops! We forgot to tell the ggplot function which points need to be connected by lines and which don’t. Let’s include the group variable this time. We’ll also add a few more lines to make the map look better.\n&gt; ggplot(data = wi, aes(x = long, y = lat, group = group)) +\ngeom_polygon(fill = NA, color = \"black\") +\ncoord_fixed(1.3) + # Adjusts the aspect ratio\ntheme_void() # Adds a theme to make the graphic clearer\nIf you are unfamiliar with the ggplot2 package, you can find more information by googling ggplot.\nIf you want to just plot the counties that exist in the dataset we’ll be working with today, you’ll need to take a few extra steps. First, we need to filter our map data so it only contains information for the counties we want to plot\n&gt; mydata$county &lt;- tolower(mydata$county) # convert counties to lower case to match our wi data\n&gt; wi_counties_subset &lt;- wi[wi$subregion %in% mydata$county, ] # This returns only the rows from the wi map data frame that have a county name that is also in the mydata data frame\nNow we can plot a map that includes just the subset of counties we’re working with.\n&gt; ggplot(data = wi_counties_subset, aes(x = long, y = lat, group = group)) +\ngeom_polygon(fill = NA, color = \"black\") +\ncoord_fixed(1.3) +\ntheme_void()",
    "crumbs": [
      "Labs",
      "Module 4: Integrating space into our models",
      "Lab 12: Lattice data: Spatial linear models"
    ]
  },
  {
    "objectID": "lab/w12-lab.html#assignment",
    "href": "lab/w12-lab.html#assignment",
    "title": "Lab 12: Lattice data: Spatial linear models",
    "section": "Assignment",
    "text": "Assignment\nRead in the data. When you plot the data in a scatterplot matrix, you may see that one point suggests an unusually high number of tornadoes. Remove the row with Dodge County from the dataset. I would not ordinarily suggest this step a priori, but I want you to focus on fitting spatial models and not testing for outliers today. Unfortunately, given the short duration of this course, we have not been able to devote very much time to statistical tests for outliers.\n\nThe first thing you have to do in spatial data analysis with areal models is to identify the coordinates of your spatial units (i.e., what is your \\(x\\) column, and what is your \\(y\\) column?). For our dataset, we will use the centroids of each county. You could specify the coordinates from the dataframe in each command that you use, or you can simply stick them together into an object as I suggest:\n&gt; mycoords &lt;- cbind(data$UTMEast, data$UTMNorth)\nNote that I’ve used the included UTM coordinates instead of latitude and longitude. There are arguments you can use to specify the latter, but I strongly prefer working with continuous \\(x\\) and \\(y\\) data. That makes it easier to specify neighborhood structures by distance (e.g., everyone within 50 km, rather than 0.5 \\(\\deg\\) longitude or latitude!).\nAfter specifying your spatial coordinates, you need to construct a neighborhood structure. There are many different methods for this. I’ve listed three of the most popular here, with example syntax. (Don’t run any code here; keep reading until instructed).\n\nDistance-based; i.e., \\(d\\) nearest neighbors. Syntax:\n&gt; myneighbors &lt;- dnearneigh(mycoords, d1, d2)\nFor example, for all neighbors between 0 and 75 km away, you would specify d1=0, d2=75.\nNumber-based; i.e., \\(k\\) nearest neighbors. Syntax:\n&gt; myneighbors &lt;- knn2nb(knearneigh(mycoords, k=1, longlat=NULL))\nFor example, to specify the closest 2 neighbors, specify k=2.\nGenerate neighborhood definitions from a regular grid or lattice (not the case with today’s data). Syntax:\n&gt; myneighbors &lt;- cell2nb(nrow, ncol, type=\"rook\", torus=F) (And, you now know what the torus argument does!)\n\nThere are other methods and neighborhood structures, such as reading a shapefile and defining neighbors as those that share common borders, but I do not include that syntax here.\nSo, there you have a few different techniques. We’ll focus on distance- (\\(d\\)) and number- (\\(k\\)) based methods for this county-level data.\nPlease construct two different neighborhood structures for the tornado data:\n\nAll neighbors within 50 km\nThree nearest neighbors\n\nFor the summary() and plot() functions, you do have to specify the coordinates:\n&gt; summary(myneighbors, mycoords)\n&gt; plot(myneighbors, mycoords)\nExamine both neighborhood structures visually. Comment on their differences. Which one do you think makes the most sense for a tornado analysis?\nNow that you have a neighborhood structure (or two of them, rather), construct weighting schemes for each. The most basic is the binary coding scheme, in which cells are either defined as being neighbors (i.e., 1) or not (i.e., 0).\nmyweights &lt;- nb2listw(myneighbors, style=\"B\")\nYou can get more specialized with inverse distance weights, etc. This code is FYI only:\n&gt; dn &lt;- dnearneigh(mycoords, 0, 100)\n&gt; plot(dn, mycoords)\n&gt; dlist &lt;- nbdists(dn, mycoords)\n&gt; dlist &lt;- lapply(dlist, function(x) 1/x)\n&gt; myweights &lt;- nb2listw(dn, style=‘‘B’’, glist=dlist)\nThe above code will create neighbors of any counties that are within 100 km of each other, and define a weight that is the reciprocal of the distance between counties, scaled by the minimum distance. (But note a limitation in this approach: the distances are measured by traversing the “links” between the centroids of each county, rather than traveling in a straight line. Programming the latter is a little more complicated\\(\\ldots\\)).\nNote that the neighbor list and the weighting scheme are technically two different things! However, in R, when you create the weighting scheme, R remembers and incorporates the neighborhood structure. So, when we specify models, all we have to do is provide the weights!\nLet’s fit a model with a SAR error structure to account for spatial dependence. Here’s the syntax to fit a SAR model:\nfm1 &lt;- errorsarlm(response\\(\\sim\\)covariate(s), data=data, listw=myweights)\nFit four separate models, examining the effect of each of four covariates on the number of tornadoes. Focus on Nlat, Wlong, log(pop90), and sqrt(area) for covariates. You will notice that I have transformed two of the covariates. To date, we have focused on transforming only the response variable when necessary to fulfill model assumptions for linear models. I found when doing this assignment that the big city of Madison in Dane County created some large skew in the population variable that sometimes made it difficult to solve parameter estimates for associated model coefficients. Hence, I encourage you to also use this simple fix, even if R does not encourage you to rescale the explanatory variable). For each of the four models, use the distance-based structure with a binary weighting scheme that you have already defined.\nTake a look at your four models, each one containing one covariate. If you were using forward selection to construct a model explaining the number of tornadoes at a county level in Wisconsin, are there any variable(s) you would summarily exclude? Why?\n(Note: You may see things in the output that we have not covered in class, like \\(\\lambda\\) estimates. Don’t worry about those, as they relate to parameterizing the SAR error terms.)\n\nBuild and report a good model out of the remaining variables. Retain the same distance-based neighborhood structure throughout to keep things simple. As you might expect, judge your model(s) by the AIC values and statistical signficance of the remaining variables.\nRemember to examine the residuals of your final model:\n\nWe assume that the model has satisfactorily accounted for spatial dependence. We can utilize Moran’s test on the residuals to check the independence assumption (Assumption 2). We have a neighborhood structure and weights predefined, so we type:\n&gt; mycheck &lt;- moran.test(fm1$resid, myweights, alternative=\"two.sided\")\n&gt; mycheck\nIf the \\(p\\)-value is \\(&gt;0.05\\), Moran’s I statistic is not significantly different from zero, providing no evidence that there is spatial autocorrelation remaining in the residuals.\nFurther, we still assume that any residual error left over, even after accounting for spatial variation, is homoscedastic and normally distributed (Assumptions 3 and 4 of linear models). Hence, we examine the residual plot:\n&gt; plot(fm1$residuals\\(\\sim\\)fm1$fitted.values)\n\n\nAfter you have found a satisfactory model, change the weighting scheme.\nFit the same final model using the nearest neighbor weights, rather than the distance weights. Rather than focusing on the exact numbers of the slopes of each covariate, looks at the trends and relative magnitudes. Did the sign and significance of your model coefficients change? How about the spatial dependence, distribution, and homoscedasticity in the residuals? Is this model substantially similar to the previous one?\n\nFinally, fit the same final model, this time ignoring any spatial error structure using our old friend the lm() command. Are the results substantially similar? (i.e., slopes, significance, residual plots, etc.). While examining \\(R^2\\) values are the most conventional methods of examining regular linear regressions, you can extract an AIC value using\n&gt; AIC(mymodel)\nso you can compare it to the previous spatial fits using likelihood methods. Do a Moran test on the residuals of this model, using either the distance-based or nearest neighbor weights, looking for evidence of spatial dependence.\nExamine your results. Hmmm. Which model would you choose? Why?",
    "crumbs": [
      "Labs",
      "Module 4: Integrating space into our models",
      "Lab 12: Lattice data: Spatial linear models"
    ]
  },
  {
    "objectID": "lab/w10-lab.html#ripleys-k",
    "href": "lab/w10-lab.html#ripleys-k",
    "title": "Lab 10: Spatial point processes: Ripley’s K",
    "section": "Ripley’s K",
    "text": "Ripley’s K\nLoad the spatstat package today. This package was originally written by Adrian Baddeley (Curtin University), Ege Rubak (Aalborg University), and Rolf Turner (now retired, formerly of University of New Brunswick). This package is the premier package for working with spatial point process data. The two senior authors Baddeley and Turner instituted a bold change from historic classroom terminology that I like very much. For distance metrics in the K function, they use \\(r\\) for radius, which is still a type of distance, rather than \\(t\\).\n\nAfter loading the spatstat package, load the data. As usual, do a summary() to ensure the data were loaded properly. In today’s data set, the marks are a factor with two levels, so be sure to convert the type column to a factor as necessary. Again, as good practice, use the summary() command on your data set to check that your variable types are properly defined both before and after an operation.\nNow, use subsetting or row selection techniques to create two more dataframes containing only the dividing cells, and only the pyknotic cells. You should now have three data sets.\n\nTo this point in the course, when we have read in data, the resulting object has been a dataframe that we can then examine with a summary(), explore with a scatterplot matrix, etc. Today, we are going to work with a slightly different object, a planar point process, abbreviated ppp.\nConvert each dataframe to a planar point process. Here’s the relevant syntax, with explanation following:\n&gt; my.ppp.marked &lt;- ppp(x=mydata$x, y=mydata$y, window=square(1), marks=mydata$type)\nThe first two arguments in the ppp() command specify where to find the spatial locations. The window specifies what the sample area looks like. I have used the simplest specification, a square with sides of length=1. The window argument is actually very flexible; you can provide coordinates from shapefiles, polygons with holes, rectangles, and more. (See a full description in the help file using ?window if you would like to investigate further with your own data).\nThe final argument, marks, specifies where to find the marks in your dataframe (here, in the type column). Note: If you are working with a univariate dataset that only contains one type of mark (e.g., only dividing cells or only maple trees, so you only have two columns with x’s and y’s to denote the events locations), then you do not need to include the marks argument.\nYou should now have three ppp objects, one for the full dataset with marks, and one for each of your univariate subsets for dividing and pyknotic cells (without marks). Be sure to name them something meaningful.\n\nNext, do a summary() on the resulting object from the ppp() from the full dataset with marks. You should see a helpful table of the total number of cells that are dividing vs. dying, their relative proportions, and the expected intensity, \\(\\lambda\\), per unit area.\nFirst assignment question: given that the unit square in our data set is actually 250 \\(\\mu\\)m long, what is the intensity of dividing cells on a per mm\\(^2\\) basis?\nLet’s work now with the dying, pyknotic cells. As you saw in the previous question, a summary() of the point process returns the intensity and details of the window in a point process without marks. Plot the point process for the pyknotic cells (I think you can guess the syntax). The pyknotic cells will show up as circles, since there is only one type of mark.\nFrom a strictly visual examination, do you think the dying cells appear to be clustered, random, or regularly spaced?\nThe human eye is actually trained to look for patterns. As such, we are by nature terrible at visually examining and diagnosing complete spatial randomness. Hence, Ripley’s K is a useful quantitative technique. Continue working with the point process restricted to these pyknotic cells.\nThe syntax for Ripley’s K is fairly simple. We use the estimation function, Kest():\n&gt; myRipleysK.pyknotic &lt;- Kest(my.ppp.pyknotic, correction=\"best\")\nThe first argument specifies the univariate planar point process (i.e., no marks - you are only looking at one type of cell). The second argument to the function specifies the border correction.\nThe Kest() command has many different border correction options that have different statistical properties (e.g., efficiency/speed of calculation, robustness to smaller data sets, shapes of windows, etc.). One of the most useful for square or rectangular geometries is Ripley’s isotropic correction. The technique I simply recommend is to specify the option correction=\"best\". Believe it or not, the creators of the package have developed a number of rules to calculate the best method based on your data set!\nNow, plot() the resulting K function. You should see a theoretical \\(K\\) line (might be blue), with a squiggly empirical \\(\\hat{K}\\) line. What border correction did the program choose for you?\nIf you were to try different border corrections, you would likely find that they all look somewhat similar. Just for fun, calculate Ripleys’ K without the correction argument and then plot the result. The resulting graph will plot the various empirical \\(\\hat{K}\\) lines denoting differing types of border corrections.\nAny one of these empirical \\(\\hat{K}\\) line(s), while helpful, are not terribly informative, since they do not tell us how far above or below the theoretical line one needs to be to be considered “significant.”\nInstruct your computer to construct a simulation envelope. With the simulation envelope, the computer randomizes the point pattern 1,000 times, calculates the empirical K function, and then takes off a specified percentile of top and bottom runs to construct a simulation or confidence envelope. Be patient; it may take a few minutes depending on the speed of your machine!\n&gt; my.env.pyknotic &lt;- envelope(my.ppp.pyknotic, Kest, nsim=999, nrank=25)\nPlot the envelope using plot(my.env.pyknotic). You should see a nice shaded region with somewhat-symmetric upper and lower confidence lines now.\n\nWhat size of confidence interval is constructed by doing 999 simulations (in addition to the 1 real data set) and taking away the maximum and minimum 25 runs?\nHow would you change the arguments if you wanted a 99% confidence interval?\n\nIt can be difficult to see where the empirical line crosses a theoretical boundary line on a quadratic scale, so plot one of these transformations instead:\n&gt; plot(myenv, sqrt(./pi)\\(\\sim\\)r)\n&gt; plot(myenv, (sqrt(./pi)-r)\\(\\sim\\)r)\nWhat is the name of the function that you have just plotted?\nLooking at your Ripley’s K function with confidence intervals, do you see evidence of clustering or regular spacing in the pyknotic cells? Does this agree with your initial guess at what the pattern would look like from visual inspection?\n\nLet’s do some work with Ripley’s bivariate K, sometimes known as K-cross functions. First, let’s check the summary of the entire marked point process (i.e., whole data set). You may need to go back to question 1 to remember where you left it.\n&gt; summary(my.ppp.marked)\nAnd graph the process:\n&gt; plot(my.ppp.marked, cols=c(\"black\", \"red\"))\nYou can see I’ve added some colour to the graph for visual distinction. From visual inspection, do you think that the dividing cells are found next to or far away from the dying, pyknotic cells? Or is there no relation (i.e., complete spatial randomness)?\nConstruct and plot Ripley’s bivariate K function for the interaction between the dividing and pyknotic cells (i.e., using the marked point process). The syntax is identical to what you have worked with in previous questions, except that you now must (1) use Kcross() instead of Kest() to estimate the K-function (again, use the correction=\"best\" argument within Kcross()) and (2) specify the marked point process where you previously specified the univariate planar point process for pyknotic cells. Simulation envelopes are constructed as before, with a reminder that construction may take a few minutes because the computer is now randomizing two point processes multiple times.\nDo dying cells tend to be clustered, far away from, or completely independent (i.e., random) from dividing cells? If you see a significant pattern, at what spatial scale(s) does it occur? (Remember that these data have been rescaled).",
    "crumbs": [
      "Labs",
      "Module 4: Integrating space into our models",
      "Lab 10: Spatial point processes: Ripley’s K"
    ]
  },
  {
    "objectID": "lab/w3-lab.html#reminder",
    "href": "lab/w3-lab.html#reminder",
    "title": "Lab 3: summarizing data",
    "section": "Reminder",
    "text": "Reminder\nPlease do keep a separate Word or text file of your favourite commands - things that you know worked! It may become invaluable later on in the semester, in other courses, or even when you are doing your own analyses after you graduate. Remember, because R can be run with simple commands, you can always copy and paste from the clipboard in Windows straight to R.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 3: Summarizing data"
    ]
  },
  {
    "objectID": "lab/w3-lab.html#the-sign",
    "href": "lab/w3-lab.html#the-sign",
    "title": "Lab 3: summarizing data",
    "section": "The + sign",
    "text": "The + sign\nWe may now get into commands that flow across your screen and onto the next line. That’s fine - just type the command until it is complete and hit Enter. If you hit Enter before the command is complete, R may detect this (e.g., perhaps the last parenthesis is not included) and display a + sign instead of the command prompt. Just finish typing the command and hit Enter.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 3: Summarizing data"
    ]
  },
  {
    "objectID": "lab/w3-lab.html#selecting-specific-rows-or-columns",
    "href": "lab/w3-lab.html#selecting-specific-rows-or-columns",
    "title": "Lab 3: summarizing data",
    "section": "Selecting specific rows or columns",
    "text": "Selecting specific rows or columns\nA quick review: suppose the second column is named \"Site\". To select this column in a dataframe of 8 columns, these commands would all be the same:\n&gt; mydata$Site        # Probably the easiest when selecting entire column\n&gt; mydata[,2]         # This works too, if you remember it is the second column!\n&gt; mydata[,c(\"Site\")] # Use this when working with dataframes.  If you need multiple columns,\n                         separate quoted titles by commas\nTo select the 5th through 10th rows of this column, these commands would all be the same:\n&gt; mydata$Site[5:10]      # Works, but confusing because brackets are only for rows\n&gt; mydata[5:10,2]         # This works too, if you remember it is the second column!\n&gt; mydata[5:10,c(\"Site\")] # Use this when working with dataframes.  Probably the easiest\n                                when selecting rows\nLast week I also included commands for conditional selection (selecting all rows where a value equals a certain value).",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 3: Summarizing data"
    ]
  },
  {
    "objectID": "lab/w3-lab.html#getting-rid-of-missing-data",
    "href": "lab/w3-lab.html#getting-rid-of-missing-data",
    "title": "Lab 3: summarizing data",
    "section": "Getting rid of missing data",
    "text": "Getting rid of missing data\nWe now know how to select certain rows, and tell R what missing data looks like when we read in the spreadsheet. R codes missing observations then as green NA values. We should note, however, that when we get into analyses, R usually includes missing data unless told otherwise. Most software packages do not give you this option (although Microsoft Excel had well-publicized problems with incorrectly calculating the degrees of freedom for \\(t\\)-tests with missing values a number of years ago). Frequently, we do want to get rid of these values.\nIf you know a priori that you do not need the missing data (often the case), I go ahead and delete it immediately. For example, let’s pretend that I know I am going to be working with a variable named Intensity. To delete all rows that have a missing value in the Intensity column, I use this syntax taking advantage of the is.na() command:\n&gt; mydata &lt;- mydata[!is.na(mydata$Intensity),]\nCan you decipher what this command does? (Hint: use the concatenate command c()to string together the numbers 1,2,3,NA,5 into a vector called x. Then try the is.na() command on x).",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 3: Summarizing data"
    ]
  },
  {
    "objectID": "lab/w3-lab.html#deriving-a-new-colum-based-on-a-condition",
    "href": "lab/w3-lab.html#deriving-a-new-colum-based-on-a-condition",
    "title": "Lab 3: summarizing data",
    "section": "Deriving a new colum based on a condition",
    "text": "Deriving a new colum based on a condition\nLast week you figured out how to do simple numeric operations on a column, such as:\n&gt; mydata$z &lt;- mydata$x * 10\n&gt; mydata$z &lt;- mydata$x / mydata$y\nSomeone asked last week if you can do conditional operations. For example, let’s say you have a column in a dataset full of zeros and various positive integers:\n&gt; mydata &lt;- data.frame(x=c(0,5,0,0,4,9))\n&gt; mydata\n  x\n1 4\n2 0\n3 0\n4 6\n5 0\n6 9\nNow, let’s say you want to recode the column so that all positive numbers become 1 while the rest become 0.\n&gt; mydata$z &lt;- ifelse(mydata$x&gt;0, 1, 0)\n&gt; mydata\n  x z\n1 4 1\n2 0 0\n3 0 0\n4 6 1\n5 0 0\n6 9 1\nThe ifelse() command uses 3 arguments: first, a logical test, second, the value if true, and third, the value to record if false.\nThere are other, more complicated operations possible, such as programming loops and the like. That is beyond the scope of this course, although I use them frequently in more complex analyses.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 3: Summarizing data"
    ]
  },
  {
    "objectID": "lab/w3-lab.html#the-aggregate-command",
    "href": "lab/w3-lab.html#the-aggregate-command",
    "title": "Lab 3: summarizing data",
    "section": "The aggregate command",
    "text": "The aggregate command\nWe often want to see how things break down in groups. R has a number of built in functions for this (tapply(), gapply(), table(), etc). I use these occasionally, but they are all derivatives of the following command: aggregate().\nIn this lab, we will experiment a little with the aggregate() command.\nSometimes, I think it could be called the “aggravate” command because the syntax can get a little confusing for large datasets. However, it is very powerful, and can give you (almost) anything you want.\nSuppose you have a dataset containing a variety of observations on cats. You want to know the average tail length and whisker length of different breeds, broken down also by sex. Here is the general structure of the command:\ncatstats &lt;- aggregate(cats[,c(\"taillength\", \"whiskerlength\")], by=list(sex=cats$sex,\n              variety=cats$breed), function(x) mean(x, na.rm=T))\n\nThe first argument, dataframe[,c(\"column\")] specifies which column variable(s) you want information on. What are you counting, taking the average of, etc.? You need to use that exact syntax: you cannot use dataframe$column.\n\nThe second argument, by=list(group1=dataframe$group1,\ngroup2=dataframe$group2) specifies how to break down the information by groups from the dataframe. Do you want the information broken down by sex? by sex and breed? These are all categorical variables (i.e., factors). You can specify as many categorical variables as you want. You may also rename each group in the new dataframe. In the example, I have renamed the breed variable to variety in the output. Generally, to avoid confusion, I keep the variable names the same.\n\nThe last argument is unusual because there is no comma between the function(x) and the specified function. The function can be whatever you want: length(x) to count the observations, mean(x) to take the average, mean(x)+\\(\\pi\\) to take the mean and add 3.14 (if you had a bizarre desire to do so), etc.. You can even write your own (very complex) functions within R. We won’t cover how to write functions in R in this course (although it is not hard). (Several years ago, someone wrote a function to order pizzas using R, a valid zip code and street address, and www.papajohnsonline.com. How cool is that?!).\n\n\n\nRemember, the aggregate() command produces a summary table (i.e., a variable custom-summarized by specified categories). In the example above, just type in the name of the table (catstats) to display it. A common mistake is overuse of the summary() command. In this instance, it would summarize the already-summarized summary table.",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 3: Summarizing data"
    ]
  },
  {
    "objectID": "lab/w3-lab.html#the-xtabs-command",
    "href": "lab/w3-lab.html#the-xtabs-command",
    "title": "Lab 3: summarizing data",
    "section": "The xtabs command",
    "text": "The xtabs command\nThe xtabs() command is another useful command. It does a crosstabulation of your data by two or more factors (i.e., it counts the observations). For example, if you typed\nmeow &lt;- xtabs(~ colour + breed, data=cats[!is.na(cats$taillength),])\nyou would get the number of observations of taillength for each category. Note that I have asked R to delete any missing observations for this variable when doing the crosstabulation (suppose I did not do this earlier after reading in my data). This command is an easy way to see where you are missing observations!",
    "crumbs": [
      "Labs",
      "Module 1: RStudio and Tidyverse crash course",
      "Lab 3: Summarizing data"
    ]
  },
  {
    "objectID": "lab/index.html",
    "href": "lab/index.html",
    "title": "Overview of labs + basic info",
    "section": "",
    "text": "Here, you’ll find the virtual companion to all of our hands-on labs for the semester. These will be the same as your physical hand-outs, but I know many folks prefer to have them up on their screens as they work through the activities.\nThe purpose of these labs is to practice applying the topics that we discuss in lecture using real-world examples and data. The associated data files for each lab are linked in the Frontmatter section atop each page (all the files are also located here). Over the semester, we’ll explore some of the frustrations and challenges of working with bio/ecological data (of which there are many!), but together we’ll conquer them, setting y’all up for success in analyzing, interpreting, and sharing your own data.\nAs a reminder, each of the 13 labs will have an assignment/report that will be evaluated. The first three periods will be devoted to a primer of using R/RStudio for managing, wrangling, and interpreting data, and the reports associated with these will receive feedback but will not be included in the course grade. The remaining 10 lab assignments/reports will be graded and count toward 40% of total course grade (4% per report). There will also be a lab final worth 15% of the total course grade. In this, students will be given a real data set to analyze and develop a report for during the last lab period.",
    "crumbs": [
      "Labs",
      "Getting started",
      "Overview of labs + basic info"
    ]
  },
  {
    "objectID": "lab/index.html#downloading-installing-and-setting-up-r-and-rstudio",
    "href": "lab/index.html#downloading-installing-and-setting-up-r-and-rstudio",
    "title": "Overview of labs + basic info",
    "section": "Downloading, installing, and setting up R and RStudio",
    "text": "Downloading, installing, and setting up R and RStudio\nFor this course, we’ll be using the Posit Cloud service for labs and assignments. It’s essentially an online virtual machine to run RStudio in your browser. This will avoid version issues, package incompatibilities, and Mac vs Windows challenges that we might otherwise face. This is a paid service we’ll be using for class and it will expire after our semester concludes, so I also encourage you to install R and R Studio on your own machines if you haven’t already. Click on the buttons below to install both R (the language, or engine that enables our analyses to be performed) and RStudio (the shell, or vessel that the engine of R sits in and gives us the nifty features to manage files, version history, and write pretty and readable code).\n\n Launch Posit Cloud  Download R  Download RStudio\n\n\nQuality of life improvements\nRStudio is a highly customizable program that will let you nerd out to your heart’s content. Feel free! A few things that I suggest you customize right away will help you (and me) to debug and read your code.\n\nAfter launching RStudio, go to Tools —&gt; Global options.\nSelect the “Appearance” tab on the side, and select an editor theme that you like best. I highly recommend one where comments (text preceded by a #) are some shade of gray. This helps them be obvious without being obnoxiously bright and distracting.\nNext, select the “Code” tab on the side, and then select the “Display” panel. Enable: Highlight selected line, Allow scroll past end of document, Highlight function calls, and Use rainbow parentheses 🌈. These things will help you better catch syntax errors with missed parentheses, and help you and me quickly see where you’re editing in your R files.",
    "crumbs": [
      "Labs",
      "Getting started",
      "Overview of labs + basic info"
    ]
  },
  {
    "objectID": "lab/index.html#completing-and-submitting-labs",
    "href": "lab/index.html#completing-and-submitting-labs",
    "title": "Overview of labs + basic info",
    "section": "Completing and submitting labs",
    "text": "Completing and submitting labs\nIn labs, we’ll be writing code in a couple of different ways, but primarily what I will be recommending is using an R Notebook to write, annotate, and interpret your code. R Notebooks are a type of Markdown file where we can mix formatted text, code chunks, and a whole bunch of other goodies. R Notebooks help to organize your code into sections and subsections, and will let you write about the output directly in the document rather than creating a Word document (shudder…). We’ll go through this a bit more in the first lab, but take a look at the following links to learn a bit more about R Notebooks and R Markdown.\n\nWriting code in R notebooks\nR Notebook (Markdown) cheatsheet",
    "crumbs": [
      "Labs",
      "Getting started",
      "Overview of labs + basic info"
    ]
  },
  {
    "objectID": "lab/w7-lab.html#transforming-the-x-variable",
    "href": "lab/w7-lab.html#transforming-the-x-variable",
    "title": "Lab 7: Curvy data",
    "section": "Transforming the \\(x\\) variable",
    "text": "Transforming the \\(x\\) variable\nDecay functions and asymptotes are frequently seen in dose-response curves where increasing doses have decreasing biological effects, or perhaps an effect decays through time. One of the easiest things to do is transform the \\(x\\) variable, fit a regular regression line, and then back-transform.\n\n       reciprocal $x$ transformation                        logarithmic $x$ transformation\n\\(y_i = \\beta_0 + \\beta_1 (1/x_i) + \\epsilon_i\\) \\(y_i = \\beta_0 + \\beta_1\\)log\\((x_i) + \\epsilon_i\\)",
    "crumbs": [
      "Labs",
      "Module 2: Reviewing foundations in statistics",
      "Lab 7: Curvy data"
    ]
  },
  {
    "objectID": "lab/w7-lab.html#polynomial-regression",
    "href": "lab/w7-lab.html#polynomial-regression",
    "title": "Lab 7: Curvy data",
    "section": "Polynomial regression",
    "text": "Polynomial regression\nThe basic formula for polynomial regression is \\[y_i = \\beta_0 + \\beta_1 x + \\beta_2 x^{2} + \\ldots + \\beta_k x^{k} + \\epsilon_i\\] The \\(\\beta_0\\) is the intercept, or where the line crosses the \\(y\\)-axis. The \\(\\beta_1\\) is the slope of the first covariate, the “linear” term, or the increase in \\(y\\) per unit change in \\(x\\) holding all other covariates constant. The \\(\\beta_2\\) is the slope of the second covariate, the “quadratic” term. The \\(\\epsilon_i\\) refers to the “errors” about the line, and we assume that they are normally distributed with a constant variance (i.e., \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)).\nAn important note when reporting a polynomial regression line: by convention, we always report “lower order” terms, even if they are not significant. So, if you have a quadratic equation, for example, where only the quadratic (i.e., squared) covariate is significant, we would still include the intercept and the linear term when reporting the equation of the line, even if they are not significantly different from zero.",
    "crumbs": [
      "Labs",
      "Module 2: Reviewing foundations in statistics",
      "Lab 7: Curvy data"
    ]
  },
  {
    "objectID": "lab/w7-lab.html#checking-assumptions",
    "href": "lab/w7-lab.html#checking-assumptions",
    "title": "Lab 7: Curvy data",
    "section": "Checking assumptions",
    "text": "Checking assumptions\nBoth techniques have the same linear model structure. As such, you are familiar with the assumptions:\n\nThe model is correct\nThe observations are independent\nThe variances are equal\nThe errors are normally distributed\n\nAs you might expect, you can check the assumptions of equal variances and normally distributed errors in your residual plot, as well as detecting if your model is perhaps misspecified.",
    "crumbs": [
      "Labs",
      "Module 2: Reviewing foundations in statistics",
      "Lab 7: Curvy data"
    ]
  },
  {
    "objectID": "lab/w7-lab.html#transforming-a-covariate-in-r",
    "href": "lab/w7-lab.html#transforming-a-covariate-in-r",
    "title": "Lab 7: Curvy data",
    "section": "Transforming a covariate in R",
    "text": "Transforming a covariate in R\nBoth of these techniques involve transforming a covariate. For example, in polynomial regression, you need to square a covariate to get the quadratic term. When doing this in R, you need to use the identity specifier, I(). That’s just a fancy way of telling the program to transform and protect the covariate. For example:\n&gt; fm1 &lt;- lm(y ~ log(x), ...) # standard function, no identity protection needed\n&gt; fm2 &lt;- lm(y ~ I(1/x), ...) # add data argument as before\n&gt; fm3 &lt;- lm(y ~ x + I(x^2), ...) # quadratic function",
    "crumbs": [
      "Labs",
      "Module 2: Reviewing foundations in statistics",
      "Lab 7: Curvy data"
    ]
  },
  {
    "objectID": "lab/w7-lab.html#other-techniques-fyi",
    "href": "lab/w7-lab.html#other-techniques-fyi",
    "title": "Lab 7: Curvy data",
    "section": "Other techniques (FYI)",
    "text": "Other techniques (FYI)\nThere are many other techniques for fitting curves: loess fitting (also known as locally weighted polynomial regression or locally weighted regression scatterplot smoothing), smoothing splines, and nonlinear models.\nFor example, the asymptotic decay function below\n\n\n\ncould also be fit using a nonlinear model like this:\n\n\\(y(x) = \\phi_1 + (\\phi_2-\\phi_1)\\)exp\\([-\\)exp\\((\\phi_3)x]\\)\n\nI am not going to cover nonlinear methods in this course. More often than not, the practitioner fits a nonlinear model because it looks good and the software offered the option (maybe the button was shiny?). Keeping with a theme in the course, DON’T do something unless you can explain what you did and why you did it. From the example above, one may arrive at a nice line, but if you ask what the estimate of \\(\\phi_3\\) represents (is it an intercept? a slope?), the practitioner may not have a clue. That’s not the way you want to conduct your analyses. (In the above example, it’s actually the logarithm of the rate constant. A half-life could be estimated \\(t_{0.5} =\\) log \\(2/\\)exp\\((\\phi_3)\\)).\nThe nice thing about using a “regular” regression line (even with a transformed \\(x\\) variable) is that the form of the equation is still very straightforward.",
    "crumbs": [
      "Labs",
      "Module 2: Reviewing foundations in statistics",
      "Lab 7: Curvy data"
    ]
  },
  {
    "objectID": "lab/w4-lab.html#keeping-notes",
    "href": "lab/w4-lab.html#keeping-notes",
    "title": "Lab 4: Independent two-sample t-tests and ANOVA",
    "section": "Keeping notes",
    "text": "Keeping notes\nAt this point, you will find it necessary to refer periodically to previous labs and/or your own Word or text file of your favourite commands - things that you know worked!",
    "crumbs": [
      "Labs",
      "Module 2: Reviewing foundations in statistics",
      "Lab 4: Independent two-sample t-tests and ANOVA"
    ]
  },
  {
    "objectID": "lab/w4-lab.html#examining-objects",
    "href": "lab/w4-lab.html#examining-objects",
    "title": "Lab 4: Independent two-sample t-tests and ANOVA",
    "section": "Examining objects",
    "text": "Examining objects\nSome of you have found your own commands like show(), print(), etc. to display objects. I am glad that you are using such commands, but often you do not have to. If you simply type in the name of the object, it will display on its own. That is, hitting the Enter key after mydata will do the same as hitting the Enter key after show(mydata). Also, remember that certain objects are already summaries, such as the output from an aggregate() command (i.e., you are asking for a data summary by one or more factors). So, if you ask for a summary of that object, you will get a summary of a summary, which might not be informative. The only time I invoke the summary() command is when I want a summary on raw data, such as a dataframe or a column, or the results of a fitted model (which we have not yet covered).",
    "crumbs": [
      "Labs",
      "Module 2: Reviewing foundations in statistics",
      "Lab 4: Independent two-sample t-tests and ANOVA"
    ]
  },
  {
    "objectID": "lab/w4-lab.html#commands-from-previous-weeks",
    "href": "lab/w4-lab.html#commands-from-previous-weeks",
    "title": "Lab 4: Independent two-sample t-tests and ANOVA",
    "section": "Commands from previous weeks",
    "text": "Commands from previous weeks\nIf you have had a hard time getting used to R so far, you can take solace in the fact that (a) you are not alone and (b) we are done with the nuts and bolts of data entry and manipulation. As we start using these pieces every week, they will soon become less challenging. The key areas with which you should be starting to feel comfortable include reading data into R, selecting specific rows and columns, and performing both graphical and numerical summaries. The latter summaries include not only dataframe summaries but also conditional summaries (by specific categories) using something like the aggregate() command. You now have all the tools you need to start discovering neat things about your data.",
    "crumbs": [
      "Labs",
      "Module 2: Reviewing foundations in statistics",
      "Lab 4: Independent two-sample t-tests and ANOVA"
    ]
  },
  {
    "objectID": "lab/w4-lab.html#useful-commands",
    "href": "lab/w4-lab.html#useful-commands",
    "title": "Lab 4: Independent two-sample t-tests and ANOVA",
    "section": "Useful commands",
    "text": "Useful commands\nIntuitively, you can use the t.test() command. For an independent \\(t\\)-test, you can use:\n&gt; t.test(group1, group2, mu=0, var.equal=T)",
    "crumbs": [
      "Labs",
      "Module 2: Reviewing foundations in statistics",
      "Lab 4: Independent two-sample t-tests and ANOVA"
    ]
  },
  {
    "objectID": "lab/w4-lab.html#fitting-an-anova-model",
    "href": "lab/w4-lab.html#fitting-an-anova-model",
    "title": "Lab 4: Independent two-sample t-tests and ANOVA",
    "section": "Fitting an ANOVA model",
    "text": "Fitting an ANOVA model\nIn performing an ANOVA, we fit a linear model to the data. Here is how the linear model command, lm(), works:\n\nDecide what to call your model. E.g.,\n&gt; fm1 &lt;-\nSpecify the lm() command, and determine what y variable you are analyzing. Is it vegetation growth? Weight gain? Test scores? Other? It should be something that is, or can be transformed to be, normally distributed.\n&gt; fm1 &lt;- lm(y\\(\\sim\\)\nList the factor(s) that you are analyzing. Does your response variable vary with Site? Sex? Group? Treatment? Other? Remember, for an ANOVA, the variable in this column must be a factor!\n&gt; fm1 &lt;- lm(y\\(\\sim\\)sex\nSpecify the dataset you are using.\n&gt; fm1 &lt;- lm(y\\(\\sim\\)sex, data=mylabdata)",
    "crumbs": [
      "Labs",
      "Module 2: Reviewing foundations in statistics",
      "Lab 4: Independent two-sample t-tests and ANOVA"
    ]
  },
  {
    "objectID": "lab/w4-lab.html#working-with-the-model",
    "href": "lab/w4-lab.html#working-with-the-model",
    "title": "Lab 4: Independent two-sample t-tests and ANOVA",
    "section": "Working with the model",
    "text": "Working with the model\n\nTo examine the significance of the factor(s) in the overall model:\n&gt; anova(fm1)\nTo get detailed information on model coefficients (i.e., \\(Y_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}\\)), use:\n&gt; summary(fm1)\nTo examine how well the model fits:\n&gt; plot(residuals(fm1)\\(\\sim\\)fitted.values(fm1))\nRemember, a residual plot should show roughly equal variance around the horizonal zero line for each group. If the errors are heteroscedastic, you may need to apply a transformation.\n\nTo apply a transformation to your response variable:\n&gt; fm1 &lt;- lm(sqrt(y)\\(\\sim\\)sex, data=mylabdata)\n\nTo specify more than one factor in a model (e.g., sex and diet), list the factors separated by the ‘+’ sign\n&gt; fm1 &lt;- lm(y\\(\\sim\\)sex + diet, data=mylabdata)\n\nTo specify interactions between factors, use a colon. Better yet (unless you well versed in special situations where you might fit an interaction term without corresponding main effects), use the asterisk sign. This automatically fits both factors and their interaction.\n&gt; fm1 &lt;- lm(y\\(\\sim\\)sex + diet + sex:diet, data=mylabdata) # Same as below\n&gt; fm1 &lt;- lm(y\\(\\sim\\)sex*diet, data=mylabdata) # Tidy\n(This is an advanced topic that I only include FYI.) To fit a “cell means model” (the estimated mean for each group), i.e., \\(Y_{ij}=\\mu_i + \\epsilon_{ij}\\), put a -1 behind the factor\n&gt; fm1 &lt;- lm(y\\(\\sim\\)sex-1, data=mylabdata)\nRemember, the -1 is just convention and has nothing to do with subtracting 1 from any quantity.",
    "crumbs": [
      "Labs",
      "Module 2: Reviewing foundations in statistics",
      "Lab 4: Independent two-sample t-tests and ANOVA"
    ]
  },
  {
    "objectID": "lab/w4-lab.html#dire-warning",
    "href": "lab/w4-lab.html#dire-warning",
    "title": "Lab 4: Independent two-sample t-tests and ANOVA",
    "section": "Dire Warning",
    "text": "Dire Warning\nA critical part of fitting an ANOVA model is that your factor variable is actually a factor! An extremely common mistake is failure to convert the proper variables to factors (such as if the types of diet are labelled 1, 2, and 3 and the statistics program treats them numerically). You cannot do an ANOVA on a variable that is not a factor.",
    "crumbs": [
      "Labs",
      "Module 2: Reviewing foundations in statistics",
      "Lab 4: Independent two-sample t-tests and ANOVA"
    ]
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Overview of lectures and lecture material",
    "section": "",
    "text": "Test 2",
    "crumbs": [
      "Content",
      "Overview",
      "Overview of lectures and lecture material"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Entomology 5126",
    "section": "",
    "text": "Anyone attempting to analyze data from lab, field, or existing datasets are frequently confronted with a stark reality: organisms rarely stay in the same place in space and time. The goal of this course is to provide a gentle, but thorough foundation to the concepts and tools needed to analyze, interpret, and communicate spatially and temporally dependent data.\n\n\n\n\n\n\nSpring 2026 Semester College of Food, Agricultural, and Natural Resource Sciences University of Minnesota"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Test."
  },
  {
    "objectID": "content/slides/slides-test.html#what-well-cover-this-week",
    "href": "content/slides/slides-test.html#what-well-cover-this-week",
    "title": "Week 1",
    "section": "What we’ll cover this week",
    "text": "What we’ll cover this week\n\n\nWho am I?\n\n\n\n\nCourse logistics and learning objectives\n\n\n\n\nData best management practices\n\n\n\n\nA brief introduction to data wrangling"
  },
  {
    "objectID": "content/slides/slides-test.html#what-were-doing-this-semester",
    "href": "content/slides/slides-test.html#what-were-doing-this-semester",
    "title": "Week 1",
    "section": "What we’re doing this semester",
    "text": "What we’re doing this semester"
  },
  {
    "objectID": "content/slides/slides-test.html#course-logistics-and-learning-objectives",
    "href": "content/slides/slides-test.html#course-logistics-and-learning-objectives",
    "title": "Week 1",
    "section": "Course logistics and learning objectives",
    "text": "Course logistics and learning objectives"
  },
  {
    "objectID": "content/slides/slides-test.html#jeremy-hemberger",
    "href": "content/slides/slides-test.html#jeremy-hemberger",
    "title": "Week 1",
    "section": "Jeremy Hemberger",
    "text": "Jeremy Hemberger\nAssistant Professor - Department of Entomology\n\n\n\nPI of iBUG lab\nResearch on global change impacts on insects\nTeaching on quantitative methods in ecology"
  },
  {
    "objectID": "content/slides/slides-test.html#jeremy-hemberger-phd",
    "href": "content/slides/slides-test.html#jeremy-hemberger-phd",
    "title": "Week 1",
    "section": "Jeremy Hemberger, PhD",
    "text": "Jeremy Hemberger, PhD\nAssistant Professor - Department of Entomology   👨🏻‍🔬 I lead the Insect Biodiversity under Global Change lab 🐝 Research: global change impacts on insects 👨🏻‍🏫 Teaching: quantitative methods in ecology"
  },
  {
    "objectID": "content/slides/slides-test.html#jeremy-hemberger-phd-1",
    "href": "content/slides/slides-test.html#jeremy-hemberger-phd-1",
    "title": "Week 1",
    "section": "Jeremy Hemberger, PhD",
    "text": "Jeremy Hemberger, PhD\nAssistant Professor - Department of Entomology   🏫 Office location: 408 Hodson  🕥 Office hours: 8:00-12:00 Mondays (in person)"
  },
  {
    "objectID": "content/slides/slides-test.html#what-are-we-doing-here",
    "href": "content/slides/slides-test.html#what-are-we-doing-here",
    "title": "Week 1",
    "section": "What are we doing here?",
    "text": "What are we doing here?\n\n\nThis course is an introduction to the application of statistical methods with biological and ecological data that is temporally or spatially structured.\n\n\n\n\nWe’ll be focusing particularly on working with data, formulating hypotheses, testing model assumptions, and doing basic statistical inference and reporting."
  },
  {
    "objectID": "content/slides/slides-test.html#what-are-we-doing-here-1",
    "href": "content/slides/slides-test.html#what-are-we-doing-here-1",
    "title": "Week 1",
    "section": "What are we doing here?",
    "text": "What are we doing here?\n\n\nWhat this course isn’t is a theoretical statistics course: we’ll be focused on application\n\n\n\n\nI do expect folks have a working understanding of basic calculus and statistics from either courses in high school or undergraduate studies"
  },
  {
    "objectID": "content/slides/slides-test.html#what-this-course-is-about",
    "href": "content/slides/slides-test.html#what-this-course-is-about",
    "title": "Week 1",
    "section": "What this course is about",
    "text": "What this course is about\n\n\nThis course is an introduction to the application of statistical methods with biological and ecological data that is temporally or spatially structured.\n\n\n\n\nWe’ll emphasize: working with data, formulating hypotheses, constructing models, testing assumptions, and doing basic statistical inference and reporting."
  },
  {
    "objectID": "content/slides/slides-test.html#what-this-course-isnt",
    "href": "content/slides/slides-test.html#what-this-course-isnt",
    "title": "Week 1",
    "section": "What this course isn’t",
    "text": "What this course isn’t\n\n\nWe’re not here to discuss theoretical statistics: we’ll be focused on applying foundational statistical tools\n\n\n\n\nMemorizing equations and hand-calculating t-tests and ANOVAs (but, sometimes the latter can be helpful…)"
  },
  {
    "objectID": "content/slides/slides-test.html#some-basic-expectations",
    "href": "content/slides/slides-test.html#some-basic-expectations",
    "title": "Week 1",
    "section": "Some basic expectations",
    "text": "Some basic expectations\n\n\nI do expect folks have a working understanding of basic calculus and statistics from past classes in high school or undergraduate studies\n\n\n\n\nIt’s ok to feel lost or get frustrated. Analyzing data (and working with ) is hard!\n\n\n\n\nAsk questions! Work together! Treat this not like a class, but like a semester long workshop"
  },
  {
    "objectID": "content/slides/slides-test.html#scheduling-stuff",
    "href": "content/slides/slides-test.html#scheduling-stuff",
    "title": "Week 1",
    "section": "Scheduling stuff",
    "text": "Scheduling stuff\nModule 1: RStudio and Tidyverse crash course\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\n\n\n\n\nWeek 1\n\n\n1/21/26\n\n\nOrientation to class\n\n\n\n\n1/23/26\n\n\nData wranglin’\n\n\n\n\nModule 2: Reviewing foundations in statistics\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\n\n\n\n\nWeek 2\n\n\n1/28/26\n\n\nSampling & probability\n\n\n\n\n1/30/26\n\n\nRandom variables and probability distributions\n\n\n\n\nWeek 3\n\n\n2/4/26\n\n\nTransformations\n\n\n\n\n2/6/26\n\n\nHypotheses and one-sample t-tests\n\n\n\n\nWeek 4\n\n\n2/11/26\n\n\nDependent and independent two-sample t-tests\n\n\n\n\n2/13/26\n\n\nAnalysis of variance (ANOVA)\n\n\n\n\nWeek 5\n\n\n2/18/26\n\n\nEvaluating assumptions, linear models, correlation\n\n\n\n\n2/20/26\n\n\nSimple linear regression\n\n\n\n\nWeek 6\n\n\n2/25/26\n\n\nMultiple regression\n\n\n\n\n2/27/26\n\n\nMultiple regression\n\n\n\n\nWeek 7\n\n\n3/4/26\n\n\nModel selection\n\n\n\n\n3/6/26\n\n\nMidterm\n\n\n\n\nModule 3: Integrating time into our models\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\n\n\n\n\nWeek 9\n\n\n3/18/26\n\n\nMidterm review\n\n\n\n\n3/20/26\n\n\nIntro to space and time\n\n\n\n\nWeek 10\n\n\n3/25/26\n\n\nMixed effects models\n\n\n\n\n3/27/26\n\n\nInformation critera\n\n\n\n\nModule 4: Integrating space into our models\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\n\n\n\n\nWeek 11\n\n\n4/1/26\n\n\nSpatial point processes: Ripley’s K\n\n\n\n\n4/3/26\n\n\nGeneralized linear models\n\n\n\n\nWeek 12\n\n\n4/8/26\n\n\nGLMs and analogies to point process models\n\n\n\n\n4/10/26\n\n\nSpatial point processes: models\n\n\n\n\nWeek 13\n\n\n4/15/26\n\n\nLattice data: MW test, Geary’s C, Moran’s I\n\n\n\n\n4/17/26\n\n\nLattice data: SAR and CAR models\n\n\n\n\nWeek 14\n\n\n4/22/26\n\n\nGeostatistics\n\n\n\n\n4/24/26\n\n\nGeostatistics\n\n\n\n\nWeek 15\n\n\n4/29/26\n\n\nCurrent topics: species distribution models\n\n\n\n\n5/1/26\n\n\nCourse wrap-up\n\n\n\n\nWeek 16\n\n\n5/6/26\n\n\nFinal exam"
  },
  {
    "objectID": "content/slides/slides-test.html#roadmap-for-the-semester",
    "href": "content/slides/slides-test.html#roadmap-for-the-semester",
    "title": "Week 1",
    "section": "Roadmap for the semester",
    "text": "Roadmap for the semester\n\nModule 1: RStudio and Tidyverse crash course\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\n\n\n\n\nWeek 1\n\n\n1/21/26\n\n\nOrientation to class\n\n\n\n\n1/23/26\n\n\nData wranglin’\n\n\n\n\nModule 2: Reviewing foundations in statistics\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\n\n\n\n\nWeek 2\n\n\n1/28/26\n\n\nSampling & probability\n\n\n\n\n1/30/26\n\n\nRandom variables and probability distributions\n\n\n\n\nWeek 3\n\n\n2/4/26\n\n\nTransformations\n\n\n\n\n2/6/26\n\n\nHypotheses and one-sample t-tests\n\n\n\n\nWeek 4\n\n\n2/11/26\n\n\nDependent and independent two-sample t-tests\n\n\n\n\n2/13/26\n\n\nAnalysis of variance (ANOVA)\n\n\n\n\nWeek 5\n\n\n2/18/26\n\n\nEvaluating assumptions, linear models, correlation\n\n\n\n\n2/20/26\n\n\nSimple linear regression\n\n\n\n\nWeek 6\n\n\n2/25/26\n\n\nMultiple regression\n\n\n\n\n2/27/26\n\n\nMultiple regression\n\n\n\n\nWeek 7\n\n\n3/4/26\n\n\nModel selection\n\n\n\n\n3/6/26\n\n\nMidterm\n\n\n\n\nModule 3: Integrating time into our models\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\n\n\n\n\nWeek 9\n\n\n3/18/26\n\n\nMidterm review\n\n\n\n\n3/20/26\n\n\nIntro to space and time\n\n\n\n\nWeek 10\n\n\n3/25/26\n\n\nMixed effects models\n\n\n\n\n3/27/26\n\n\nInformation critera\n\n\n\n\nModule 4: Integrating space into our models\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\n\n\n\n\nWeek 11\n\n\n4/1/26\n\n\nSpatial point processes: Ripley’s K\n\n\n\n\n4/3/26\n\n\nGeneralized linear models\n\n\n\n\nWeek 12\n\n\n4/8/26\n\n\nGLMs and analogies to point process models\n\n\n\n\n4/10/26\n\n\nSpatial point processes: models\n\n\n\n\nWeek 13\n\n\n4/15/26\n\n\nLattice data: MW test, Geary’s C, Moran’s I\n\n\n\n\n4/17/26\n\n\nLattice data: SAR and CAR models\n\n\n\n\nWeek 14\n\n\n4/22/26\n\n\nGeostatistics\n\n\n\n\n4/24/26\n\n\nGeostatistics\n\n\n\n\nWeek 15\n\n\n4/29/26\n\n\nCurrent topics: species distribution models\n\n\n\n\n5/1/26\n\n\nCourse wrap-up\n\n\n\n\nWeek 16\n\n\n5/6/26\n\n\nFinal exam"
  },
  {
    "objectID": "content/slides/slides-test.html#learning-objectives-for-the-semester",
    "href": "content/slides/slides-test.html#learning-objectives-for-the-semester",
    "title": "Week 1",
    "section": "Learning objectives for the semester",
    "text": "Learning objectives for the semester\n\nApply the appropriate techniques to characterize spatial and temporal dependence data\nUnderstand how spatial and temporal dependence can affect statistical inference\nIdentify and choose the appropriate analytical methods to contend with such dependence in widely-used statistical methods like regression and ANOVA\nCritically evaluate and understand the strengths and limits of these approaches\nPresent analytical results in a format befitting a scientific presentation or publication"
  },
  {
    "objectID": "content/slides/slides-test.html#course-logistics",
    "href": "content/slides/slides-test.html#course-logistics",
    "title": "Week 1",
    "section": "Course logistics",
    "text": "Course logistics\nMeeting times\nLectures\n🕥 Wednesday and Friday from 9:30-10:20am 📍 Hodson 511 \nLabs\n🕥 Friday from 10:30-12:30 📍 Hodson 511"
  },
  {
    "objectID": "content/slides/slides-test.html#grades-and-assessments",
    "href": "content/slides/slides-test.html#grades-and-assessments",
    "title": "Week 1",
    "section": "Grades and assessments",
    "text": "Grades and assessments\nLecture: 45% of grade including midterm (20%) and comprehensive final (45%) Lab: 55% of grade incuding 10 lab reports (40%) and a lab final (15%) \n\n\n\n\nPoint Breakdown\n\n\nAssignment\nPoints\nPercent\n\n\n\n\nLab reports (10 × 16)\n160\n40%\n\n\nLab final\n60\n15%\n\n\nLecture midterm\n80\n20%\n\n\nLecture final\n100\n25%\n\n\nTotal\n400\n100%\n\n\n\n\n\nGrade Breakdown\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n93–100%\nC\n73–76%\n\n\nA−\n90–92%\nC−\n70–72%\n\n\nB+\n87–89%\nD+\n67–69%\n\n\nB\n83–86%\nD\n63–66%\n\n\nB−\n80–82%\nD−\n60–62%\n\n\nC+\n77–79%\nF\n&lt; 60%"
  },
  {
    "objectID": "content/slides/slides-test.html#learning-objectives-for-the-semester-1",
    "href": "content/slides/slides-test.html#learning-objectives-for-the-semester-1",
    "title": "Week 1",
    "section": "Learning objectives for the semester",
    "text": "Learning objectives for the semester\n\nApply the appropriate techniques to characterize spatial and temporal dependence data\nUnderstand how spatial and temporal dependence can affect statistical inference"
  },
  {
    "objectID": "content/slides/slides-test.html#learning-objectives-for-the-semester-2",
    "href": "content/slides/slides-test.html#learning-objectives-for-the-semester-2",
    "title": "Week 1",
    "section": "Learning objectives for the semester",
    "text": "Learning objectives for the semester\n\nApply the appropriate techniques to characterize spatial and temporal dependence data\nUnderstand how spatial and temporal dependence can affect statistical inference\nIdentify and choose the appropriate analytical methods to contend with such dependence in widely-used statistical methods like regression and ANOVA"
  },
  {
    "objectID": "content/slides/slides-test.html#learning-objectives-for-the-semester-3",
    "href": "content/slides/slides-test.html#learning-objectives-for-the-semester-3",
    "title": "Week 1",
    "section": "Learning objectives for the semester",
    "text": "Learning objectives for the semester\n\nApply the appropriate techniques to characterize spatial and temporal dependence data\nUnderstand how spatial and temporal dependence can affect statistical inference\nIdentify and choose the appropriate analytical methods to contend with such dependence in widely-used statistical methods like regression and ANOVA\nCritically evaluate and understand the strengths and limits of these approaches"
  },
  {
    "objectID": "content/slides/slides-test.html#learning-objectives-for-the-semester-4",
    "href": "content/slides/slides-test.html#learning-objectives-for-the-semester-4",
    "title": "Week 1",
    "section": "Learning objectives for the semester",
    "text": "Learning objectives for the semester\n\nApply the appropriate techniques to characterize spatial and temporal dependence data\nUnderstand how spatial and temporal dependence can affect statistical inference\nIdentify and choose the appropriate analytical methods to contend with such dependence in widely-used statistical methods like regression and ANOVA\nCritically evaluate and understand the strengths and limits of these approaches\nPresent analytical results in a format befitting a scientific presentation or publication"
  },
  {
    "objectID": "content/slides/slides-test.html#course-logistics-1",
    "href": "content/slides/slides-test.html#course-logistics-1",
    "title": "Week 1",
    "section": "Course logistics",
    "text": "Course logistics\nRoadmap for the semester\n\nModule 1: RStudio and Tidyverse crash course\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\n\n\n\n\nWeek 1\n\n\n1/21/26\n\n\nOrientation to class\n\n\n\n\n1/23/26\n\n\nData wranglin’\n\n\n\n\nModule 2: Reviewing foundations in statistics\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\n\n\n\n\nWeek 2\n\n\n1/28/26\n\n\nSampling & probability\n\n\n\n\n1/30/26\n\n\nRandom variables and probability distributions\n\n\n\n\nWeek 3\n\n\n2/4/26\n\n\nTransformations\n\n\n\n\n2/6/26\n\n\nHypotheses and one-sample t-tests\n\n\n\n\nWeek 4\n\n\n2/11/26\n\n\nDependent and independent two-sample t-tests\n\n\n\n\n2/13/26\n\n\nAnalysis of variance (ANOVA)\n\n\n\n\nWeek 5\n\n\n2/18/26\n\n\nEvaluating assumptions, linear models, correlation\n\n\n\n\n2/20/26\n\n\nSimple linear regression\n\n\n\n\nWeek 6\n\n\n2/25/26\n\n\nMultiple regression\n\n\n\n\n2/27/26\n\n\nMultiple regression\n\n\n\n\nWeek 7\n\n\n3/4/26\n\n\nModel selection\n\n\n\n\n3/6/26\n\n\nMidterm\n\n\n\n\nModule 3: Integrating time into our models\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\n\n\n\n\nWeek 9\n\n\n3/18/26\n\n\nMidterm review\n\n\n\n\n3/20/26\n\n\nIntro to space and time\n\n\n\n\nWeek 10\n\n\n3/25/26\n\n\nMixed effects models\n\n\n\n\n3/27/26\n\n\nInformation critera\n\n\n\n\nModule 4: Integrating space into our models\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\n\n\n\n\nWeek 11\n\n\n4/1/26\n\n\nSpatial point processes: Ripley’s K\n\n\n\n\n4/3/26\n\n\nGeneralized linear models\n\n\n\n\nWeek 12\n\n\n4/8/26\n\n\nGLMs and analogies to point process models\n\n\n\n\n4/10/26\n\n\nSpatial point processes: models\n\n\n\n\nWeek 13\n\n\n4/15/26\n\n\nLattice data: MW test, Geary’s C, Moran’s I\n\n\n\n\n4/17/26\n\n\nLattice data: SAR and CAR models\n\n\n\n\nWeek 14\n\n\n4/22/26\n\n\nGeostatistics\n\n\n\n\n4/24/26\n\n\nGeostatistics\n\n\n\n\nWeek 15\n\n\n4/29/26\n\n\nCurrent topics: species distribution models\n\n\n\n\n5/1/26\n\n\nCourse wrap-up\n\n\n\n\nWeek 16\n\n\n5/6/26\n\n\nFinal exam"
  },
  {
    "objectID": "content/slides/slides-test.html#attendance",
    "href": "content/slides/slides-test.html#attendance",
    "title": "Week 1",
    "section": "Attendance",
    "text": "Attendance\nSyllabus is pretty clear on this, I hope you want to be here!"
  },
  {
    "objectID": "content/slides/slides-test.html#ai-llms-bullshit",
    "href": "content/slides/slides-test.html#ai-llms-bullshit",
    "title": "Week 1",
    "section": "AI, LLMs, Bullshit",
    "text": "AI, LLMs, Bullshit\n\n\nI highly recommend NOT using generative AI tools in this course\n\n\n\n\n\n\n\n\n\nTip\n\n\nThere are times when using AI tools are appropriate, but learning to recognize those instances is critical to your learning."
  },
  {
    "objectID": "content/slides/slides-test.html#ai-llms-and-bullshit",
    "href": "content/slides/slides-test.html#ai-llms-and-bullshit",
    "title": "Week 1",
    "section": "AI, LLMs, and Bullshit",
    "text": "AI, LLMs, and Bullshit\n\n\nI highly recommend NOT using generative AI tools in this course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nThere are times when using AI tools are appropriate, but learning to recognize those instances is critical to your learning.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nIf you use LLMs in your work, be honest and report how and why you used them"
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling",
    "href": "content/slides/slides-test.html#data-handling",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\n\n\n“Statistics” or data analysis begins long before you launch \n\n\n\n\nTo call in the statistician after the experiment is done may be no more than asking them to perform a post-mortem examination: they may be able to say what the experiment died of. - Ronald Aylmer Fisher"
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling-1",
    "href": "content/slides/slides-test.html#data-handling-1",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nBest practices\n\n\nFollow your lab’s data management plan\n\n\n\n\nDon’t have one? Lobby your group to create one!\n\n\n\n\nData management plans should stress: data integrity and reproducible research"
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling-2",
    "href": "content/slides/slides-test.html#data-handling-2",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nBest practices\n\nFollowing a rigorous data management plan will help future you to:\n\n\nRemember how and why you performed specific analyses\nQuickly and simply modify analyses and figures long after you have moved on to other projects\nQuickly reconfigure previous coding tasks so you don’t have to reinvent processes\nIndicate rigor, trustworthiness, and transparency to other professionals\nIncrease paper citation rates and allow data and code citation in addition to manuscripts\nMeet journal requirements1\n\nList adapted from Schulte-Moore data management plan"
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling-3",
    "href": "content/slides/slides-test.html#data-handling-3",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nBest practices\n\nBacking up your data is of utmost importance. Follow the 3-2-1 rule:\n\n{.absolute-width=“800” .center-x}"
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling-4",
    "href": "content/slides/slides-test.html#data-handling-4",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nData considerations\n\nData privacy & sensitive data"
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling-5",
    "href": "content/slides/slides-test.html#data-handling-5",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nData entry\n\n\nIn previous courses and statistics textbooks, data are neat and tidy. We’ll be working with biological and ecological data, which are rarely as neatly packaged.\n\n\n\n\n\n\n\n\n\nImportant\n\n\nWhen it comes to analyzing data, 75% or more of your time will be taken up collating, recording, arranging, and tidying your data in preparation for the actual statistical analysis"
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling-6",
    "href": "content/slides/slides-test.html#data-handling-6",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nData entry\n\n\nTypically, data are transcribed from paper data sheets (or directly into) spreadsheets before importing it your desired analysis program.\n\n\n\n\nThe most common structure for entering is “rectangular” or “flat”: each line of the data contains all of the variables for a single observation, even if they are blank\n\n\n\n\n\n\n\n\n\nTip\n\n\nUse a column for each variable; don’t create “amalgam” columns\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nAppropriately setting up and entering data initially will avoid a lot of headaches in the future!"
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling-7",
    "href": "content/slides/slides-test.html#data-handling-7",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nExample of a “bad” setup"
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling-8",
    "href": "content/slides/slides-test.html#data-handling-8",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nExample of an improved setup\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\nEach cell value should only contain data, not formulae or any other dependent expression. All of your data manipulations should happen in  where they are repeatable! Raw data should never be manipulated in Excel."
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling-9",
    "href": "content/slides/slides-test.html#data-handling-9",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nExample of an improved setup\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nDon’t put too much work into making your datasheet “pretty”.  won’t care and it will likely interfere with data importing."
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling-10",
    "href": "content/slides/slides-test.html#data-handling-10",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nExample of data that are too pretty\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nTechnically, these data are “flat”, but none of the formatting or special features will be read into \n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nIf data are never going to be used outside of spreadsheet, you can do what you want! If it’s going into , make it machine readable!"
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling-11",
    "href": "content/slides/slides-test.html#data-handling-11",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nMetadata\n\nData must be kept along with a codebook or metadata that, at the bare minimum, describes the variables in your flat data file. Ideally, it should contain:\n\n\nDescription of how data were collected including sampling design, protocols, etc.\nVariables contained in the spreadsheet\nThe location, format, and units for each variable within the raw data file\nMeaning/definition of coded values for variable’s observation\nFor surveys, the survey instrument/questionaire used to solicit responses along with the coded values for each question\n\n\n\n\n\n\n\n\nTip\n\n\nI recommend keeping this metadata file as a separate plain-text or Markdown file in the same folder as your flat data file."
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling-12",
    "href": "content/slides/slides-test.html#data-handling-12",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nMetadata: example"
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling-13",
    "href": "content/slides/slides-test.html#data-handling-13",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nCommon data import problems\nComputers can have trouble reading and processing data. Some tips to avoid issues when importing into :\n\nDon’t use any spaces in column names or in data entries. Use \"-\" or \"_\". I don’t recommend using \".\"\nDon’t mix characters into columns that are supposed to be numeric, or vice-versa.\nNo special characters (!@#$%^&*&lt;&gt;/\\{}[])"
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling-14",
    "href": "content/slides/slides-test.html#data-handling-14",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nGetting ready to import data into \n\n\nOnce your data is entered, do some QA/QC. This can be done by:\n\n\n\n\nSubsetting several sections and examining for errors\nEmploying friend to examine column/variable names and comparing with metadata (and seeing if they think your names make any sense)\nImport it into R and perform some data cleaning (e.g., use the janitor package, build some basic summaries and plots to see if values make sense)"
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling-15",
    "href": "content/slides/slides-test.html#data-handling-15",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nExporting data\nMany programs, R included, are capable of directly handling propriatary file-types like .xcel files. However, you should save your file as something universally useable by folks who may not have access to Excel! These include:\n\n*.txt plain text files\n*.txt tab-delimited plain text\n*.dat space-delimited plain text\n*.csv comma separated values\n\n\n\n\n\n\n\nNote\n\n\nMost of us will work primarily with .csv files, and these are most often the files we’ll see in online data repositories for tabular data."
  },
  {
    "objectID": "content/slides/slides-test.html#tldr",
    "href": "content/slides/slides-test.html#tldr",
    "title": "Week 1",
    "section": "TLDR",
    "text": "TLDR\n\n\n\n\n\n\n\nBlah blah blah"
  },
  {
    "objectID": "content/slides/slides-test.html#data-handling-tldr",
    "href": "content/slides/slides-test.html#data-handling-tldr",
    "title": "Week 1",
    "section": "Data handling: TLDR",
    "text": "Data handling: TLDR\n\n Develop a complete data management plan\n\n\n Spend time and energy properly entering and managing data.\n\n\n Develop clear documentation and metadata surrounding your data set"
  },
  {
    "objectID": "content/slides/w1-slides.html#what-well-cover-this-week",
    "href": "content/slides/w1-slides.html#what-well-cover-this-week",
    "title": "Week 1",
    "section": "What we’ll cover this week",
    "text": "What we’ll cover this week\n\n\n Who am I?\n\n\n\n\n Course logistics and learning objectives\n\n\n\n\n Data best management practices\n\n\n\n\n A brief introduction to data wranglin’"
  },
  {
    "objectID": "content/slides/w1-slides.html#jeremy-hemberger-phd",
    "href": "content/slides/w1-slides.html#jeremy-hemberger-phd",
    "title": "Week 1",
    "section": "Jeremy Hemberger, PhD",
    "text": "Jeremy Hemberger, PhD\nAssistant Professor - Department of Entomology   👨🏻‍🔬 I lead the Insect Biodiversity under Global Change lab 🐝 Research: global change impacts on insects 👨🏻‍🏫 Teaching: quantitative methods in ecology"
  },
  {
    "objectID": "content/slides/w1-slides.html#jeremy-hemberger-phd-1",
    "href": "content/slides/w1-slides.html#jeremy-hemberger-phd-1",
    "title": "Week 1",
    "section": "Jeremy Hemberger, PhD",
    "text": "Jeremy Hemberger, PhD\nAssistant Professor - Department of Entomology   🏫 Office location: 408 Hodson  🕥 Office hours: 8:00-12:00 Mondays (in person)"
  },
  {
    "objectID": "content/slides/w1-slides.html#what-this-course-is-about",
    "href": "content/slides/w1-slides.html#what-this-course-is-about",
    "title": "Week 1",
    "section": "What this course is about",
    "text": "What this course is about\n\n\nThis course is an introduction to the application of statistical methods with biological and ecological data that is temporally or spatially structured.\n\n\n\n\nWe’ll emphasize: working with data, formulating hypotheses, constructing models, testing assumptions, and doing basic statistical inference and reporting."
  },
  {
    "objectID": "content/slides/w1-slides.html#what-this-course-isnt",
    "href": "content/slides/w1-slides.html#what-this-course-isnt",
    "title": "Week 1",
    "section": "What this course isn’t",
    "text": "What this course isn’t\n\n\nWe’re not here to discuss theoretical statistics: we’ll be focused on applying foundational statistical tools\n\n\n\n\nMemorizing equations and hand-calculating t-tests and ANOVAs (but, sometimes the latter can be helpful…)"
  },
  {
    "objectID": "content/slides/w1-slides.html#some-basic-expectations",
    "href": "content/slides/w1-slides.html#some-basic-expectations",
    "title": "Week 1",
    "section": "Some basic expectations",
    "text": "Some basic expectations\n\n\nI do expect folks have a working understanding of basic calculus and statistics from past classes in high school or undergraduate studies\n\n\n\n\nIt’s ok to feel lost or get frustrated. Analyzing data (and working with ) is hard!\n\n\n\n\nAsk questions! Work together! Treat this not like a class, but like a semester long workshop"
  },
  {
    "objectID": "content/slides/w1-slides.html#learning-objectives-for-the-semester",
    "href": "content/slides/w1-slides.html#learning-objectives-for-the-semester",
    "title": "Week 1",
    "section": "Learning objectives for the semester",
    "text": "Learning objectives for the semester\n\nApply the appropriate techniques to characterize spatial and temporal dependence data\nUnderstand how spatial and temporal dependence can affect statistical inference\nIdentify and choose the appropriate analytical methods to contend with such dependence in widely-used statistical methods like regression and ANOVA\nCritically evaluate and understand the strengths and limits of these approaches\nPresent analytical results in a format befitting a scientific presentation or publication"
  },
  {
    "objectID": "content/slides/w1-slides.html#course-logistics",
    "href": "content/slides/w1-slides.html#course-logistics",
    "title": "Week 1",
    "section": "Course logistics",
    "text": "Course logistics\nMeeting times\nLectures\n🕥 Wednesday and Friday from 9:30-10:20am 📍 Hodson 511 \nLabs\n🕥 Friday from 10:30-12:30 📍 Hodson 511"
  },
  {
    "objectID": "content/slides/w1-slides.html#course-logistics-1",
    "href": "content/slides/w1-slides.html#course-logistics-1",
    "title": "Week 1",
    "section": "Course logistics",
    "text": "Course logistics\nRoadmap for the semester\n\nModule 1: RStudio and Tidyverse crash course\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\n\n\n\n\nWeek 1\n\n\n1/21/26\n\n\nOrientation to class\n\n\n\n\n1/23/26\n\n\nData wranglin’\n\n\n\n\nModule 2: Reviewing foundations in statistics\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\n\n\n\n\nWeek 2\n\n\n1/28/26\n\n\nSampling & probability\n\n\n\n\n1/30/26\n\n\nRandom variables and probability distributions\n\n\n\n\nWeek 3\n\n\n2/4/26\n\n\nTransformations\n\n\n\n\n2/6/26\n\n\nHypotheses and one-sample t-tests\n\n\n\n\nWeek 4\n\n\n2/11/26\n\n\nDependent and independent two-sample t-tests\n\n\n\n\n2/13/26\n\n\nAnalysis of variance (ANOVA)\n\n\n\n\nWeek 5\n\n\n2/18/26\n\n\nEvaluating assumptions, linear models, correlation\n\n\n\n\n2/20/26\n\n\nSimple linear regression\n\n\n\n\nWeek 6\n\n\n2/25/26\n\n\nMultiple regression\n\n\n\n\n2/27/26\n\n\nMultiple regression\n\n\n\n\nWeek 7\n\n\n3/4/26\n\n\nModel selection\n\n\n\n\n3/6/26\n\n\nMidterm\n\n\n\n\nModule 3: Integrating time into our models\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\n\n\n\n\nWeek 9\n\n\n3/18/26\n\n\nMidterm review\n\n\n\n\n3/20/26\n\n\nIntro to space and time\n\n\n\n\nWeek 10\n\n\n3/25/26\n\n\nMixed effects models\n\n\n\n\n3/27/26\n\n\nInformation critera\n\n\n\n\nModule 4: Integrating space into our models\n\n\n\n\nWeek\n\n\nDate\n\n\nTopic\n\n\n\n\n\n\nWeek 11\n\n\n4/1/26\n\n\nSpatial point processes: Ripley’s K\n\n\n\n\n4/3/26\n\n\nGeneralized linear models\n\n\n\n\nWeek 12\n\n\n4/8/26\n\n\nGLMs and analogies to point process models\n\n\n\n\n4/10/26\n\n\nSpatial point processes: models\n\n\n\n\nWeek 13\n\n\n4/15/26\n\n\nLattice data: MW test, Geary’s C, Moran’s I\n\n\n\n\n4/17/26\n\n\nLattice data: SAR and CAR models\n\n\n\n\nWeek 14\n\n\n4/22/26\n\n\nGeostatistics\n\n\n\n\n4/24/26\n\n\nGeostatistics\n\n\n\n\nWeek 15\n\n\n4/29/26\n\n\nCurrent topics: species distribution models\n\n\n\n\n5/1/26\n\n\nCourse wrap-up\n\n\n\n\nWeek 16\n\n\n5/6/26\n\n\nFinal exam"
  },
  {
    "objectID": "content/slides/w1-slides.html#grades-and-assessments",
    "href": "content/slides/w1-slides.html#grades-and-assessments",
    "title": "Week 1",
    "section": "Grades and assessments",
    "text": "Grades and assessments\nLecture: 45% of grade including midterm (20%) and comprehensive final (45%) Lab: 55% of grade incuding 10 lab reports (40%) and a lab final (15%) \n\n\n\n\nPoint Breakdown\n\n\nAssignment\nPoints\nPercent\n\n\n\n\nLab reports (10 × 16)\n160\n40%\n\n\nLab final\n60\n15%\n\n\nLecture midterm\n80\n20%\n\n\nLecture final\n100\n25%\n\n\nTotal\n400\n100%\n\n\n\n\n\nGrade Breakdown\n\n\nGrade\nRange\nGrade\nRange\n\n\n\n\nA\n93–100%\nC\n73–76%\n\n\nA−\n90–92%\nC−\n70–72%\n\n\nB+\n87–89%\nD+\n67–69%\n\n\nB\n83–86%\nD\n63–66%\n\n\nB−\n80–82%\nD−\n60–62%\n\n\nC+\n77–79%\nF\n&lt; 60%"
  },
  {
    "objectID": "content/slides/w1-slides.html#attendance",
    "href": "content/slides/w1-slides.html#attendance",
    "title": "Week 1",
    "section": "Attendance",
    "text": "Attendance\nSyllabus is pretty clear on this, I hope you want to be here!"
  },
  {
    "objectID": "content/slides/w1-slides.html#ai-llms-and-bullshit",
    "href": "content/slides/w1-slides.html#ai-llms-and-bullshit",
    "title": "Week 1",
    "section": "AI, LLMs, and Bullshit",
    "text": "AI, LLMs, and Bullshit\n\n\nI highly recommend NOT using generative AI tools in this course\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nThere are times when using AI tools are appropriate, but learning to recognize those instances is critical to your learning.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nIf you use LLMs in your work, be honest and report how and why you used them"
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling",
    "href": "content/slides/w1-slides.html#data-handling",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\n\n\n“Statistics” or data analysis begins long before you launch \n\n\n\n\nTo call in the statistician after the experiment is done may be no more than asking them to perform a post-mortem examination: they may be able to say what the experiment died of. - Ronald Aylmer Fisher"
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling-1",
    "href": "content/slides/w1-slides.html#data-handling-1",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nBest practices\n\n\nFollow your lab’s data management plan\n\n\n\n\nDon’t have one? Lobby your group to create one!\n\n\n\n\nData management plans should stress: data integrity and reproducible research"
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling-2",
    "href": "content/slides/w1-slides.html#data-handling-2",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nBest practices\n\nFollowing a rigorous data management plan will help future you to:\n\n\nRemember how and why you performed specific analyses\nQuickly and simply modify analyses and figures long after you have moved on to other projects\nQuickly reconfigure previous coding tasks so you don’t have to reinvent processes\nIndicate rigor, trustworthiness, and transparency to other professionals\nIncrease paper citation rates and allow data and code citation in addition to manuscripts\nMeet journal requirements1\n\nList adapted from Schulte-Moore data management plan"
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling-3",
    "href": "content/slides/w1-slides.html#data-handling-3",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nBest practices\n\nBacking up your data is of utmost importance. Follow the 3-2-1 rule:\n\n{.absolute-width=“800” .center-x}"
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling-4",
    "href": "content/slides/w1-slides.html#data-handling-4",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nData considerations\n\nData privacy & sensitive data"
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling-5",
    "href": "content/slides/w1-slides.html#data-handling-5",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nData entry\n\n\nIn previous courses and statistics textbooks, data are neat and tidy. We’ll be working with biological and ecological data, which are rarely as neatly packaged.\n\n\n\n\n\n\n\n\n\nImportant\n\n\nWhen it comes to analyzing data, 75% or more of your time will be taken up collating, recording, arranging, and tidying your data in preparation for the actual statistical analysis"
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling-6",
    "href": "content/slides/w1-slides.html#data-handling-6",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nData entry\n\n\nTypically, data are transcribed from paper data sheets (or directly into) spreadsheets before importing it your desired analysis program.\n\n\n\n\nThe most common structure for entering is “rectangular” or “flat”: each line of the data contains all of the variables for a single observation, even if they are blank\n\n\n\n\n\n\n\n\n\nTip\n\n\nUse a column for each variable; don’t create “amalgam” columns\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nAppropriately setting up and entering data initially will avoid a lot of headaches in the future!"
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling-7",
    "href": "content/slides/w1-slides.html#data-handling-7",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nExample of a “bad” setup"
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling-8",
    "href": "content/slides/w1-slides.html#data-handling-8",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nExample of an improved setup\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\nEach cell value should only contain data, not formulae or any other dependent expression. All of your data manipulations should happen in  where they are repeatable! Raw data should never be manipulated in Excel."
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling-9",
    "href": "content/slides/w1-slides.html#data-handling-9",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nExample of an improved setup\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nDon’t put too much work into making your datasheet “pretty”.  won’t care and it will likely interfere with data importing."
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling-10",
    "href": "content/slides/w1-slides.html#data-handling-10",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nExample of data that are too pretty\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nTechnically, these data are “flat”, but none of the formatting or special features will be read into \n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nIf data are never going to be used outside of spreadsheet, you can do what you want! If it’s going into , make it machine readable!"
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling-11",
    "href": "content/slides/w1-slides.html#data-handling-11",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nMetadata\n\nData must be kept along with a codebook or metadata that, at the bare minimum, describes the variables in your flat data file. Ideally, it should contain:\n\n\nDescription of how data were collected including sampling design, protocols, etc.\nVariables contained in the spreadsheet\nThe location, format, and units for each variable within the raw data file\nMeaning/definition of coded values for variable’s observation\nFor surveys, the survey instrument/questionaire used to solicit responses along with the coded values for each question\n\n\n\n\n\n\n\n\nTip\n\n\nI recommend keeping this metadata file as a separate plain-text or Markdown file in the same folder as your flat data file."
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling-12",
    "href": "content/slides/w1-slides.html#data-handling-12",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nMetadata: example"
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling-13",
    "href": "content/slides/w1-slides.html#data-handling-13",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nCommon data import problems\nComputers can have trouble reading and processing data. Some tips to avoid issues when importing into :\n\nDon’t use any spaces in column names or in data entries. Use \"-\" or \"_\". I don’t recommend using \".\"\nDon’t mix characters into columns that are supposed to be numeric, or vice-versa.\nNo special characters (!@#$%^&*&lt;&gt;/\\{}[])"
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling-14",
    "href": "content/slides/w1-slides.html#data-handling-14",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nGetting ready to import data into \n\n\nOnce your data is entered, do some QA/QC. This can be done by:\n\n\n\n\nSubsetting several sections and examining for errors\nEmploying friend to examine column/variable names and comparing with metadata (and seeing if they think your names make any sense)\nImport it into R and perform some data cleaning (e.g., use the janitor package, build some basic summaries and plots to see if values make sense)"
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling-15",
    "href": "content/slides/w1-slides.html#data-handling-15",
    "title": "Week 1",
    "section": "Data handling",
    "text": "Data handling\nExporting data\nMany programs, R included, are capable of directly handling propriatary file-types like .xcel files. However, you should save your file as something universally useable by folks who may not have access to Excel! These include:\n\n*.txt plain text files\n*.txt tab-delimited plain text\n*.dat space-delimited plain text\n*.csv comma separated values\n\n\n\n\n\n\n\nNote\n\n\nMost of us will work primarily with .csv files, and these are most often the files we’ll see in online data repositories for tabular data."
  },
  {
    "objectID": "content/slides/w1-slides.html#data-handling-tldr",
    "href": "content/slides/w1-slides.html#data-handling-tldr",
    "title": "Week 1",
    "section": "Data handling: TLDR",
    "text": "Data handling: TLDR\n\n Develop a complete data management plan\n\n\n Spend time and energy properly entering and managing data.\n\n\n Develop clear documentation and metadata surrounding your data set"
  },
  {
    "objectID": "content/w2-content.html",
    "href": "content/w2-content.html",
    "title": "Week 2",
    "section": "",
    "text": "View slides in full screen\n       \n      \n    \n  \n\n\n\n    View slides in full screen\n       \n      \n    \n  \n\n\n\n\n\n\nTip\n\n\n\nTip: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.",
    "crumbs": [
      "Content",
      "Module 2: Reviewing foundations in statistics",
      "Week 2: Sampling, probability, random variables"
    ]
  },
  {
    "objectID": "content/w2-content.html#slides",
    "href": "content/w2-content.html#slides",
    "title": "Week 2",
    "section": "",
    "text": "View slides in full screen\n       \n      \n    \n  \n\n\n\n    View slides in full screen\n       \n      \n    \n  \n\n\n\n\n\n\nTip\n\n\n\nTip: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.",
    "crumbs": [
      "Content",
      "Module 2: Reviewing foundations in statistics",
      "Week 2: Sampling, probability, random variables"
    ]
  },
  {
    "objectID": "content/slides/w2-slides.html",
    "href": "content/slides/w2-slides.html",
    "title": "Week 2",
    "section": "",
    "text": "Who am I?\n\n\n\n\n\n\n\nWho am I?"
  },
  {
    "objectID": "content/slides/w2-slides.html#slide-1.1",
    "href": "content/slides/w2-slides.html#slide-1.1",
    "title": "Week 2",
    "section": "Slide 1.1",
    "text": "Slide 1.1\n\n\nWho am I?"
  },
  {
    "objectID": "content/slides/w2-slides.html#slide-1.2",
    "href": "content/slides/w2-slides.html#slide-1.2",
    "title": "Week 2",
    "section": "Slide 1.2",
    "text": "Slide 1.2\n\n\nWho am I?"
  },
  {
    "objectID": "content/slides/w2-slides.html#slide-2.1",
    "href": "content/slides/w2-slides.html#slide-2.1",
    "title": "Week 2",
    "section": "Slide 2.1",
    "text": "Slide 2.1\n\n\nWho am I?"
  },
  {
    "objectID": "content/slides/w2-slides.html#slide-2.2",
    "href": "content/slides/w2-slides.html#slide-2.2",
    "title": "Week 2",
    "section": "Slide 2.2",
    "text": "Slide 2.2\n\n\nWho am I?"
  },
  {
    "objectID": "content/slides/w2-slides.html#what-well-cover-this-week",
    "href": "content/slides/w2-slides.html#what-well-cover-this-week",
    "title": "Week 2",
    "section": "What we’ll cover this week",
    "text": "What we’ll cover this week\n\n\n What is statistics?\n\n\n\n\n Sampling\n\n\n\n\n Probability\n\n\n\n\n Random variables"
  },
  {
    "objectID": "content/slides/w1-slides.html#learning-objectives-for-the-semester-.incremental",
    "href": "content/slides/w1-slides.html#learning-objectives-for-the-semester-.incremental",
    "title": "Week 1",
    "section": "Learning objectives for the semester {.incremental}",
    "text": "Learning objectives for the semester {.incremental}\n\nApply the appropriate techniques to characterize spatial and temporal dependence data\nUnderstand how spatial and temporal dependence can affect statistical inference\nIdentify and choose the appropriate analytical methods to contend with such dependence in widely-used statistical methods like regression and ANOVA\nCritically evaluate and understand the strengths and limits of these approaches\nPresent analytical results in a format befitting a scientific presentation or publication"
  },
  {
    "objectID": "content/slides/w2-slides.html#definition",
    "href": "content/slides/w2-slides.html#definition",
    "title": "Week 2",
    "section": "Definition",
    "text": "Definition\n\n\nStatistics is the art / science of collecting, analyzing, interpreting, and presenting data"
  },
  {
    "objectID": "content/slides/w2-slides.html#so-is-it-an-art-or-a-science",
    "href": "content/slides/w2-slides.html#so-is-it-an-art-or-a-science",
    "title": "Week 2",
    "section": "So is it an art or a science?",
    "text": "So is it an art or a science?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nThe art/science of designing and conducting a good experiment or observational study makes the art/science of doing statistics far easier."
  },
  {
    "objectID": "content/slides/w2-slides.html#why-do-we-need-statistics",
    "href": "content/slides/w2-slides.html#why-do-we-need-statistics",
    "title": "Week 2",
    "section": "Why do we need statistics?",
    "text": "Why do we need statistics?\n\n\nStatistics are an integral component of the scientific process that allow us separate signal from noise in complex biological and ecological systems so that we can understand how systems work and make informed decisions instead of just…\n…going with your gut and “vibes”."
  },
  {
    "objectID": "content/slides/w2-slides.html#population-vs.-sample",
    "href": "content/slides/w2-slides.html#population-vs.-sample",
    "title": "Week 2",
    "section": "Population vs. sample",
    "text": "Population vs. sample\n\n\nA population is a whole set/group of individuals/objects which we are interested in studying. I.e. the complete set of all possible observations (finite or infinite)\n\n All plants of species X, all lakes in Minnesota\n\n\n\nA sample is a representative subset of the population that we actually observe in order to learn about the population\n\n 100 plants of species X at 25 sites, 50 random lakes in Minnesota"
  },
  {
    "objectID": "content/slides/w2-slides.html#population-vs.-sample-1",
    "href": "content/slides/w2-slides.html#population-vs.-sample-1",
    "title": "Week 2",
    "section": "Population vs. sample",
    "text": "Population vs. sample\nWe use different symbols/notation for the same parameters depending on whether we’re referring to the population or to the sample\n Mean:\\(\\mu\\) (population) vs \\(\\bar{x}\\) (sample)\n Variance:\\(\\sigma^2\\) (population) vs \\(s^2\\) (sample)"
  },
  {
    "objectID": "content/slides/w2-slides.html#primary-realms-of-statistics",
    "href": "content/slides/w2-slides.html#primary-realms-of-statistics",
    "title": "Week 2",
    "section": "Primary realms of statistics",
    "text": "Primary realms of statistics\nDescriptive statistics\nProbability\nInferential statistics"
  },
  {
    "objectID": "content/slides/w2-slides.html#the-primary-realms-of-statistics",
    "href": "content/slides/w2-slides.html#the-primary-realms-of-statistics",
    "title": "Week 2",
    "section": "The primary realms of statistics",
    "text": "The primary realms of statistics\n\n\nDescriptive statistics\n\n\nProbability\n\n\nInferential statistics\n\n\n\nTogether, these elements are used to make inferences about a collection of data and address specific hypotheses"
  },
  {
    "objectID": "content/slides/w2-slides.html#realms-of-statistics",
    "href": "content/slides/w2-slides.html#realms-of-statistics",
    "title": "Week 2",
    "section": "Realms of statistics",
    "text": "Realms of statistics\nDescriptive statistics\n\nDisplaying and summarizing the main features of data in a sample\n\n e.g., mean, median, mode, max, min"
  },
  {
    "objectID": "content/slides/w2-slides.html#the-two-primary-realms-of-statistics",
    "href": "content/slides/w2-slides.html#the-two-primary-realms-of-statistics",
    "title": "Week 2",
    "section": "The two primary realms of statistics",
    "text": "The two primary realms of statistics\n\n\nDescriptive statistics\n\n\nInferential statistics\n\n\n\nTogether, these elements are used to make inferences about a collection of data and address specific hypotheses"
  },
  {
    "objectID": "content/slides/w2-slides.html#the-three-primary-realms-of-statistics",
    "href": "content/slides/w2-slides.html#the-three-primary-realms-of-statistics",
    "title": "Week 2",
    "section": "The three primary realms of statistics",
    "text": "The three primary realms of statistics\n\n\nDescriptive statistics\n\n\nProbability\n\n\nInferential statistics\n\n\n\nTogether, these elements are used to make inferences about a collection of data and address specific hypotheses"
  },
  {
    "objectID": "content/slides/w2-slides.html#realms-of-statistics-1",
    "href": "content/slides/w2-slides.html#realms-of-statistics-1",
    "title": "Week 2",
    "section": "Realms of statistics",
    "text": "Realms of statistics\nProbability: the foundation of statistics\nGiven a population, understand the uncertainty associated with taking a sample taken from the population\nProbability is the mathematics of chance and randomness. Properties of a population are assumed as known and questions about a sample are posed and answered. The approach is deductive (general  specific)"
  },
  {
    "objectID": "content/slides/w2-slides.html#realms-of-statistics-2",
    "href": "content/slides/w2-slides.html#realms-of-statistics-2",
    "title": "Week 2",
    "section": "Realms of statistics",
    "text": "Realms of statistics\nInferential statistics\nGiven a sample, learn methods to draw conclusions about a population while taking into account uncertainties in the sample.\nIn inferential statistics, properties of a sample are available and conclusions about the population are drawn based on the sample. The approach is inductive (specific  general)"
  },
  {
    "objectID": "content/slides/w2-slides.html#discussion-questions",
    "href": "content/slides/w2-slides.html#discussion-questions",
    "title": "Week 2",
    "section": "Discussion questions ",
    "text": "Discussion questions \nPair up and have a quick chat (2-3 minutes) about the following questions. We’ll then discuss as a larger group.\n\n\n Are statistics always necessary for population-level inferences?\n\n\n\n\n Can you think of situations where sampling is needed for inference?\n\n\n\n\n How can we be sure our sampling schemes are representative of the population?"
  },
  {
    "objectID": "content/slides/w2-slides.html#bias",
    "href": "content/slides/w2-slides.html#bias",
    "title": "Week 2",
    "section": "Bias ",
    "text": "Bias \n\nBias is an extremely important concept in science and statistics: it is a type of systematic error (as opposed to random error) that can impact:\n\n\n Study design (e.g., improper design of experiment)\n Data collection (e.g., sampling from only one side of a field)\n Statistical analyses (e.g., improperly accounting for confounding variables)\n\n\nCollectively, bias impacts our interpretation of our research and threatens the scientific enterprise writ large. It’s super important that we attempt to control or account for as many potential sources of bias as possible.\nIn this course, one of our objectives is to account for the impact of space and time as a source of bias in our data."
  },
  {
    "objectID": "content/slides/w2-slides.html#probability-1",
    "href": "content/slides/w2-slides.html#probability-1",
    "title": "Week 2",
    "section": "Probability 1",
    "text": "Probability 1\nThe foundation of statistics\n\nThe concept of probability is central to all statistical methods & sub-disciplines. Here, we define it as the likely outcome of an event of which we are unsure (e.g., a coin toss)\n\n. . .\n\\[\nProbability = \\frac{n\\:outcomes}{n\\:trials}\n\\]\n. . .\n\\[\n0 ≤ P ≤ 1\n\\]\n. . .\nUncertainty arises because of natural (i.e. random) variation in the world we live in"
  },
  {
    "objectID": "content/slides/w2-slides.html#probability-2",
    "href": "content/slides/w2-slides.html#probability-2",
    "title": "Week 2",
    "section": "Probability 2",
    "text": "Probability 2\nThe foundation of statistics\n\nReminder, probability is defined as the likely outcome of an event of which we are unsure. To measure probability, we need to consider:\n\n. . .\n\n Events: a simple process with a well-defined beginning and end\n\n. . .\n\n Outcomes: the result of an event. Defining ecological outcomes can be tricky\n\n. . .\n\n Discrete outcomes: outcome assigned a positive integer (e.g., live = 1, dead = 0)\n\n. . .\n\n Sample space: The set formed from all of the possible outcomes\n\n. . .\n\n Trials: a single event, from start to finish, ending in an outcome\n\n. . .\n\n Replicates: a single trial\n\n. . .\n\n Experiments: a collection of trials"
  },
  {
    "objectID": "content/slides/w2-slides.html#random-variables-1",
    "href": "content/slides/w2-slides.html#random-variables-1",
    "title": "Week 2",
    "section": "Random variables",
    "text": "Random variables\nNeither random, nor variable…🙄\n\nA variable is the actual property measured by individual observations in a trial (e.g., length, pH, fitness)\n\n\n\nA random variable is a variable whose values are not known before a sample is taken — a variable that depends on the outcome of a chance situation.\n\n\n\n\n Mathematically, it’s a rule or a function that assigns a numerical value to each possible outcome of an experiment in the sample space."
  },
  {
    "objectID": "content/slides/w1-slides.html",
    "href": "content/slides/w1-slides.html",
    "title": "Week 1",
    "section": "",
    "text": ". . .\n\n Who am I?\n\n. . .\n\n Course logistics and learning objectives\n\n. . .\n\n Data best management practices\n\n. . .\n\n A brief introduction to data wranglin’"
  },
  {
    "objectID": "content/slides/w1-slides.html#footnotes",
    "href": "content/slides/w1-slides.html#footnotes",
    "title": "Week 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nList adapted from Schulte-Moore data management plan↩︎"
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "Entomology 5126",
    "section": "Instructor",
    "text": "Instructor\n Dr. Jeremy Hemberger  408 Hodson Hall  jhemberg@umn.edu"
  },
  {
    "objectID": "index.html#lecture-details",
    "href": "index.html#lecture-details",
    "title": "Entomology 5126",
    "section": "Lecture details",
    "text": "Lecture details\n Wednesday and Friday  9:30-10:20am  408 Hodson Hall"
  },
  {
    "objectID": "index.html#lab-details",
    "href": "index.html#lab-details",
    "title": "Entomology 5126",
    "section": "Lab details",
    "text": "Lab details\n Friday  10:30-12:30pm  408 Hodson Hall"
  },
  {
    "objectID": "index.html#important-links",
    "href": "index.html#important-links",
    "title": "Entomology 5126",
    "section": "Important links",
    "text": "Important links\n Canvas Site\n Class Github"
  },
  {
    "objectID": "content/slides/w2-slides.html#probability-key-concepts",
    "href": "content/slides/w2-slides.html#probability-key-concepts",
    "title": "Week 2",
    "section": "Probability: key concepts",
    "text": "Probability: key concepts\n\nReminder, probability is defined as the likely outcome of an event of which we are unsure. To measure probability, we need to consider:\n\n\n Events: a simple process with a well-defined beginning and end\n\n\n Outcomes: the result of an event. Defining ecological outcomes can be tricky\n\n\n Discrete outcomes: outcome assigned a positive integer (e.g., live = 1, dead = 0)\n\n\n Sample space: The set formed from all of the possible outcomes\n\n\n Trials: a single event, from start to finish, ending in an outcome\n\n\n Replicates: a single trial\n\n\n Experiments: a collection of trials"
  },
  {
    "objectID": "content/slides/w2-slides.html#a-brief-survey-of-basic-statistical-concepts",
    "href": "content/slides/w2-slides.html#a-brief-survey-of-basic-statistical-concepts",
    "title": "Week 2",
    "section": "A brief survey of basic statistical concepts",
    "text": "A brief survey of basic statistical concepts\n\n\n What is statistics?\n\n\n\n\n Sampling\n\n\n\n\n Probability\n\n\n\n\n Random variables"
  },
  {
    "objectID": "content/slides/w2-slides.html#population-vs.-sample-2",
    "href": "content/slides/w2-slides.html#population-vs.-sample-2",
    "title": "Week 2",
    "section": "Population vs. sample 2",
    "text": "Population vs. sample 2\nWe use different symbols/notation for the same parameters depending on whether we’re referring to the population or to the sample\n Mean:\\(\\mu\\) (population) vs \\(\\bar{x}\\) (sample)\n Variance:\\(\\sigma^2\\) (population) vs \\(s^2\\) (sample)"
  },
  {
    "objectID": "content/slides/w2-slides.html#neither-random-nor-variable",
    "href": "content/slides/w2-slides.html#neither-random-nor-variable",
    "title": "Week 2",
    "section": "Neither random, nor variable…🙄",
    "text": "Neither random, nor variable…🙄\n\nA variable is the actual property measured by individual observations in a trial (e.g., length, pH, fitness)\n\n\n\nA random variable is a variable whose values are not known before a sample is taken — a variable that depends on the outcome of a chance situation.\n\n\n\n\n Mathematically, it’s a rule or a function that assigns a numerical value to each possible outcome of an experiment in the sample space."
  },
  {
    "objectID": "content/slides/w2-slides.html#the-foundation-of-statistics",
    "href": "content/slides/w2-slides.html#the-foundation-of-statistics",
    "title": "Week 2",
    "section": "The foundation of statistics",
    "text": "The foundation of statistics\n\nThe concept of probability is central to all statistical methods & sub-disciplines. Here, we define it as the likely outcome of an event of which we are unsure (e.g., a coin toss)\n\n\n\\[\nProbability = \\frac{n\\:outcomes}{n\\:trials}\n\\]\n\n\n\\[\n0 ≤ P ≤ 1\n\\]\n\n\nUncertainty arises because of natural (i.e. random) variation in the world we live in"
  },
  {
    "objectID": "content/slides/w2-slides.html#probability-is-the-foundation-of-statistics",
    "href": "content/slides/w2-slides.html#probability-is-the-foundation-of-statistics",
    "title": "Week 2",
    "section": "Probability is the foundation of statistics",
    "text": "Probability is the foundation of statistics\n\nThe concept of probability is central to all statistical methods & sub-disciplines. Here, we define it as the likely outcome of an event of which we are unsure (e.g., a coin toss)\n\n\n\\[ Probability = \\frac{n\\:outcomes}{n\\:trials} \\]\n\n\n\\[ 0 ≤ P ≤ 1 \\]\n\n\nUncertainty arises because of natural (i.e., random) variation in the world we live in"
  },
  {
    "objectID": "content/slides/w2-slides.html#probability-1-1",
    "href": "content/slides/w2-slides.html#probability-1-1",
    "title": "Week 2",
    "section": "Probability 1",
    "text": "Probability 1\nThe foundation of statistics\n\nThe concept of probability is central to all statistical methods & sub-disciplines. Here, we define it as the likely outcome of an event of which we are unsure (e.g., a coin toss)\n\n. . .\n\\[\nProbability = \\frac{n\\:outcomes}{n\\:trials}\n\\]\n. . .\n\\[\n0 ≤ P ≤ 1\n\\]\n. . .\nUncertainty arises because of natural (i.e. random) variation in the world we live in"
  },
  {
    "objectID": "content/slides/w2-slides.html#some-key-terms-that-we-need-to-know",
    "href": "content/slides/w2-slides.html#some-key-terms-that-we-need-to-know",
    "title": "Week 2",
    "section": "Some key terms that we need to know",
    "text": "Some key terms that we need to know\n\nReminder, probability is defined as the likely outcome of an event of which we are unsure. To measure probability, we need to consider…\n\n\n\n Events: a simple process with a well-defined beginning and end\n\n\n\n\n Outcomes: the result of an event. Defining ecological outcomes can be tricky\n\n\n\n\n Discrete outcomes: outcome assigned a positive integer (e.g., live = 1, dead = 0)\n\n\n\n\n Sample space: The set formed from all of the possible outcomes\n\n\n\n\n Trials: a single event, from start to finish, ending in an outcome\n\n\n\n\n Replicates: a single trial\n\n\n\n\n Experiments: a collection of trials"
  },
  {
    "objectID": "content/slides/w2-slides.html#types-of-random-variables",
    "href": "content/slides/w2-slides.html#types-of-random-variables",
    "title": "Week 2",
    "section": "Types of random variables",
    "text": "Types of random variables\nThere are two primary types of random variables\n\nA discrete random variable are characterized by a finite number of possible values (e.g., \\(y_1, .... y_n\\)).\n\n\n\n Examples include presence/absence of a species, number of offspring, number of seeds germinating\n\n\n\n\nA continuous random variable are those that take on any value in a smooth interval.\n\n\n\n\n Examples include mass of an insect, leaf area consumed, dissolved oxygen content in water sample"
  },
  {
    "objectID": "content/slides/w2-slides.html#types-of-random-variables-1",
    "href": "content/slides/w2-slides.html#types-of-random-variables-1",
    "title": "Week 2",
    "section": "Types of random variables",
    "text": "Types of random variables\n\nAll random variables can be characterized by a probability distribution\n\n\n\n Area under a probability distribution always sums to 1\n\n\n\n\n Ecological data can be described by many common probability distributions (we’ll get into this in the next lecture)\n\n\n\n\n Pretty much every statistical test we do depends on an assumption of data fitting a specific probability distribution (most commonly, the Normal or Gaussian distribution)\n\n\n\n\n Data that don’t fit must be transformed or modeled using different approaches (e.g., generalized linear models)"
  },
  {
    "objectID": "content/slides/w2-slides.html#characterizing-the-expectation-of-rv-probability-distribution",
    "href": "content/slides/w2-slides.html#characterizing-the-expectation-of-rv-probability-distribution",
    "title": "Week 2",
    "section": "Characterizing the expectation of RV probability distribution",
    "text": "Characterizing the expectation of RV probability distribution\n\nRarely do we need to know the entire distribution, but rather we want to characterize the expectation (i.e., most typical value) and the variance\n\n\n Expectation of a random variable \\(Y\\) is the population mean of the probability distribution of \\(Y\\).\n\n\n Denoted as \\(E(Y)\\:or\\:\\mu_Y\\)"
  },
  {
    "objectID": "content/slides/w2-slides.html#characterizing-the-variance-of-rv-probability-distribution",
    "href": "content/slides/w2-slides.html#characterizing-the-variance-of-rv-probability-distribution",
    "title": "Week 2",
    "section": "Characterizing the variance of RV probability distribution",
    "text": "Characterizing the variance of RV probability distribution\n\nRarely do we need to know the entire distribution, but rather we want to characterize the expectation (i.e., most typical value) and the variance\n\n\n Variance of a random variable \\(Y\\) measures the population spread/variability of the probability distribution of \\(Y\\).\n\n\n Can be thought of as amount \\(Y\\) deviates from the expectation \\(mu_Y\\)\n\n\n Denoted as \\(Var(Y)\\:or\\:\\sigma^2_Y\\)\n\n\n Standard deviation is simply the square root of the variance:\n\n\\[\\sigma^2_Y = \\sqrt{Var(Y)}\\]"
  },
  {
    "objectID": "content/slides/w2-slides.html#a-quick-review-of-different-probability-distributions",
    "href": "content/slides/w2-slides.html#a-quick-review-of-different-probability-distributions",
    "title": "Week 2",
    "section": "A quick review of different probability distributions",
    "text": "A quick review of different probability distributions\n\n\n Discrete probability distributions\n\n\n\n Bernoulli, Binomial, and Poisson\n\n\n\n Continuous probability distributions\n\n\n\n Uniform, Normal, and Exponential"
  },
  {
    "objectID": "content/slides/w2-slides.html#bernoulli-random-variables",
    "href": "content/slides/w2-slides.html#bernoulli-random-variables",
    "title": "Week 2",
    "section": "Bernoulli random variables",
    "text": "Bernoulli random variables\n\nBernoulli random variables describe an outcome of an experiment or event that has only two outcomes (commonly referred to as Bernoulli trials)\n\n\n Common applications in ecology are for species presence/absence or for individuals that are alive or dead.\n\n\n Usually coded as 0/1 in a datasheet, with 1 the positive outcome (alive, present) and 0 the negative (dead, absent) 1\n\nBe sure you note this in your metadata!"
  },
  {
    "objectID": "content/slides/w2-slides.html#bernoulli-random-variables-1",
    "href": "content/slides/w2-slides.html#bernoulli-random-variables-1",
    "title": "Week 2",
    "section": "Bernoulli random variables",
    "text": "Bernoulli random variables\n\nWe say that a random variable \\(X\\) follows a Bernoulli distribution: \\(X \\sim Bernoulli(p)\\) or \\(P(X) = p\\)\n\n\n\n Most familiar example is a coin toss, where \\(P = 0.50\\)"
  },
  {
    "objectID": "content/slides/w2-slides.html#binomial-random-variables",
    "href": "content/slides/w2-slides.html#binomial-random-variables",
    "title": "Week 2",
    "section": "Binomial random variables",
    "text": "Binomial random variables\n\nA binomial random variable is simply a collection of multiple, independent Bernoulli trials"
  },
  {
    "objectID": "content/slides/w2-slides.html#binomial-random-variables-1",
    "href": "content/slides/w2-slides.html#binomial-random-variables-1",
    "title": "Week 2",
    "section": "Binomial random variables",
    "text": "Binomial random variables\n\nA binomial random variable is simply a collection of multiple, independent Bernoulli trials"
  },
  {
    "objectID": "content/slides/w2-slides.html#binomial-random-variables-example",
    "href": "content/slides/w2-slides.html#binomial-random-variables-example",
    "title": "Week 2",
    "section": "Binomial random variables: example",
    "text": "Binomial random variables: example\nResults of seed germination experiment\nTake three seeds. The seeds are independent (harvested from different plants of the same species). We soak them in water for 24 hours and plant them into a planting mix. After 7 days, we record successful germinations. Let \\(Y\\) be the number of successful germinations. Our sample space is then defined as:\n\n\n\n\n1st seed\n2nd seed\n3rd seed\nGerminations\n\n\n\n\nY\nY\nY\n3\n\n\nY\nY\nN\n2\n\n\nY\nN\nY\n2\n\n\nN\nY\nY\n2\n\n\nY\nN\nN\n1\n\n\nN\nY\nN\n1\n\n\nN\nN\nY\n1\n\n\nN\nN\nN\n0"
  },
  {
    "objectID": "content/slides/w2-slides.html#example-results-of-seed-germination-experiment",
    "href": "content/slides/w2-slides.html#example-results-of-seed-germination-experiment",
    "title": "Week 2",
    "section": "Example: Results of seed germination experiment",
    "text": "Example: Results of seed germination experiment\nTake three seeds. The seeds are independent (harvested from different plants of the same species). We soak them in water for 24 hours and plant them into a planting mix. After 7 days, we record successful germinations. Let \\(Y\\) be the number of successful germinations. Our sample space is then defined as:\n\n\n\n\n1st seed\n2nd seed\n3rd seed\nGerminations\n\n\n\n\nY\nY\nY\n3\n\n\nY\nY\nN\n2\n\n\nY\nN\nY\n2\n\n\nN\nY\nY\n2\n\n\nY\nN\nN\n1\n\n\nN\nY\nN\n1\n\n\nN\nN\nY\n1\n\n\nN\nN\nN\n0"
  },
  {
    "objectID": "content/slides/w2-slides.html#example-seed-germination-experiment",
    "href": "content/slides/w2-slides.html#example-seed-germination-experiment",
    "title": "Week 2",
    "section": "Example: Seed germination experiment",
    "text": "Example: Seed germination experiment\nTake three seeds. The seeds are independent (harvested from different plants of the same species). We soak them in water for 24 hours and plant them into a planting mix. After 7 days, we record successful germinations. Let \\(Y\\) be the number of successful germinations. Our sample space is then defined as:\n\n\n\n\n1st seed\n2nd seed\n3rd seed\nGerminations\n\n\n\n\nY\nY\nY\n3\n\n\nY\nY\nN\n2\n\n\nY\nN\nY\n2\n\n\nN\nY\nY\n2\n\n\nY\nN\nN\n1\n\n\nN\nY\nN\n1\n\n\nN\nN\nY\n1\n\n\nN\nN\nN\n0"
  },
  {
    "objectID": "content/slides/w2-slides.html#example-seed-germination-experiment-1",
    "href": "content/slides/w2-slides.html#example-seed-germination-experiment-1",
    "title": "Week 2",
    "section": "Example: Seed germination experiment",
    "text": "Example: Seed germination experiment\nSuppose there is a 50% chance of each seed germinating:\n\\[\nP(Y) = P(N) = 1/2\n\\]\nThen: \\[\nP(Y = 3) = P(YYY)\\\\\n         = P(Y)P(Y)P(Y)\\\\\n         = 1/2 * 1/2 * 1/2 = 1/8\\\\\nP(Y = 2) = P(YYN \\ or \\ YNY \\ or \\ NYY)\\\\\n         = P(YYN) + P(YNY) + P(NYY)\\\\\n         = P(Y)P(Y)P(N) + P(Y)P(N)P(Y) + P(N)P(Y)P(Y)\\\\\n         = 1/8 + 1/8 + 1/8 = 3/8\n\\]"
  },
  {
    "objectID": "content/slides/w2-slides.html#example-seed-germination-experiment-2",
    "href": "content/slides/w2-slides.html#example-seed-germination-experiment-2",
    "title": "Week 2",
    "section": "Example: Seed germination experiment",
    "text": "Example: Seed germination experiment\nWe can use a line graph to show the probability distribution of \\(Y\\).\n\n\\[Y \\sim Bernoulli(n, p)\\] \\[Y \\sim Bernoulli(3, 0.5)\\]\n\n\n\np(y) |\n     |\n3/8  |     T  T\n     |     |  |\n1/8  |  T  |  |  T\n     |--|--|--|--|--&gt; y\n        0  1  2  3"
  },
  {
    "objectID": "content/slides/w2-slides.html#poisson-random-variables",
    "href": "content/slides/w2-slides.html#poisson-random-variables",
    "title": "Week 2",
    "section": "Poisson random variables",
    "text": "Poisson random variables\n\nThe Poisson distribution is used to describe the number of occurrences of an event recorded in a sample of fixed area or during a fixed time interval. It’s used frequently in spatial and temporal statistics (and for count data!)\n\n\n Examples include number of plants in a quadrat, number of birds visiting a feeder over a period of time, etc.\n\n\n Modeled as \\(X \\sim Poisson(\\lambda)\\)\n\n\n \\(\\lambda\\), the “intensity”, is the average value of the number of occurrences of the event in each sample. Critical in spatial point processes!"
  },
  {
    "objectID": "content/slides/w2-slides.html#continuous-random-variables",
    "href": "content/slides/w2-slides.html#continuous-random-variables",
    "title": "Week 2",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nThe probability distribution of a continuous random variable is described by a probability density curve such that the area under the curve corresponds to the probability\n\n\n\n With continuous random variables, we can no longer find probability of a discrete outcome (i.e., specific value), instead we use intervals\n\n\n\n\n Examples of continuous random variables include body mass, wing length, tree height, concentration, etc.\n\n\n\n\n We’ll discuss uniform, normal, and standard normal distributions (but others exist including exponential, log-normal, Chi-Square, F, gamma, beta)"
  },
  {
    "objectID": "content/slides/w2-slides.html#the-uniform-random-variable",
    "href": "content/slides/w2-slides.html#the-uniform-random-variable",
    "title": "Week 2",
    "section": "The uniform random variable",
    "text": "The uniform random variable\n\nA uniform random variable is one that has an equal probability at each and every subinterval along the interval\n\n\nThe probability distribution of such a variable is a rectangle with an area under the curve equal to 1 (The First Axiom of Probability)"
  },
  {
    "objectID": "content/slides/w2-slides.html#the-normal-random-variable",
    "href": "content/slides/w2-slides.html#the-normal-random-variable",
    "title": "Week 2",
    "section": "The normal random variable",
    "text": "The normal random variable\n\nA normal random variable is the most popular continuous probability distribution due to its fit of so many empirical datasets (i.e., continuous data infuenced by small and unrelated random effects that are normally distributed)\n\n\n\n Also referred to as a “Gaussian” probability distribution, normal curve, or bell curve."
  },
  {
    "objectID": "content/slides/w2-slides.html#properties-of-the-normal-random-probability-distribution",
    "href": "content/slides/w2-slides.html#properties-of-the-normal-random-probability-distribution",
    "title": "Week 2",
    "section": "Properties of the normal random probability distribution",
    "text": "Properties of the normal random probability distribution\n\nA random variable \\(Y\\) is said to have a normal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\) and is written as:\n\n\\[\nY \\sim N(\\mu, \\sigma^2)\n\\]\n\n\n Total area under the distribution curve is 1 (First Axiom)\n\n\n\n\n Distribution is symmtric about the expectation, \\(\\mu\\), such that:\n\n\n\n\\[\nE(Y) = \\mu :\\:\\:\\ Var(Y) = \\sigma^2\n\\]"
  },
  {
    "objectID": "content/slides/w2-slides.html#properties-of-the-normal-random-probability-distribution-1",
    "href": "content/slides/w2-slides.html#properties-of-the-normal-random-probability-distribution-1",
    "title": "Week 2",
    "section": "Properties of the normal random probability distribution",
    "text": "Properties of the normal random probability distribution\n\nWe can transform normal distributions with scaling and shifting operations. Consider two random variables \\(X\\) and \\(Y\\):\n\n\n\\[\nX \\sim N(\\mu, \\sigma^2) :\\:\\:\\ Y = aX + b\n\\]"
  },
  {
    "objectID": "content/slides/w2-slides.html#transforming-to-a-standard-normal-probability-distribution",
    "href": "content/slides/w2-slides.html#transforming-to-a-standard-normal-probability-distribution",
    "title": "Week 2",
    "section": "Transforming to a standard normal probability distribution",
    "text": "Transforming to a standard normal probability distribution\n\nSpecial case of applying a shift and scale operation in which \\(a = 1/\\sigma\\) and \\(b = -1(\\mu/\\sigma)\\)\n\n\\[\nFor:\\ X \\sim N(\\mu, \\sigma), Y = aX + b\n\\]\n\n\\[\n= (1/\\sigma)X - \\mu/\\sigma\n\\]\n\n\n\\[\n= \\frac{X}{1} * \\frac{1}{\\sigma} - \\frac{\\mu}{\\sigma}\n\\]\n\n\n\\[\n= \\frac{X}{\\sigma} - \\frac{\\mu}{\\sigma}\n\\]\n\n\n\\[\n= \\frac{X - \\mu}{\\sigma}\n\\]"
  },
  {
    "objectID": "content/slides/w2-slides.html#standard-normal-probability-distribution",
    "href": "content/slides/w2-slides.html#standard-normal-probability-distribution",
    "title": "Week 2",
    "section": "Standard normal probability distribution",
    "text": "Standard normal probability distribution\n\n\\(\\frac{X - \\mu}{\\sigma}\\) –&gt; This random variable is referred to as Z, or a Z-score!\n\n\n\n We write that \\(Z \\sim N(\\mu, \\sigma^2\\) with expectation \\(\\mu=0\\) and variance \\(\\sigma^2=1\\), or:)\n\n\\[\nZ \\sim N(0, 1)\n\\]\n\n\n\n The distribution is symmetric around the center \\(\\mu=0\\)\n\n\n\n\n The area between [-1, 1] (i.e., \\(\\mu ± \\sigma\\)) is 0.6826\n\n\n\n\n The area between [-2, 2] is 0.9545"
  },
  {
    "objectID": "content/slides/w2-slides.html#standard-normal-probability-distribution-1",
    "href": "content/slides/w2-slides.html#standard-normal-probability-distribution-1",
    "title": "Week 2",
    "section": "Standard normal probability distribution",
    "text": "Standard normal probability distribution\n\nSuppose \\(Z\\) is a standard normal random variable (i.e., \\(Z \\sim N(0, 1)\\)):\n\n\n\n \\(P(Z ≤ 0) = 0.5\\)\n\n\n\n\n \\(P(-\\infty ≤ Z ≤ +\\infty) = 0.5\\)\n\n\n\n\n \\(P(Z = 1) = 0\\) and in general, \\(P(Z = c) = 0\\) for any \\(c\\)\n\n  How do we find \\(P(Z ≥ / ≤ z)\\) for any given value of \\(z\\)?\n\n\n\n Use table from a book or statistical software! (hint, draw pictures!)"
  },
  {
    "objectID": "content/slides/w2-slides.html#find-the-probability-under-the-curve",
    "href": "content/slides/w2-slides.html#find-the-probability-under-the-curve",
    "title": "Week 2",
    "section": "Find the probability under the curve",
    "text": "Find the probability under the curve\n\\[\\begin{eqnarray*}\nP(Z\\geq 1.5) &=& 0.0668 \\\\\nP(Z\\leq 1.5) &=& 1-P(Z\\geq 1.5) \\\\ &=& 1-0.0668 = 0.9332 \\\\\nP(0\\leq Z\\leq 1.5) &=& P(Z\\geq 0)-P(Z\\geq 1.5) \\\\ &=& 0.5-0.0668 = 0.4332\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "content/slides/w2-slides.html#find-the-probability-under-the-curve-1",
    "href": "content/slides/w2-slides.html#find-the-probability-under-the-curve-1",
    "title": "Week 2",
    "section": "Find the probability under the curve",
    "text": "Find the probability under the curve\n\\[\\begin{eqnarray*}\nP(Z\\geq -0.6) &=& 1-P(Z\\leq -0.6)\\\\ &=& 1-P(Z\\geq 0.6) \\\\ &=& 1-0.2743 = 0.7257\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "content/slides/w2-slides.html#find-the-probability-under-the-curve-2",
    "href": "content/slides/w2-slides.html#find-the-probability-under-the-curve-2",
    "title": "Week 2",
    "section": "Find the probability under the curve",
    "text": "Find the probability under the curve\n\\[\\begin{eqnarray*}\nP(-1.7 \\leq Z\\leq 1.7) &=& 2\\times P(0\\leq Z\\leq 1.7) \\\\ &=& 2\\times(0.5-0.0446) = 0.9108\n\\end{eqnarray*}\\]\n\nOR\n\n\n\\[\\begin{eqnarray*}\nP(-1.7 \\leq Z\\leq 1.7) &=& 1-P(Z\\leq -1.7 {\\rm ~or~} Z\\geq 1.7) \\\\ &=& 1-2\\times P(Z\\geq 1.7) \\\\ &=& 1-2\\times 0.0446 = 1-0.0892 = 0.9108.\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "content/slides/w2-slides.html#transforming-a-general-normal-variable-to-a-standard-normal-variable",
    "href": "content/slides/w2-slides.html#transforming-a-general-normal-variable-to-a-standard-normal-variable",
    "title": "Week 2",
    "section": "Transforming a general normal variable to a standard normal variable",
    "text": "Transforming a general normal variable to a standard normal variable\nA random variable \\(Y\\) may be normally distributed with parameters \\(\\mu\\) and \\(\\sigma^2\\) and written as:\n\\[\nY\\sim N(\\mu,\\sigma^2)\n\\]\n\nFor example, \\(Y\\sim N(20,3^2)\\) stands for normal distribution with \\(\\mu=20,\\sigma^2=9\\).\n\n\n Suppose \\(Y\\sim N(\\mu,\\sigma^2)\\), then a very useful transformation is:\n\\[\nZ=\\frac{Y-\\mu}{\\sigma}\\sim N(0,1)\n\\]"
  },
  {
    "objectID": "content/slides/w2-slides.html#transforming-a-general-normal-variable-to-a-standard-normal-variable-1",
    "href": "content/slides/w2-slides.html#transforming-a-general-normal-variable-to-a-standard-normal-variable-1",
    "title": "Week 2",
    "section": "Transforming a general normal variable to a standard normal variable",
    "text": "Transforming a general normal variable to a standard normal variable\nFor \\(Y\\sim N(20,3^2)\\) (i.e. \\(\\mu=20,\\sigma^2=9\\)), we have \\[\\begin{eqnarray*}\nP(20\\leq Y\\leq 23) &=& P(20-\\mu\\leq Y-\\mu\\leq 23-\\mu) \\\\\n&=& P\\left(\\frac{20-\\mu}{\\sigma}\\leq \\frac{Y-\\mu}{\\sigma}\\leq \\frac{23-\\mu}{\\sigma}\\right) \\\\\n&=& P\\left(\\frac{20-20}{3}\\leq Z\\leq \\frac{23-20}{3}\\right) \\\\\n&=& P(0\\leq Z\\leq 1) \\\\\n&=& P(Z\\geq 0)-P(Z\\geq 1) \\\\\n&=& 0.5-0.1587 = 0.3413\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "content/slides/w2-slides.html#transforming-a-general-normal-variable-to-a-standard-normal-variable-2",
    "href": "content/slides/w2-slides.html#transforming-a-general-normal-variable-to-a-standard-normal-variable-2",
    "title": "Week 2",
    "section": "Transforming a general normal variable to a standard normal variable",
    "text": "Transforming a general normal variable to a standard normal variable\nSuppose \\(Y\\sim N(20,3^2)\\), what is \\(P(Y\\leq 15.5)\\)?\n\\[\\begin{eqnarray*}\nP(Y\\leq 15.5) &=& P\\left(\\frac{Y-\\mu}{\\sigma}\\leq \\frac{15.5-\\mu}{\\sigma}\\right) \\\\\n&=& P\\left(Z\\leq \\frac{15.5-20}{3}\\right) \\\\\n&=& P(Z\\leq -1.5) \\\\\n&=& 0.0668\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "content/slides/w2-slides.html#z-scores-and-interpretation",
    "href": "content/slides/w2-slides.html#z-scores-and-interpretation",
    "title": "Week 2",
    "section": "Z-scores and interpretation",
    "text": "Z-scores and interpretation\n\nGiven a value \\(y\\), the term: \\[\nz=\\frac{y-\\mu}{\\sigma}\n\\] is called the z-score corresponding to y. It represents the number of standard deviations \\(y\\) is from \\(\\mu\\).\n\n\nFor \\(Y\\sim N(20,3^2)\\),\n\n\n\n 23 is \\(1\\sigma\\) away from \\(\\mu=20\\).\n\n\n\n\n 20 is \\(0\\sigma\\) away from \\(\\mu=20\\) (i.e. at \\(\\mu=20\\)).\n\n\n\n\n 24 is \\(\\frac{4}{3}\\sigma\\) away from \\(\\mu=20\\).\n\n\n\n\n 17 is \\(-1\\sigma\\) away from \\(\\mu=20\\)."
  },
  {
    "objectID": "content/w2-content.html#lecture-2.2",
    "href": "content/w2-content.html#lecture-2.2",
    "title": "Week 2",
    "section": "Lecture 2.2",
    "text": "Lecture 2.2\n    View slides in full screen\n       \n      \n    \n  \n\n\n\n\n\n\nTip\n\n\n\nTip: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.",
    "crumbs": [
      "Content",
      "Module 2: Reviewing foundations in statistics",
      "Week 2: Sampling, probability, random variables"
    ]
  },
  {
    "objectID": "content/slides/w2_1-slides.html",
    "href": "content/slides/w2_1-slides.html",
    "title": "Week 2",
    "section": "",
    "text": ". . .\n\n What is statistics?\n\n. . .\n\n Sampling\n\n. . .\n\n Probability\n\n. . .\n\n Random variables"
  },
  {
    "objectID": "content/slides/w2_1-slides.html#a-brief-survey-of-basic-statistical-concepts",
    "href": "content/slides/w2_1-slides.html#a-brief-survey-of-basic-statistical-concepts",
    "title": "Week 2",
    "section": "A brief survey of basic statistical concepts",
    "text": "A brief survey of basic statistical concepts\n\n\n What is statistics?\n\n\n\n\n Sampling\n\n\n\n\n Probability\n\n\n\n\n Random variables"
  },
  {
    "objectID": "content/slides/w2_1-slides.html#definition",
    "href": "content/slides/w2_1-slides.html#definition",
    "title": "Week 2",
    "section": "Definition",
    "text": "Definition\n\n\nStatistics is the art / science of collecting, analyzing, interpreting, and presenting data"
  },
  {
    "objectID": "content/slides/w2_1-slides.html#why-do-we-need-statistics",
    "href": "content/slides/w2_1-slides.html#why-do-we-need-statistics",
    "title": "Week 2",
    "section": "Why do we need statistics?",
    "text": "Why do we need statistics?\n\n\nStatistics are an integral component of the scientific process that allow us separate signal from noise in complex biological and ecological systems so that we can understand how systems work and make informed decisions instead of just…\n…going with your gut and “vibes”."
  },
  {
    "objectID": "content/slides/w2_1-slides.html#so-is-it-an-art-or-a-science",
    "href": "content/slides/w2_1-slides.html#so-is-it-an-art-or-a-science",
    "title": "Week 2",
    "section": "So is it an art or a science?",
    "text": "So is it an art or a science?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nThe art/science of designing and conducting a good experiment or observational study makes the art/science of doing statistics far easier."
  },
  {
    "objectID": "content/slides/w2_1-slides.html#population-vs.-sample",
    "href": "content/slides/w2_1-slides.html#population-vs.-sample",
    "title": "Week 2",
    "section": "Population vs. sample",
    "text": "Population vs. sample\n\n\nA population is a whole set/group of individuals/objects which we are interested in studying. I.e. the complete set of all possible observations (finite or infinite)\n\n All plants of species X, all lakes in Minnesota\n\n\n\nA sample is a representative subset of the population that we actually observe in order to learn about the population\n\n 100 plants of species X at 25 sites, 50 random lakes in Minnesota"
  },
  {
    "objectID": "content/slides/w2_1-slides.html#population-vs.-sample-1",
    "href": "content/slides/w2_1-slides.html#population-vs.-sample-1",
    "title": "Week 2",
    "section": "Population vs. sample",
    "text": "Population vs. sample\nWe use different symbols/notation for the same parameters depending on whether we’re referring to the population or to the sample\n Mean:\\(\\mu\\) (population) vs \\(\\bar{x}\\) (sample)\n Variance:\\(\\sigma^2\\) (population) vs \\(s^2\\) (sample)"
  },
  {
    "objectID": "content/slides/w2_1-slides.html#the-three-primary-realms-of-statistics",
    "href": "content/slides/w2_1-slides.html#the-three-primary-realms-of-statistics",
    "title": "Week 2",
    "section": "The three primary realms of statistics",
    "text": "The three primary realms of statistics\n\n\nDescriptive statistics\n\n\nProbability\n\n\nInferential statistics\n\n\n\nTogether, these elements are used to make inferences about a collection of data and address specific hypotheses"
  },
  {
    "objectID": "content/slides/w2_1-slides.html#realms-of-statistics",
    "href": "content/slides/w2_1-slides.html#realms-of-statistics",
    "title": "Week 2",
    "section": "Realms of statistics",
    "text": "Realms of statistics\nDescriptive statistics\n\nDisplaying and summarizing the main features of data in a sample\n\n e.g., mean, median, mode, max, min"
  },
  {
    "objectID": "content/slides/w2_1-slides.html#realms-of-statistics-1",
    "href": "content/slides/w2_1-slides.html#realms-of-statistics-1",
    "title": "Week 2",
    "section": "Realms of statistics",
    "text": "Realms of statistics\nProbability: the foundation of statistics\nGiven a population, understand the uncertainty associated with taking a sample taken from the population\nProbability is the mathematics of chance and randomness. Properties of a population are assumed as known and questions about a sample are posed and answered. The approach is deductive (general  specific)"
  },
  {
    "objectID": "content/slides/w2_1-slides.html#realms-of-statistics-2",
    "href": "content/slides/w2_1-slides.html#realms-of-statistics-2",
    "title": "Week 2",
    "section": "Realms of statistics",
    "text": "Realms of statistics\nInferential statistics\nGiven a sample, learn methods to draw conclusions about a population while taking into account uncertainties in the sample.\nIn inferential statistics, properties of a sample are available and conclusions about the population are drawn based on the sample. The approach is inductive (specific  general)"
  },
  {
    "objectID": "content/slides/w2_1-slides.html#discussion-questions",
    "href": "content/slides/w2_1-slides.html#discussion-questions",
    "title": "Week 2",
    "section": "Discussion questions ",
    "text": "Discussion questions \nPair up and have a quick chat (2-3 minutes) about the following questions. We’ll then discuss as a larger group.\n\n\n Are statistics always necessary for population-level inferences?\n\n\n\n\n Can you think of situations where sampling is needed for inference?\n\n\n\n\n How can we be sure our sampling schemes are representative of the population?"
  },
  {
    "objectID": "content/slides/w2_1-slides.html#bias",
    "href": "content/slides/w2_1-slides.html#bias",
    "title": "Week 2",
    "section": "Bias ",
    "text": "Bias \n\nBias is an extremely important concept in science and statistics: it is a type of systematic error (as opposed to random error) that can impact:\n\n\n Study design (e.g., improper design of experiment)\n Data collection (e.g., sampling from only one side of a field)\n Statistical analyses (e.g., improperly accounting for confounding variables)\n\n\nCollectively, bias impacts our interpretation of our research and threatens the scientific enterprise writ large. It’s super important that we attempt to control or account for as many potential sources of bias as possible.\nIn this course, one of our objectives is to account for the impact of space and time as a source of bias in our data."
  },
  {
    "objectID": "content/slides/w2_1-slides.html#probability-is-the-foundation-of-statistics",
    "href": "content/slides/w2_1-slides.html#probability-is-the-foundation-of-statistics",
    "title": "Week 2",
    "section": "Probability is the foundation of statistics",
    "text": "Probability is the foundation of statistics\n\nThe concept of probability is central to all statistical methods & sub-disciplines. Here, we define it as the likely outcome of an event of which we are unsure (e.g., a coin toss)\n\n\n\\[ Probability = \\frac{n\\:outcomes}{n\\:trials} \\]\n\n\n\\[ 0 ≤ P ≤ 1 \\]\n\n\nUncertainty arises because of natural (i.e., random) variation in the world we live in"
  },
  {
    "objectID": "content/slides/w2_1-slides.html#some-key-terms-that-we-need-to-know",
    "href": "content/slides/w2_1-slides.html#some-key-terms-that-we-need-to-know",
    "title": "Week 2",
    "section": "Some key terms that we need to know",
    "text": "Some key terms that we need to know\n\nReminder, probability is defined as the likely outcome of an event of which we are unsure. To measure probability, we need to consider…\n\n\n\n Events: a simple process with a well-defined beginning and end\n\n\n\n\n Outcomes: the result of an event. Defining ecological outcomes can be tricky\n\n\n\n\n Discrete outcomes: outcome assigned a positive integer (e.g., live = 1, dead = 0)\n\n\n\n\n Sample space: The set formed from all of the possible outcomes\n\n\n\n\n Trials: a single event, from start to finish, ending in an outcome\n\n\n\n\n Replicates: a single trial\n\n\n\n\n Experiments: a collection of trials"
  },
  {
    "objectID": "content/slides/w2_1-slides.html#neither-random-nor-variable",
    "href": "content/slides/w2_1-slides.html#neither-random-nor-variable",
    "title": "Week 2",
    "section": "Neither random, nor variable…🙄",
    "text": "Neither random, nor variable…🙄\n\nA variable is the actual property measured by individual observations in a trial (e.g., length, pH, fitness)\n\n\n\nA random variable is a variable whose values are not known before a sample is taken — a variable that depends on the outcome of a chance situation.\n\n\n\n\n Mathematically, it’s a rule or a function that assigns a numerical value to each possible outcome of an experiment in the sample space."
  },
  {
    "objectID": "content/slides/w2_1-slides.html#types-of-random-variables",
    "href": "content/slides/w2_1-slides.html#types-of-random-variables",
    "title": "Week 2",
    "section": "Types of random variables",
    "text": "Types of random variables\nThere are two primary types of random variables\n\nA discrete random variable are characterized by a finite number of possible values (e.g., \\(y_1, .... y_n\\)).\n\n\n\n Examples include presence/absence of a species, number of offspring, number of seeds germinating\n\n\n\n\nA continuous random variable are those that take on any value in a smooth interval.\n\n\n\n\n Examples include mass of an insect, leaf area consumed, dissolved oxygen content in water sample"
  },
  {
    "objectID": "content/slides/w2_1-slides.html#types-of-random-variables-1",
    "href": "content/slides/w2_1-slides.html#types-of-random-variables-1",
    "title": "Week 2",
    "section": "Types of random variables",
    "text": "Types of random variables\n\nAll random variables can be characterized by a probability distribution\n\n\n\n Area under a probability distribution always sums to 1\n\n\n\n\n Ecological data can be described by many common probability distributions (we’ll get into this in the next lecture)\n\n\n\n\n Pretty much every statistical test we do depends on an assumption of data fitting a specific probability distribution (most commonly, the Normal or Gaussian distribution)\n\n\n\n\n Data that don’t fit must be transformed or modeled using different approaches (e.g., generalized linear models)"
  },
  {
    "objectID": "content/slides/w2_1-slides.html#characterizing-the-expectation-of-rv-probability-distribution",
    "href": "content/slides/w2_1-slides.html#characterizing-the-expectation-of-rv-probability-distribution",
    "title": "Week 2",
    "section": "Characterizing the expectation of RV probability distribution",
    "text": "Characterizing the expectation of RV probability distribution\n\nRarely do we need to know the entire distribution, but rather we want to characterize the expectation (i.e., most typical value) and the variance\n\n\n Expectation of a random variable \\(Y\\) is the population mean of the probability distribution of \\(Y\\).\n\n\n Denoted as \\(E(Y)\\:or\\:\\mu_Y\\)"
  },
  {
    "objectID": "content/slides/w2_1-slides.html#characterizing-the-variance-of-rv-probability-distribution",
    "href": "content/slides/w2_1-slides.html#characterizing-the-variance-of-rv-probability-distribution",
    "title": "Week 2",
    "section": "Characterizing the variance of RV probability distribution",
    "text": "Characterizing the variance of RV probability distribution\n\nRarely do we need to know the entire distribution, but rather we want to characterize the expectation (i.e., most typical value) and the variance\n\n\n\n Variance of a random variable \\(Y\\) measures the population spread/variability of the probability distribution of \\(Y\\).\n\n\n\n\n Can be thought of as amount \\(Y\\) deviates from the expectation \\(mu_Y\\)\n\n\n\n\n Denoted as \\(Var(Y)\\:or\\:\\sigma^2_Y\\)\n\n\n\n\n Standard deviation is simply the square root of the variance:\n\n\\[\\sigma^2_Y = \\sqrt{Var(Y)}\\]"
  },
  {
    "objectID": "content/slides/w2_1-slides.html#independence-of-random-variables",
    "href": "content/slides/w2_1-slides.html#independence-of-random-variables",
    "title": "Week 2",
    "section": "Independence of random variables",
    "text": "Independence of random variables\n\nTwo random variables \\(X\\) and \\(Y\\) are independent if knowledge of the value of one has no effect on the probability of the other.\n\n\n\n For example, the size and weight of a moose are probably not independent!"
  },
  {
    "objectID": "content/w3-content.html",
    "href": "content/w3-content.html",
    "title": "Week 3",
    "section": "",
    "text": "Tip\n\n\n\nTip: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.",
    "crumbs": [
      "Content",
      "Module 2: Reviewing foundations in statistics",
      "Week 3: Data transformations, hypotheses, t-tests"
    ]
  },
  {
    "objectID": "content/w3-content.html#slides",
    "href": "content/w3-content.html#slides",
    "title": "Week 3",
    "section": "",
    "text": "Tip\n\n\n\nTip: If you type ? (or shift + /) while going through the slides, you can see a list of special slide-specific commands.",
    "crumbs": [
      "Content",
      "Module 2: Reviewing foundations in statistics",
      "Week 3: Data transformations, hypotheses, t-tests"
    ]
  },
  {
    "objectID": "content/slides/w2_2-slides.html#a-quick-review-of-different-probability-distributions",
    "href": "content/slides/w2_2-slides.html#a-quick-review-of-different-probability-distributions",
    "title": "Week 2",
    "section": "A quick review of different probability distributions",
    "text": "A quick review of different probability distributions\n\n\n Discrete probability distributions\n\n\n\n Bernoulli, Binomial, and Poisson\n\n\n\n Continuous probability distributions\n\n\n\n Uniform, Normal, and Exponential"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#bernoulli-random-variables",
    "href": "content/slides/w2_2-slides.html#bernoulli-random-variables",
    "title": "Week 2",
    "section": "Bernoulli random variables",
    "text": "Bernoulli random variables\n\nBernoulli random variables describe an outcome of an experiment or event that has only two outcomes (commonly referred to as Bernoulli trials)\n\n\n Common applications in ecology are for species presence/absence or for individuals that are alive or dead.\n\n\n Usually coded as 0/1 in a datasheet, with 1 the positive outcome (alive, present) and 0 the negative (dead, absent) 1\n\nBe sure you note this in your metadata!"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#bernoulli-random-variables-1",
    "href": "content/slides/w2_2-slides.html#bernoulli-random-variables-1",
    "title": "Week 2",
    "section": "Bernoulli random variables",
    "text": "Bernoulli random variables\n\nWe say that a random variable \\(X\\) follows a Bernoulli distribution: \\(X \\sim Bernoulli(p)\\) or \\(P(X) = p\\)\n\n\n\n Most familiar example is a coin toss, where \\(P = 0.50\\)"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#binomial-random-variables",
    "href": "content/slides/w2_2-slides.html#binomial-random-variables",
    "title": "Week 2",
    "section": "Binomial random variables",
    "text": "Binomial random variables\n\nA binomial random variable is simply a collection of multiple, independent Bernoulli trials"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#binomial-random-variables-1",
    "href": "content/slides/w2_2-slides.html#binomial-random-variables-1",
    "title": "Week 2",
    "section": "Binomial random variables",
    "text": "Binomial random variables\n\nA binomial random variable is simply a collection of multiple, independent Bernoulli trials"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#example-seed-germination-experiment",
    "href": "content/slides/w2_2-slides.html#example-seed-germination-experiment",
    "title": "Week 2",
    "section": "Example: Seed germination experiment",
    "text": "Example: Seed germination experiment\nTake three seeds. The seeds are independent (harvested from different plants of the same species). We soak them in water for 24 hours and plant them into a planting mix. After 7 days, we record successful germinations. Let \\(Y\\) be the number of successful germinations. Our sample space is then defined as:\n\n\n\n\n1st seed\n2nd seed\n3rd seed\nGerminations\n\n\n\n\nY\nY\nY\n3\n\n\nY\nY\nN\n2\n\n\nY\nN\nY\n2\n\n\nN\nY\nY\n2\n\n\nY\nN\nN\n1\n\n\nN\nY\nN\n1\n\n\nN\nN\nY\n1\n\n\nN\nN\nN\n0"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#example-seed-germination-experiment-1",
    "href": "content/slides/w2_2-slides.html#example-seed-germination-experiment-1",
    "title": "Week 2",
    "section": "Example: Seed germination experiment",
    "text": "Example: Seed germination experiment\nSuppose there is a 50% chance of each seed germinating:\n\\[\nP(Y) = P(N) = 1/2\n\\]\nThen: \\[\nP(Y = 3) = P(YYY)\\\\\n         = P(Y)P(Y)P(Y)\\\\\n         = 1/2 * 1/2 * 1/2 = 1/8\\\\\nP(Y = 2) = P(YYN \\ or \\ YNY \\ or \\ NYY)\\\\\n         = P(YYN) + P(YNY) + P(NYY)\\\\\n         = P(Y)P(Y)P(N) + P(Y)P(N)P(Y) + P(N)P(Y)P(Y)\\\\\n         = 1/8 + 1/8 + 1/8 = 3/8\n\\]"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#example-seed-germination-experiment-2",
    "href": "content/slides/w2_2-slides.html#example-seed-germination-experiment-2",
    "title": "Week 2",
    "section": "Example: Seed germination experiment",
    "text": "Example: Seed germination experiment\nWe can use a line graph to show the probability distribution of \\(Y\\).\n\n\\[Y \\sim Bernoulli(n, p)\\] \\[Y \\sim Bernoulli(3, 0.5)\\]\n\n\n\np(y) |\n     |\n3/8  |     T  T\n     |     |  |\n1/8  |  T  |  |  T\n     |--|--|--|--|--&gt; y\n        0  1  2  3"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#poisson-random-variables",
    "href": "content/slides/w2_2-slides.html#poisson-random-variables",
    "title": "Week 2",
    "section": "Poisson random variables",
    "text": "Poisson random variables\n\nThe Poisson distribution is used to describe the number of occurrences of an event recorded in a sample of fixed area or during a fixed time interval. It’s used frequently in spatial and temporal statistics (and for count data!)\n\n\n Examples include number of plants in a quadrat, number of birds visiting a feeder over a period of time, etc.\n\n\n Modeled as \\(X \\sim Poisson(\\lambda)\\)\n\n\n \\(\\lambda\\), the “intensity”, is the average value of the number of occurrences of the event in each sample. Critical in spatial point processes!"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#continuous-random-variables",
    "href": "content/slides/w2_2-slides.html#continuous-random-variables",
    "title": "Week 2",
    "section": "Continuous random variables",
    "text": "Continuous random variables\n\nThe probability distribution of a continuous random variable is described by a probability density curve such that the area under the curve corresponds to the probability\n\n\n\n With continuous random variables, we can no longer find probability of a discrete outcome (i.e., specific value), instead we use intervals\n\n\n\n\n Examples of continuous random variables include body mass, wing length, tree height, concentration, etc.\n\n\n\n\n We’ll discuss uniform, normal, and standard normal distributions (but others exist including exponential, log-normal, Chi-Square, F, gamma, beta)"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#the-uniform-random-variable",
    "href": "content/slides/w2_2-slides.html#the-uniform-random-variable",
    "title": "Week 2",
    "section": "The uniform random variable",
    "text": "The uniform random variable\n\nA uniform random variable is one that has an equal probability at each and every subinterval along the interval\n\n\nThe probability distribution of such a variable is a rectangle with an area under the curve equal to 1 (The First Axiom of Probability)"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#the-normal-random-variable",
    "href": "content/slides/w2_2-slides.html#the-normal-random-variable",
    "title": "Week 2",
    "section": "The normal random variable",
    "text": "The normal random variable\n\nA normal random variable is the most popular continuous probability distribution due to its fit of so many empirical datasets (i.e., continuous data infuenced by small and unrelated random effects that are normally distributed)\n\n\n\n Also referred to as a “Gaussian” probability distribution, normal curve, or bell curve."
  },
  {
    "objectID": "content/slides/w2_2-slides.html#properties-of-the-normal-random-probability-distribution",
    "href": "content/slides/w2_2-slides.html#properties-of-the-normal-random-probability-distribution",
    "title": "Week 2",
    "section": "Properties of the normal random probability distribution",
    "text": "Properties of the normal random probability distribution\n\nA random variable \\(Y\\) is said to have a normal distribution with parameters \\(\\mu\\) and \\(\\sigma^2\\) and is written as:\n\n\\[\nY \\sim N(\\mu, \\sigma^2)\n\\]\n\n\n Total area under the distribution curve is 1 (First Axiom)\n\n\n\n\n Distribution is symmtric about the expectation, \\(\\mu\\), such that:\n\n\n\n\\[\nE(Y) = \\mu :\\:\\:\\ Var(Y) = \\sigma^2\n\\]"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#properties-of-the-normal-random-probability-distribution-1",
    "href": "content/slides/w2_2-slides.html#properties-of-the-normal-random-probability-distribution-1",
    "title": "Week 2",
    "section": "Properties of the normal random probability distribution",
    "text": "Properties of the normal random probability distribution\n\nWe can transform normal distributions with scaling and shifting operations. Consider two random variables \\(X\\) and \\(Y\\):\n\n\n\\[\nX \\sim N(\\mu, \\sigma^2) :\\:\\:\\ Y = aX + b\n\\]"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#transforming-to-a-standard-normal-probability-distribution",
    "href": "content/slides/w2_2-slides.html#transforming-to-a-standard-normal-probability-distribution",
    "title": "Week 2",
    "section": "Transforming to a standard normal probability distribution",
    "text": "Transforming to a standard normal probability distribution\n\nSpecial case of applying a shift and scale operation in which \\(a = 1/\\sigma\\) and \\(b = -1(\\mu/\\sigma)\\)\n\n\\[\nFor:\\ X \\sim N(\\mu, \\sigma), Y = aX + b\n\\]\n\n\\[\n= (1/\\sigma)X - \\mu/\\sigma\n\\]\n\n\n\\[\n= \\frac{X}{1} * \\frac{1}{\\sigma} - \\frac{\\mu}{\\sigma}\n\\]\n\n\n\\[\n= \\frac{X}{\\sigma} - \\frac{\\mu}{\\sigma}\n\\]\n\n\n\\[\n= \\frac{X - \\mu}{\\sigma}\n\\]"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#standard-normal-probability-distribution",
    "href": "content/slides/w2_2-slides.html#standard-normal-probability-distribution",
    "title": "Week 2",
    "section": "Standard normal probability distribution",
    "text": "Standard normal probability distribution\n\n\\(\\frac{X - \\mu}{\\sigma}\\) –&gt; This random variable is referred to as Z, or a Z-score!\n\n\n\n We write that \\(Z \\sim N(\\mu, \\sigma^2\\) with expectation \\(\\mu=0\\) and variance \\(\\sigma^2=1\\), or:)\n\n\\[\nZ \\sim N(0, 1)\n\\]\n\n\n\n The distribution is symmetric around the center \\(\\mu=0\\)\n\n\n\n\n The area between [-1, 1] (i.e., \\(\\mu ± \\sigma\\)) is 0.6826\n\n\n\n\n The area between [-2, 2] is 0.9545"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#standard-normal-probability-distribution-1",
    "href": "content/slides/w2_2-slides.html#standard-normal-probability-distribution-1",
    "title": "Week 2",
    "section": "Standard normal probability distribution",
    "text": "Standard normal probability distribution\n\nSuppose \\(Z\\) is a standard normal random variable (i.e., \\(Z \\sim N(0, 1)\\)):\n\n\n\n \\(P(Z ≤ 0) = 0.5\\)\n\n\n\n\n \\(P(-\\infty ≤ Z ≤ +\\infty) = 0.5\\)\n\n\n\n\n \\(P(Z = 1) = 0\\) and in general, \\(P(Z = c) = 0\\) for any \\(c\\)\n\n  How do we find \\(P(Z ≥ / ≤ z)\\) for any given value of \\(z\\)?\n\n\n\n Use table from a book or statistical software! (hint, draw pictures!)"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#find-the-probability-under-the-curve",
    "href": "content/slides/w2_2-slides.html#find-the-probability-under-the-curve",
    "title": "Week 2",
    "section": "Find the probability under the curve",
    "text": "Find the probability under the curve\n\\[\\begin{eqnarray*}\nP(Z\\geq 1.5) &=& 0.0668 \\\\\nP(Z\\leq 1.5) &=& 1-P(Z\\geq 1.5) \\\\ &=& 1-0.0668 = 0.9332 \\\\\nP(0\\leq Z\\leq 1.5) &=& P(Z\\geq 0)-P(Z\\geq 1.5) \\\\ &=& 0.5-0.0668 = 0.4332\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#find-the-probability-under-the-curve-1",
    "href": "content/slides/w2_2-slides.html#find-the-probability-under-the-curve-1",
    "title": "Week 2",
    "section": "Find the probability under the curve",
    "text": "Find the probability under the curve\n\\[\\begin{eqnarray*}\nP(Z\\geq -0.6) &=& 1-P(Z\\leq -0.6)\\\\ &=& 1-P(Z\\geq 0.6) \\\\ &=& 1-0.2743 = 0.7257\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#find-the-probability-under-the-curve-2",
    "href": "content/slides/w2_2-slides.html#find-the-probability-under-the-curve-2",
    "title": "Week 2",
    "section": "Find the probability under the curve",
    "text": "Find the probability under the curve\n\\[\\begin{eqnarray*}\nP(-1.7 \\leq Z\\leq 1.7) &=& 2\\times P(0\\leq Z\\leq 1.7) \\\\ &=& 2\\times(0.5-0.0446) = 0.9108\n\\end{eqnarray*}\\]\n\nOR\n\n\n\\[\\begin{eqnarray*}\nP(-1.7 \\leq Z\\leq 1.7) &=& 1-P(Z\\leq -1.7 {\\rm ~or~} Z\\geq 1.7) \\\\ &=& 1-2\\times P(Z\\geq 1.7) \\\\ &=& 1-2\\times 0.0446 = 1-0.0892 = 0.9108.\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#transforming-a-general-normal-variable-to-a-standard-normal-variable",
    "href": "content/slides/w2_2-slides.html#transforming-a-general-normal-variable-to-a-standard-normal-variable",
    "title": "Week 2",
    "section": "Transforming a general normal variable to a standard normal variable",
    "text": "Transforming a general normal variable to a standard normal variable\nA random variable \\(Y\\) may be normally distributed with parameters \\(\\mu\\) and \\(\\sigma^2\\) and written as:\n\\[\nY\\sim N(\\mu,\\sigma^2)\n\\]\n\nFor example, \\(Y\\sim N(20,3^2)\\) stands for normal distribution with \\(\\mu=20,\\sigma^2=9\\).\n\n\n Suppose \\(Y\\sim N(\\mu,\\sigma^2)\\), then a very useful transformation is:\n\\[\nZ=\\frac{Y-\\mu}{\\sigma}\\sim N(0,1)\n\\]"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#transforming-a-general-normal-variable-to-a-standard-normal-variable-1",
    "href": "content/slides/w2_2-slides.html#transforming-a-general-normal-variable-to-a-standard-normal-variable-1",
    "title": "Week 2",
    "section": "Transforming a general normal variable to a standard normal variable",
    "text": "Transforming a general normal variable to a standard normal variable\nFor \\(Y\\sim N(20,3^2)\\) (i.e. \\(\\mu=20,\\sigma^2=9\\)), we have \\[\\begin{eqnarray*}\nP(20\\leq Y\\leq 23) &=& P(20-\\mu\\leq Y-\\mu\\leq 23-\\mu) \\\\\n&=& P\\left(\\frac{20-\\mu}{\\sigma}\\leq \\frac{Y-\\mu}{\\sigma}\\leq \\frac{23-\\mu}{\\sigma}\\right) \\\\\n&=& P\\left(\\frac{20-20}{3}\\leq Z\\leq \\frac{23-20}{3}\\right) \\\\\n&=& P(0\\leq Z\\leq 1) \\\\\n&=& P(Z\\geq 0)-P(Z\\geq 1) \\\\\n&=& 0.5-0.1587 = 0.3413\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "content/slides/w2_2-slides.html#z-scores-and-interpretation",
    "href": "content/slides/w2_2-slides.html#z-scores-and-interpretation",
    "title": "Week 2",
    "section": "Z-scores and interpretation",
    "text": "Z-scores and interpretation\n\nGiven a value \\(y\\), the term: \\[\nz=\\frac{y-\\mu}{\\sigma}\n\\] is called the z-score corresponding to y. It represents the number of standard deviations \\(y\\) is from \\(\\mu\\).\n\n\nFor \\(Y\\sim N(20,3^2)\\),\n\n\n\n 23 is \\(1\\sigma\\) away from \\(\\mu=20\\).\n\n\n\n\n 20 is \\(0\\sigma\\) away from \\(\\mu=20\\) (i.e. at \\(\\mu=20\\)).\n\n\n\n\n 24 is \\(\\frac{4}{3}\\sigma\\) away from \\(\\mu=20\\).\n\n\n\n\n 17 is \\(-1\\sigma\\) away from \\(\\mu=20\\)."
  },
  {
    "objectID": "content/slides/w2_2-slides.html#transforming-a-general-normal-variable-to-a-standard-normal-variable-2",
    "href": "content/slides/w2_2-slides.html#transforming-a-general-normal-variable-to-a-standard-normal-variable-2",
    "title": "Week 2",
    "section": "Transforming a general normal variable to a standard normal variable",
    "text": "Transforming a general normal variable to a standard normal variable\nSuppose \\(Y\\sim N(20,3^2)\\), what is \\(P(Y\\leq 15.5)\\)?\n\\[\\begin{eqnarray*}\nP(Y\\leq 15.5) &=& P\\left(\\frac{Y-\\mu}{\\sigma}\\leq \\frac{15.5-\\mu}{\\sigma}\\right) \\\\\n&=& P\\left(Z\\leq \\frac{15.5-20}{3}\\right) \\\\\n&=& P(Z\\leq -1.5) \\\\\n&=& 0.0668\n\\end{eqnarray*}\\]"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#the-beginnings-of-statistical-analyses",
    "href": "content/slides/w3_1-slides.html#the-beginnings-of-statistical-analyses",
    "title": "Week 3",
    "section": "The beginnings of statistical analyses",
    "text": "The beginnings of statistical analyses\n\n\n Statistical method\n\n\n\n\nn-1 Degrees of freedom\n\n\n\n\n Data transformations"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#the-protocol-for-answering-a-research-question",
    "href": "content/slides/w3_1-slides.html#the-protocol-for-answering-a-research-question",
    "title": "Week 3",
    "section": "The protocol for answering a research question",
    "text": "The protocol for answering a research question\n\nA population is a whole set/group of individuals/objects which we are interested in studying. I.e. the complete set of all possible observations (finite or infinite)"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#statistical-method-1",
    "href": "content/slides/w3_1-slides.html#statistical-method-1",
    "title": "Week 3",
    "section": "Statistical method",
    "text": "Statistical method\n\n\nOur typical approach to answering scientific questions uses inductive reasoning: attempting to form generalities from specific cases (i.e., data)\n\n\n\n The sparrows nest outside my window is made of grass\n\n\n The nest (and my window) is near a grassland\n\n\n Sparrows use grasses in their nests because grass is more abundant in their habitat\n\n\n\n\n\n\n\n\nImportant\n\n\nWe will also use deductive reasoning (general –&gt; specific) to derive specific predictions from our general hypotheses (e.g., If sparrows use grass because it is more abundant, then sparrows in a woodland habitat should use more twigs)"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#a-formal-statistical-method",
    "href": "content/slides/w3_1-slides.html#a-formal-statistical-method",
    "title": "Week 3",
    "section": "A formal statistical method",
    "text": "A formal statistical method\n\nState research question\nFormulate null and alternative hypotheses\nIdentify population variable and, when possible, its distribution\nSample data according to chosen sampling procedure\nExamine your data\nDetermine and calculate appropriate test statistic\nReject or “accept” null hypothesis\nState conclusion and answer question in step 1"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#what-we-dont-do",
    "href": "content/slides/w3_1-slides.html#what-we-dont-do",
    "title": "Week 3",
    "section": "What we DON’T do",
    "text": "What we DON’T do\n\nState research question\nFormulate null and alternative hypotheses\nFormulate conclusion based on perception, prior knowledge, or other maligned incentive\nIdentify population variable and distribution\nSample data according to chosen sampling procedure\nExamine data\nDetermine and calculate test statistic most likely to conform to predetermined conclusion\nReject or “accept” null hypothesis\nState “conclusion” and answer question in step 1\n\n\n\n\n\n\n\n\nDanger!\n\n\nThis process is one of many ways to “p-hack” your way to “significant” results"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#df---a-concept-that-takes-some-time-to-get",
    "href": "content/slides/w3_1-slides.html#df---a-concept-that-takes-some-time-to-get",
    "title": "Week 3",
    "section": "df - a concept that takes some time to get",
    "text": "df - a concept that takes some time to get"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#df-degrees-of-what",
    "href": "content/slides/w3_1-slides.html#df-degrees-of-what",
    "title": "Week 3",
    "section": "df: degrees of what?",
    "text": "df: degrees of what?\n\nWe’re putting all of the pieces in place to begin doing statistical tests of our data\n\n\n A key component of most statistical tests is the degrees of freedom of the reference distribution of our tests statistic (e.g., t, F)\n\n\n It’s very important, but can be a bit of a mind-bending process to fully comprehend. Let’s break it down."
  },
  {
    "objectID": "content/slides/w3_1-slides.html#example-using-sample-variance",
    "href": "content/slides/w3_1-slides.html#example-using-sample-variance",
    "title": "Week 3",
    "section": "Example using sample variance",
    "text": "Example using sample variance\n\nWe learned that sample variance (\\(s^2\\)) is calculated as:\n\\[\ns^2 = \\frac{\\sum_{i=1}^n (x_i-\\bar x)^2}{n-1}\n\\]\nwhere \\(x\\) is a given sample, and \\(\\bar x\\) is the sample mean.\n\n\n\nWhy do we divide by \\(n-1\\) but not by \\(n\\)?"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#degrees-of-freedom-1",
    "href": "content/slides/w3_1-slides.html#degrees-of-freedom-1",
    "title": "Week 3",
    "section": "Degrees of freedom",
    "text": "Degrees of freedom\n\nIn this case, \\(n-1\\) is the degrees of freedom in our sample. This refers to the number of observations within our sample that are free to vary.\n\n\nPut another way, it’s the number of observations that you would need to recover all of the information about a sample if all that you had was a summary statistic, the sample mean (\\(\\bar x\\))"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#examples",
    "href": "content/slides/w3_1-slides.html#examples",
    "title": "Week 3",
    "section": "Examples",
    "text": "Examples\n A coin toss gives us 1 degree of freedom. Even with 100 tosses, the fact that there are only two outcomes means that we can recover all of the information about a sample with 1 piece of information (e.g,, n heads, or tails). Only one of those values is free to vary!"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#example-1",
    "href": "content/slides/w3_1-slides.html#example-1",
    "title": "Week 3",
    "section": "Example 1",
    "text": "Example 1\n A coin toss gives us 1 degree of freedom. Even with 100 tosses, the fact that there are only two outcomes means that we can recover all of the information about a sample with 1 piece of information (e.g,, n heads, or tails). Only one of those values is free to vary!"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#example-2",
    "href": "content/slides/w3_1-slides.html#example-2",
    "title": "Week 3",
    "section": "Example 2",
    "text": "Example 2\n A traffic light gives us 2 degrees of freedom. We have 3 total outcomes (Red, Yellow, and Green), and we need two required pieces of information to resolve the full dataset (and whether there is a cat on the light)"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#general-formula-for-the-df",
    "href": "content/slides/w3_1-slides.html#general-formula-for-the-df",
    "title": "Week 3",
    "section": "General formula for the df",
    "text": "General formula for the df\n\nIn general, formula to calculate degrees of freedom is: \\(df = n - p\\)\n\n \\(n\\) is equal to the sample size\n\n \\(p\\) is equal to the number of parameters (restrictions) that are imposed)"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#practical-implications-for-df",
    "href": "content/slides/w3_1-slides.html#practical-implications-for-df",
    "title": "Week 3",
    "section": "Practical implications for df",
    "text": "Practical implications for df\n\nDividing by \\(n\\) will consistently underestimate the variance of a population. This, in effect, is because when we divide by \\(n\\), we essentially double dip with one of our pieces of information, given that we already know the sample mean. The effect of this double dipping is ending up with an estimate of the population variance (\\(s^2\\)) that will always be smaller than the true population variance (\\(\\sigma^2\\)).\n\n\n\n\n\n\n\n\nImportant\n\n\nThis is the real reason why degrees of freedom is so important. Without accounting for the used up bits of information, we end up with biased estimates of our parameters, and this can lead to inflated error rates in our statistical tests and inference."
  },
  {
    "objectID": "content/slides/w3_1-slides.html#practical-implications-for-df-1",
    "href": "content/slides/w3_1-slides.html#practical-implications-for-df-1",
    "title": "Week 3",
    "section": "Practical implications for df",
    "text": "Practical implications for df\n\nMore degrees of freedom means that we have more independent pieces of information with which to estimate our parameter of choice. This leads to more precise estimates, and more powerful statistical tests.\n\n\n\n\ndf and \\(t\\) distribution \n\ndf and \\(\\chi^2\\) distribution"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#why-transform-data",
    "href": "content/slides/w3_1-slides.html#why-transform-data",
    "title": "Week 3",
    "section": "Why transform data?",
    "text": "Why transform data?\n\nMost of the parametric (i.e., “classical”) statistical analyses we conduct depend on an assumption of data that are independent and normally distributed (or have residuals that are normally distributed).\n\n\n\n Most of the time in ecology, this ain’t the case!\n\n\n\n\n To meet this assumption, we have a couple of options. Transforming our data is one of the options that we have."
  },
  {
    "objectID": "content/slides/w3_1-slides.html#what-transformations-do",
    "href": "content/slides/w3_1-slides.html#what-transformations-do",
    "title": "Week 3",
    "section": "What transformations do",
    "text": "What transformations do\n\nTransformations change the spacing between our data points, but do not change the rank order (e.g., If we transform data points that have the values 4.5 and 10.7, the transformed 10.7 will always be &gt; the transformed 4.5)\n\n\n\n Often, we have to try several transformations to best “normalize” our data, but we have a few tried-and-true starting points.\n\n\n\n\n Typical transformations include: log, square-root, arcsin (angular), reciprocal, and Box-Cox.\n\n\n\n\n\n\n\n\n\nBe warned!\n\n\nThere is a lively debate on the practice of transforming data among ecological/statistical practitioners and experts. In general, I caution against transforming data unless there is clear precedent in your field and/or a more robust analysis framework (e.g., generalized linear models) cannot be used (which, in my experience, is rare)."
  },
  {
    "objectID": "content/slides/w3_1-slides.html#examples-of-transformations-square-root",
    "href": "content/slides/w3_1-slides.html#examples-of-transformations-square-root",
    "title": "Week 3",
    "section": "Examples of transformations: square root",
    "text": "Examples of transformations: square root\n\nThe square root transformation (\\(\\sqrt x\\)) is often used on discrete count variables (e.g., the number of insects in a pitfall trap, number of flowers visited by a bee).\n\n\n\n Count data is often Poisson distributed, and taking the square root tends to normalize the data (i.e., approximates a normal distribution).\n\n\n\n\n The better way to do this, however, is to use Poisson regression (i.e., a generalized linear model)"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#examples-of-transformations-logarithmic",
    "href": "content/slides/w3_1-slides.html#examples-of-transformations-logarithmic",
    "title": "Week 3",
    "section": "Examples of transformations: logarithmic",
    "text": "Examples of transformations: logarithmic\n\nThe log transformation (\\(log(x)\\)) is used to normalize very skewed count data (i.e., data that has a long tail).\n\n\n\n Most of the time, when we say log transformed, we are referrng ot the natural log [\\(ln(x)\\)] and NOT the base-10 log [\\(log_{10}(x)\\)]\n\n\n\n\n Often, we use \\(log(x_1)\\). Why?\n\n\n\n\n Knowing which base you are using is important for back-transforming your data to interpret/plot"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#examples-of-transformations-angular-arcsin",
    "href": "content/slides/w3_1-slides.html#examples-of-transformations-angular-arcsin",
    "title": "Week 3",
    "section": "Examples of transformations: angular (arcsin)",
    "text": "Examples of transformations: angular (arcsin)\n\nThe angular transformation \\(arcsin(\\sqrt x)\\) is used to normalize proportional data between 0 and 1.\n\n\n\n This method can have distinct problems if your sample size for each proportion (i.e., the denominator) is not the same for all proportions analyzed (https://esj-journals.onlinelibrary.wiley.com/doi/10.1111/j.1440-1703.2003.00620.x)\n\n\n\n\n For analyzing proportion data, there are better methods such as using logistic regression (i.e., a GLM)."
  },
  {
    "objectID": "content/slides/w3_1-slides.html#examples-of-transformations-reciprocal",
    "href": "content/slides/w3_1-slides.html#examples-of-transformations-reciprocal",
    "title": "Week 3",
    "section": "Examples of transformations: reciprocal",
    "text": "Examples of transformations: reciprocal\n\nThe reciprocal transformation \\(1/x\\) is used to normalize rate data (e.g., the number of offspring per female)"
  },
  {
    "objectID": "content/slides/w3_1-slides.html#examples-of-transformations-box-cox",
    "href": "content/slides/w3_1-slides.html#examples-of-transformations-box-cox",
    "title": "Week 3",
    "section": "Examples of transformations: Box-Cox",
    "text": "Examples of transformations: Box-Cox\n\nThe Box-Cox transformation is a power transformation that is done automagically using software that determines the best exponent to transform and normalize data (e.g, \\(y^0.056493\\))\n\n\n\n This is a lot to do, and now that we have other methods like generalized linear models, I would start with much simpler transformations or more appropriate tools given the type of data you are attempting to model."
  },
  {
    "objectID": "content/slides/w3_1-slides.html#transformations-which-is-best",
    "href": "content/slides/w3_1-slides.html#transformations-which-is-best",
    "title": "Week 3",
    "section": "Transformations: which is best?",
    "text": "Transformations: which is best?\n\nWhen fitting a model, examining your residual plots can help guide your choice of transformation. These plots are diagnostic of many ways in which we can violate the assumptions of our various modeling approaches.\n\n\n\n In all cases, the first choice should be using a model framework more appropriate to your particular data than shoe-horning transformed data into a linear model because that’s what you know how to use.\n\n\n\n\n However, there are particular cases where tranforming data is a normal practice and, provided you can support your decision with a clear rationale or citations."
  },
  {
    "objectID": "content/slides/w3_2-slides.html#structuring-statistics-around-our-research-questions",
    "href": "content/slides/w3_2-slides.html#structuring-statistics-around-our-research-questions",
    "title": "Week 3",
    "section": "Structuring statistics around our research questions",
    "text": "Structuring statistics around our research questions\n\n\n Hypothesis formulation\n\n\n\n\n One-sample t tests\n\n\n\n\n Paired, two-sample t tests"
  },
  {
    "objectID": "content/slides/w3_2-slides.html#the-tail-pun-intended-of-two-hypotheses",
    "href": "content/slides/w3_2-slides.html#the-tail-pun-intended-of-two-hypotheses",
    "title": "Week 3",
    "section": "The tail (pun intended) of two hypotheses",
    "text": "The tail (pun intended) of two hypotheses\n\nLet’s start by formulating some basic hypotheses around a simple research question. To do that, we will frame out two competing hypotheses:\n\n\n\n First, write the null hypothesis, \\(H_0\\). This reflects the claim believed to be true, that a system is governed by simple, random variation in the universe.\n\n\n\n\n Next, specify the alternative hypothesis, \\(H_A\\). This reflects a departure from \\(H_0\\) that we are designing and experiment or study to be able to detect: that the results of our sampling reflect some factor or influence other than random variation."
  },
  {
    "objectID": "content/slides/w3_2-slides.html#how-many-tails-do-we-use",
    "href": "content/slides/w3_2-slides.html#how-many-tails-do-we-use",
    "title": "Week 3",
    "section": "How many tails do we use?",
    "text": "How many tails do we use?\n\nIn previous stats courses, you likely covered one- and two-tailed statistical tests, which ultimately stem from the structure of our hypotheses.\n\n\n\n In practice, we almost always will be using two-tailed tests, unless we have very specific reasons to suspect a directional hypothesis.\n\n\n\n\n\n\n\n\n\nMake your intentions clear!\n\n\nBefore conducting any tests (or even looking at your data), decide on whether you will be using a one- vs. two-tailed test of \\(H_A\\)."
  },
  {
    "objectID": "content/slides/w3_2-slides.html#hypothesis-formulation-example-one-tailed",
    "href": "content/slides/w3_2-slides.html#hypothesis-formulation-example-one-tailed",
    "title": "Week 3",
    "section": "Hypothesis formulation example: one-tailed",
    "text": "Hypothesis formulation example: one-tailed\n\nResearch question: Are maple trees taller than oak trees?\n\n\n\n \\(H_0\\): There is no difference between the heights of oak and maple trees\n\n\\[\\mu_{oak} = \\mu_{maple}\\]\n\n\n\n \\(H_A\\): Oak trees are taller than maple trees\n\n\\[\\mu_{oak} &gt; \\mu_{maple}\\]"
  },
  {
    "objectID": "content/slides/w3_2-slides.html#hypothesis-formulation-example-two-tailed",
    "href": "content/slides/w3_2-slides.html#hypothesis-formulation-example-two-tailed",
    "title": "Week 3",
    "section": "Hypothesis formulation example: two-tailed",
    "text": "Hypothesis formulation example: two-tailed\n\nIn practice, we would want to formulate our hypothesis more conservatively – allowing for any potential differences between our sampled populations:\n\n\n\n \\(H_0\\): There is no difference between the heights of oak and maple trees\n\n\\[\\mu_{oak} = \\mu_{maple}\\]\n\n\n\n \\(H_A\\): There is a difference in height between oak and maple trees\n\n\\[\\mu_{oak} ≠ \\mu_{maple}\\]"
  },
  {
    "objectID": "content/slides/w3_2-slides.html#hypothesis-formulation-example-two-tailed-1",
    "href": "content/slides/w3_2-slides.html#hypothesis-formulation-example-two-tailed-1",
    "title": "Week 3",
    "section": "Hypothesis formulation example: two-tailed",
    "text": "Hypothesis formulation example: two-tailed\n\nResearch question: We want to know whether patients experience a reduction in coughing when given cough medication \n\n\n\n \\(H_0\\): There is no difference in number of coughs per hour with or without \n\n\\[\\mu_{medicated} = \\mu_{unmedicated}\\]\n\n\n\n \\(H_A\\): There is a difference coughs per hour when taking \n\n\\[\\mu_{medicated} ≠ \\mu_{unmedicated}\\]"
  },
  {
    "objectID": "content/slides/w3_2-slides.html#pair-up-and-formulate",
    "href": "content/slides/w3_2-slides.html#pair-up-and-formulate",
    "title": "Week 3",
    "section": "Pair up and formulate!",
    "text": "Pair up and formulate!\n\nResearch question: We want to know whether a particular species of trout prefers riffles or pools (two habitat types) within a stream"
  },
  {
    "objectID": "content/slides/w3_2-slides.html#significance-tests-an-overview",
    "href": "content/slides/w3_2-slides.html#significance-tests-an-overview",
    "title": "Week 3",
    "section": "Significance tests: an overview",
    "text": "Significance tests: an overview\n\nWhen we go out and perform an experiment and collect data, our sample data are outcomes of a data generating process\n\n\n\n The data generating process may be described by a probability distribution with certain parameters\n\n\n\n\n We begin by considering a random sampole \\(Y_1, Y_2, Y_3,...Y_n\\) of size \\(n\\) from a normal distribution \\(N(\\mu, \\sigma^2)\\) with unknown \\(\\mu\\) and known \\(\\sigma^2\\).\n\n\n\n\n We then use statistics from the sample data to draw inference about the parameters of the probability distribution (e.g., whether the mean is different than our hypothesized value)"
  },
  {
    "objectID": "content/slides/w3_2-slides.html#significance-tests-an-overview-1",
    "href": "content/slides/w3_2-slides.html#significance-tests-an-overview-1",
    "title": "Week 3",
    "section": "Significance tests: an overview",
    "text": "Significance tests: an overview\n\nFor example, use the sample mean \\(\\bar{Y}\\) to draw inference about the population mean \\(\\mu\\).\n\n\n\n There are several ways of pursuing statistical inference. One of them is called significance testing.\n\n\n\n\n The main idea of significance tests is to make a yes/no decision about some aspect of a population, based on a single sample."
  },
  {
    "objectID": "content/slides/w3_2-slides.html#significance-tests-the-ingredients-needed",
    "href": "content/slides/w3_2-slides.html#significance-tests-the-ingredients-needed",
    "title": "Week 3",
    "section": "Significance tests: the ingredients needed",
    "text": "Significance tests: the ingredients needed\n\nAspect of a population that is of interest (motivated by research question)\nNull hypothesis \\(H_0\\)\nAlternative hypothesis \\(H_A\\)\nTest statistic and null distribution\nMeasure evidence against \\(H_0\\)\nMake a decision and interpret in the context of the problem or question"
  },
  {
    "objectID": "content/slides/w3_2-slides.html#significance-tests-the-recipe-needed",
    "href": "content/slides/w3_2-slides.html#significance-tests-the-recipe-needed",
    "title": "Week 3",
    "section": "Significance tests: the recipe needed",
    "text": "Significance tests: the recipe needed\n\nAspect of a population that is of interest (motivated by research question)\nNull hypothesis \\(H_0\\)\nAlternative hypothesis \\(H_A\\)\nTest statistic and null distribution\nMeasure evidence against \\(H_0\\)\nMake a decision and interpret in the context of the problem or question"
  },
  {
    "objectID": "content/slides/w3_2-slides.html#test-statistic-distribtiions",
    "href": "content/slides/w3_2-slides.html#test-statistic-distribtiions",
    "title": "Week 3",
    "section": "Test statistic distribtiions",
    "text": "Test statistic distribtiions\n\n\\(Z\\): standard normal distribution - no parameters, mean/variance fixed\n\\(t\\): one parameter, two-tailed distribution\n\\(F\\): two-parameter, one-tailed distribution\n\\(\\chi^2\\): one-parameter, one-tailed distribution"
  },
  {
    "objectID": "content/slides/w3_2-slides.html#significance-tests-for-a-known-sigma",
    "href": "content/slides/w3_2-slides.html#significance-tests-for-a-known-sigma",
    "title": "Week 3",
    "section": "Significance tests for a known \\(\\sigma\\)",
    "text": "Significance tests for a known \\(\\sigma\\)\n\nIn your previous courses/stats work, you probably started with calculating a \\(Z\\) statistic using the formula:\n\n\\[Z = \\frac{\\bar{Y} - \\mu}{\\sigma / \\sqrt{n}}\\]\n\n\n This value was then compared to a \\(Z\\) distribution where you could obtain a p-value.\n\n\n\n\n In ecology (and really, every field), we never know the population variance \\(\\sigma^2\\), so we cannot use a \\(Z\\) statistic.\n\n\n\n\n Instead, we estimate \\(\\sigma^2\\) using the sample variance \\(S^2\\) and calculate a \\(t\\) statistic instead!"
  },
  {
    "objectID": "content/slides/w3_2-slides.html#t-z-whats-the-difference",
    "href": "content/slides/w3_2-slides.html#t-z-whats-the-difference",
    "title": "Week 3",
    "section": "\\(t\\), \\(Z\\), what’s the difference!?",
    "text": "\\(t\\), \\(Z\\), what’s the difference!?"
  }
]