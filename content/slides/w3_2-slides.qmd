---
title: "Week 3"
subtitle: "Hypotheses and t-tests"
editor:
  mode: source
format:
  revealjs: 
    output-file: w3_2-slides.html
    theme: ["simple", "slide_styles/slide-styles.scss"]
    smaller: true
    # navigation-mode: vertical
    slide-number: c/t
    chalkboard: 
      buttons: true
    preview-links: auto
    auto-stretch: false
    transition: fade
    transition-speed: slow
    scrollable: true
    logo: /media/5126-header.png
    css: ["default", "slide_styles/slide-styles.css", "slide_styles/slide-styles.scss"]
---

# What we'll cover today

## Structuring statistics around our research questions

. . .

::: box-1-list
[{{< fa circle-question >}}]{.margin-1} Hypothesis formulation
:::

. . .

::: box-1-list
[{{< fa chart-simple >}}]{.margin-1} One-sample *t* tests
:::

. . .

::: box-1-list
[{{< fa chart-simple >}}]{.margin-1} Paired, two-sample *t* tests
:::

# Hypothesis formulation

## The tail (pun intended) of two hypotheses

::: box-6-list
Let's start by formulating some basic hypotheses around a simple research question. To do that, we will frame out two competing hypotheses:
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> First, write the [null hypothesis]{.highlight-tomato}, $H_0$. This reflects the claim believed to be true, that a system is governed by simple, random variation in the universe.
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> Next, specify the [alternative hypothesis]{.highlight-tomato}, $H_A$. This reflects a departure from $H_0$ that we are designing and experiment or study to be able to detect: that the results of our sampling reflect some factor or influence other than random variation.
:::

## Hypothesis formulation example:

::: box-6-list
Research question: Are maple trees ***taller than*** oak trees? 
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> $H_0$: There is no difference between the heights of oak and maple trees
:::

. . . 

$$\mu_{oak} = \mu_{maple}$$

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> $H_A$: Oak trees are taller than maple trees
:::

. . . 

$$\mu_{oak} > \mu_{maple}$$

## How many tails do we use?

::: box-6-list
In previous stats courses, you likely covered one- and two-tailed statistical tests, which ultimately stem from the structure of our hypotheses.
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> In practice, we almost always will be using *two-tailed* tests, unless we have very specific reasons to suspect a directional hypothesis. 
:::

. . . 

:::{.callout-important}
## Make your intentions clear!
Before conducting any tests (or even looking at your data), decide on whether you will be using a one- vs. two-tailed test of $H_A$.
:::


## Hypothesis formulation example: two-tailed

::: box-6-list
In practice, we would want to formulate our hypothesis more conservatively -- allowing for any potential differences between our sampled populations:
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> $H_0$: There is no difference between the heights of oak and maple trees
:::

. . . 

$$
\mu_{oak} = \mu_{maple}
$$

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> $H_A$: There is a difference in height between oak and maple trees
:::

. . . 

$$\mu_{oak} ≠ \mu_{maple}$$

## Hypothesis formulation example: two-tailed

::: box-6-list
Research question: We want to know whether patients experience a reduction in coughing when given cough medication {{< fa capsules >}}
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> $H_0$: There is no difference in number of coughs per hour with or without {{< fa capsules >}}
:::

$$\mu_{medicated} = \mu_{unmedicated}$$

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> $H_A$: There is a difference coughs per hour when taking {{< fa capsules >}}
:::

$$\mu_{medicated} ≠ \mu_{unmedicated}$$

## Pair up and formulate!

::: box-6-list
Research question: We want to know whether a particular species of trout prefers riffles or pools (two habitat types) within a stream {{< fa fish-fins >}}. In your group, formulate a scientific and statistical hypothesis to address this question.
:::

::: {.center-x}
![](https://healthyheadwaterslab.ca/wp-content/uploads/2021/03/v2H.png)
:::

# One-sample *t* tests

## Significance tests: an overview
::: box-6-list
When we go out and perform an experiment and collect data, our sample data are outcomes of a data generating process
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> The data generating process may be described by a probability distribution with certain parameters
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> We begin by considering a random sample $Y_1, Y_2, Y_3,...Y_n$ of size $n$ from a normal distribution $N(\mu, \sigma^2)$ with unknown $\mu$ and known $\sigma^2$. 
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> We then use statistics from the sample data to draw *inference* about the parameters of the probability distribution (e.g., whether the mean is different than our hypothesized value)
:::

## Significance tests: an overview
::: box-6-list
For example, use the sample mean $\bar{Y}$ to draw inference about the population mean $\mu$.
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> There are several ways of pursuing statistical inference. One of them is called [significance testing]{.highlight-tomato}.
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> The main idea of significance tests is to make a yes/no decision about some aspect of a population, based on a single sample. 
:::

## Significance tests: the recipe needed

::: {.incremental}
1. Aspect of a population that is of interest (motivated by research question)
2. Null hypothesis $H_0$
3. Alternative hypothesis $H_A$
4. Test statistic and null distribution
5. Measure evidence against $H_0$
6. Make a decision and interpret in the context of the problem or question
:::

## Test statistic distribtiions

::: {.incremental}
1. $Z$: standard normal distribution - no parameters, mean/variance fixed
2. $t$: one parameter, two-tailed distribution
3. $F$: two-parameter, one-tailed distribution
4. $\chi^2$: one-parameter, one-tailed distribution
:::

## Significance tests for a known $\sigma$
::: box-6-list
In your previous courses/stats work, you probably started with calculating a $Z$ statistic using the formula: 
:::

$$Z = \frac{\bar{Y} - \mu}{\sigma / \sqrt{n}}$$

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> This value was then compared to a $Z$ distribution where you could obtain a p-value. 
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> In ecology (and really, every field), we never know the population variance $\sigma^2$, so we cannot use a $Z$ statistic. 
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> Instead, we estimate $\sigma^2$ using the sample variance $S^2$ and calculate a $t$ statistic instead!
:::

## $t$, $Z$, what's the difference!?
::: box-6-list
Like a $Z$ distribution, a $t$ distribution is defined by a symmetric, bell-shaped curve symmetric about 0
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> But, the $t$ distribution has heavier tails than the $Z$ does
:::

## $t$, $Z$, what's the difference!?

::: {.center-x}
![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k19QblVo30Cb2Q_rOs0QAA.png)
:::

## $t$, $Z$, what's the difference!?
::: box-6-list
Like a $Z$ distribution, a $t$ distribution is defined by a symmetric, bell-shaped curve symmetric about 0
:::

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> But, the $t$ distribution has heavier tails than the $Z$ does
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> The shape of the curve depends on the sample size, $n$. The larger the $n$, the thinner the tails/lesser the spread. In fact, $t_\infty = Z$
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> This shape takes into consideration the [degrees of freedom]{.highlight-tomato} of our sample, effectively penalyzing for our uncertainty of the population variance given we're estimating it with the sample variance.
:::

## $t$ significance testing with unknown $\sigma$
::: box-6-list
Consider a random sample $Y_1, Y_2, Y_3,...Y_n$ of size $n$ from a normal distribution $N(\mu, \sigma^2)$ with *unknown* $\mu$ and *unknown* $\sigma^2$.
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> Suppose our significance test of interest is: 
:::

$$H_0 : \mu = \mu_0$$

. . .

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> We can calculate our test statistic as:
:::

$$T = \frac{\bar{Y} - \mu_0}{S / \sqrt{n}} \sim T_{n-1}$$ 

::: indented-text
where $T_{n-1}$ is a $T$ distribution with $n-1$ degrees of freedom.
:::

## $t$ significance testing with unknown $\sigma$
**Wheat yield example**

::: box-6-list
Six, 1-acre plots are sown with a new variety of wheat. The yields for each plot (in cwt/acre) are: `25, 21, 24, 20, 26, 22` and are from $N(\mu, \sigma^2)$. 
:::

. . .

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> Is there evidence that the population mean yield for this variety of wheat differs from the historical average yield (20 cwt/acre) of the old variety of wheat you were using?
:::

## $t$ significance testing with unknown $\sigma$
**Wheat yield example**

::: box-6-list
Let $\mu$ = population mean yield of this variety of wheat. What are our null and alternative hypotheses?
:::

. . .

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> $H_0 : \mu = 20$
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> $H_A : \mu ≠ 20$
:::

. . .

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> Under $H_0$, the test statistic is: 
:::

$$T = \frac{\bar{Y} - \mu_0}{S / \sqrt{n}} \sim T_{n-1}$$

## $t$ significance testing with unknown $\sigma$
**Wheat yield example**

::: box-6-list
Let's gather and list out our values of interest. 
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> $n = 6, \bar{y} = 23, s = 2.37$
:::


. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> Our observed test statistic is then:
:::

$$T = \frac{\bar{Y} - \mu_0}{S / \sqrt{n}} = \frac{23 - 20}{2.37 / \sqrt{6}} = 3.10$$

::: indented-text
on $df = n - 1 = 5$
:::

::: indented-text
Using our $t$ table or some statistics software, we can calculate that:
:::

$$P(T_5 ≥ 3.10) \approx 0.0138 \times 2 = \textbf{0.0276}$$

## $t$ significance testing with unknown $\sigma$
**Wheat yield example**

::: box-6-list
We can then use this p-value and compare it against a [critical threshold]{.highlight-tomato} of, for example, $\alpha = 0.05$. If our p-value is < 0.05, we can then...
:::

. . . 

:::{.center-x}
*Reject $H_0$ at the $\alpha = 0.05$ level*
:::
</br>

. . . 

:::{.center-x}
or...
:::
</br>

:::{.center-x}
*"The mean yield of our new wheat variety is significantly different than the historical mean yield."*
:::

## Interpreting p-values
::: box-6-list
The p-value can be interpreted as the evidence that we have against $H_0$. The p-value is the probability that we would observe a value equal to or more extreme than our sample mean *if the null hypothesis were true*. 
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> At our value of $p = 0.0276$, we would be unlikely to find a mean sample yield of 23 if our null hypothesis of $\mu_0 = 20$ were true. 
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> P-values are nothing more than a guide to making a decision. It allows us to make conclusions about our *statistical hypothesis* of whether there are patterns in our data, but ***not*** whether those patterns are driven by our *scientific hypothesis*
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> It's up to us to draw conclusions about the validity of our scientific hypothesis based on the statistical patterns in the data. The strength of this inference depends on many factors. 
:::

## A general guide to interpreting p-values

::: box-6-list
You'll often read papers and see language describing the strength of statistical support. Assuming a critical threshold of $\alpha = 0.05$, then we can intrepret a p-value: 
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> ≥ 0.10 as no evidence against $H_0$
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> between 0.05 and 0.10 as weak evidence against $H_0$
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> between 0.01 and 0.05 as moderate evidence against $H_0$
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> between 0.001 and 0.01 as strong evidence against $H_0$
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> smaller than 0.001 as very strong evidence against $H_0$
:::

## Assumptions for t-tests

::: {.incremental}
1. Data are independent
2. $Y_1....Y_n$ form a random sample of size $n$ from $N(\mu, \sigma^2)$
3. $\sigma^2$ is unknown (i.e., no assumption about population variance)
4. Any sample size $n$ (i.e., no assumption about sample size)
:::

. . . 

:::{.callout-note}
## Assumptions are not equal!
T-tests are relatively robust to data that are not normally distributed. However, tests are very sensitive to dependence among samples. Our data must be independent!
:::

# Paired, two-sample $t$ test

## Comparing two samples
::: box-6-list
Two-sample comparisons are far more common in ecology and biology than one-sample studies. Some examples include:
:::

. . . 

::: indented-text
[{{< fa cow >}}]{.margin-1} Compare milk yields of cows on two different diets
:::

. . . 

::: indented-text
[{{< fa tree >}}]{.margin-1} Compare timber volumes of two species of tree in a forest
:::

. . . 

::: indented-text
[{{< fa heart >}}]{.margin-1} Compare heart rates of patients before and after a drug treatment
:::

. . . 

::: indented-text
[{{< fa dog >}}]{.margin-1} Compare blood flow in two different arteries of dogs
:::

## Paired or unpaired?
::: box-6-list
There are two types of two-sample experiments: paired vs. unpaired two-sample designs
:::

. . .

::: indented-text
[{{< fa person >}}]{.margin-1} In an independent, two-sample design, there are two treatments (e.g., treatment and placebo), but there is *no direct relationship* between an observation on trt A and an observation on trt B
:::

. . .

::: indented-text
[{{< fa people-arrows >}}]{.margin-1} In an paired two-sample design, there are two treatments (e.g., treatment and control) and each observation of treatment A is paired with an observation of treatment B. Related or the same experimental units are used for both treatments (i.e., on the same person, animal, etc.)
:::

. . .

::: {.callout-important}
## Design of experimental is critical!
Choice of paired versus independent two-sample experiment is an important experimental design issue that must be considered!
:::

. . .

::: {.callout-important}
## Choice of analyses is critical!
In addition, it's critical that we always analyze an experiment consistent with the manner in which it was designed.
:::

## Paired or unpaired? An example

::: box-6-list
Let's imagine comparing the heart rates of 10 patients before and after a drug treatment. Which design would you prefer?
:::

. . .

::: indented-text
{{< fa heart-pulse >}}{{< fa person >}} Heart rates of 10 patients before and after a drug treatment (same experimental units)
:::

. . .

::: indented-text
{{< fa heart-pulse >}}{{< fa people-arrows >}} Heart rates of 10 patients before drug treatment and heart rates of another 10 patients after the drug treatment
:::

. . . 

::: box-6-list
We would chose the first design to remove the effect of variability among individuals and increase our precision in estimating the treatment differences (i.e., effect of the medication)
:::

## Paired samples

::: box-6-list
When two samples are paired, you will find that the pairing generally falls into one of two types:
:::

. . . 

::: indented-text
{{< fa people-arrows >}} [Matched sample]{.highlight-tomato}: two groups are matched on a particular variable (e.g., twins, spouses)
:::

. . . 

::: indented-text
{{< fa person >}}{{< fa arrow-right >}}{{< fa person >}} [Repeated measures]{.highlight-tomato}: a measurement on the same experimental unit before and after
:::

## Paired or unpaired?

::: box-6-list
How would you design experiments around the following research questions? Paired, or unpaired?
:::

. . . 

::: indented-text
{{< fa tree >}} How does timber volume vary between two species of tree?
:::

. . . 

::: indented-text
{{< fa dog >}} How does the blood flow of two different arteries compare in dogs? 
:::

. . . 

::: indented-text
{{< fa cow >}} How does the milk yield of cows vary on two different diets?
:::

## Paired two-sample t-test
**Blood pressure example**

::: box-6-list
A new drug is thought to reduce blood pressure. It is called haemowussilin. An experiment is conducted on 15 middle-aged, male hypertension patients to evaluate the effect of the drug. For each patient, blood pressure is measured, and then after 6 months of the drug treatment, blood pressure is measured again. The question of interest is whether there is any evidence that haemowussilin has an effect on blood pressure.
:::

:::{.center-x}
{{< fa person >}} + {{< fa tablets >}} + {{< fa heart >}}
:::

## Paired two-sample t-test
**Blood pressure example: the data**

| Subject | Before ($Y_1$) | After ($Y_2$) |
|---------|----------------|---------------|
| 1       | 90             | 88            |
| 2       | 100            | 92            |
| 3       | 92             | 82            |
| 4       | 96             | 90            |
| 5       | 96             | 78            |
| 6       | 96             | 86            |
| 7       | 92             | 88            |
| 8       | 98             | 72            |
| 9       | 102            | 84            |
| 10      | 94             | 102           |
| 11      | 94             | 94            |
| 12      | 102            | 70            |
| 13      | 94             | 94            |
| 14      | 88             | 92            |
| 15      | 104            | 94            |

## Paired two-sample t-test
**Blood pressure example: the method**

::: box-6-list
Recall the general formula of a t-test:
:::

$$T = \frac{\bar{Y} - \mu_0}{S / \sqrt{n}} \sim T_{n-1}$$

. . . 

How do we get this to fit the experiment? We calculate the *difference*

## Paired two-sample t-test
**Blood pressure example: the data**

| Subject | Before ($Y_1$) | After ($Y_2$) | Difference ($D = Y_1 - Y_2$) |
|---------|----------------|---------------|------------------------------|
| 1       | 90             | 88            | 2                            |
| 2       | 100            | 92            | 8                            |
| 3       | 92             | 82            | 10                           |
| 4       | 96             | 90            | 6                            |
| 5       | 96             | 78            | 18                           |
| 6       | 96             | 86            | 10                           |
| 7       | 92             | 88            | 4                            |
| 8       | 98             | 72            | 26                           |
| 9       | 102            | 84            | 18                           |
| 10      | 94             | 102           | -8                           |
| 11      | 94             | 94            | 0                            |
| 12      | 102            | 70            | 32                           |
| 13      | 94             | 94            | 0                            |
| 14      | 88             | 92            | -4                           |
| 15      | 104            | 94            | 10                           |

## Paired two-sample t-test
**Blood pressure example: the math**

::: box-6-list
Let $\mu_1 =$ the population mean blood pressure before the drug treatment and $\mu_2 =$ the population mean blood pressure after the drug treatment. 
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> Let $\mu_D = \mu_1 - \mu_2$ denote the difference between the two blood pressure levels. 
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> We want to test $\mu_1 = \mu_2$ or $\mu_1 ≠ \mu_2$. Equivalently, we can test: 
:::

$$H_0: \mu_D = 0\:\:\ vs \:\:\ H_A: \mu_D ≠ 0$$

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> Let $D = Y_1 - Y_2$ denote the blood pressure difference and assume a random sample $D_1, D_2...,D_15$ of sample size $n = 15$ from $N(\mu_D, \sigma_D^2)$ 
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> Using this framework, we can use a one-sample t-test.
:::

## Paired two-sample t-test
**Blood pressure example: the math**

Under $H_0 : \mu_D = 0$, the test statistic is:

. . . 

$$T = \frac{\bar{D} - 0}{S_D / \sqrt{n}} \sim T_{n-1}$$

. . .

From the data, the observed $\bar{d} = 8.80$, $s_d = 10.98$. Thus the observed t statistic:

. . . 

$$T = \frac{\bar{d} - 0}{S_d / \sqrt{n}} = \frac{8.80 - 0}{10.98 / \sqrt{15}} = 3.10$$

on $n-1 = 14$ degrees of freedom

. . . 

p-value is: $2 \times P(T_{14} ≥ 3.10) = 0.0078$. We reject $H_0$ at the 1% ($\alpha = 0.01$) level. There is strong evidence against $H_0$. 

## Paired two-sample t-test: assumptions

::: box-6-list
$D$ is a random sample from a normal distribution *(We can check this with a histogram, density plot)*
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> Individual $Y_1$ and $Y_2$ need not be normally distributed. We're not performing the test on the individual scores, but rather the *difference*
:::

. . . 

::: box-6-list
Independence of the differences *(This is addressed during study/experimental design)*
:::

. . . 

::: indented-text
<i class="fa-solid fa-turn-up fa-rotate-90"></i> Note that $Y_1$ and $Y_2$ are not usually independent because of the pairing/same experimental unit
:::
